<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[用TravisCI持续集成自动部署Hexo博客的个人实践]]></title>
    <url>%2Fpost%2F90a759d5.html</url>
    <content type="text"><![CDATA[优秀的程序员都是不用持续集成的 前几天我面试了一个码农，连续几个专业问题他都没答上来。尴尬之余，我问它：「你没有什么理想吗？你现在最渴望的事情是什么？」码农转悠着大眼睛，不假思索道：「做一个持续集成的自动部署！」真没想到在面试中居然还有这种操作。我问为什么这能成为现阶段最渴望的事情。他反问「你就没有改动代码频繁，deploy无数次到崩溃的夜晚吗？寂寞长夜，总想好好专心写写博客，宽慰下自己啊。」今天我就要做为一个不优秀的程序员使用持续集成。 观看本教程的前置条件： 已经配置好了hexo博客。拥有一个仓库如：mtianyan.github.io\ 已经将博客的源码也push到了一个仓库如：hexoBlog-Github 已经配置了使用gulp进行博文压缩。 参考：推荐观看我的博文《Hexo+Next主题搭建博客安装美化及SEO优化指南》中将博客源码备份到github或码云 与 使用gulp进行博文压缩 完成上述前置条件，虽然不是必须的，但是可以与我保持一致。减少错误。 部分内容转载来源参考博客: https://www.karlzhou.com/2016/05/28/travis-ci-deploy-blog/ 个人对上面博客做了新版本的修订。我最近每天改动博客的内容。一会目录有问题，一会改了站点参数，主题参数，出现Bug,不停的提交快崩溃了：寂寞长夜，总想好好专心写写博客，宽慰下自己啊。 本文是一次在这个命题下的个人实践记录：大部分文字转载自参考博客 通常更新一篇Hexo博客文章，基本流程是： 本地新建post页面 12hexo n travis-ci-deploy-blogINFO Created: e:\WORK\GitHub\think-diff.me.source\source\_posts\travis-ci-deploy-blog.md 在文本编辑器里用markdown语法编辑新建页面 本地生成public文件：hexo g &amp;&amp; gulp 启动本地测试web server：hexo s --debug 浏览器打开http://localhost:4000/, 浏览生成文章 如果满意，即可部署到Github存放page仓库里：hexo d 本文主要介绍如何利用TravisCI自动完成第3-6步. What is Travis CI?Travis CI CI(Continuous Integration)翻译为持续集成。Travis CI是一个提供持续集成功能的平台，在Github上，可以添加Travis CI，当有code push时候，会推送通知到Travis，根据设置的脚本运行指定任务。 目前有两个站点: Travis.org 对于所有public项目完全免费 Travics.com 只针对private项目，提供更多一些额外功能，如cache，并行build个数 两个站点只能看到各自的项目，不能通用。 Why we need Travis CI?有人可能会有疑问: 在本地写完博客，直接一个命令hexo d，不就搞定了么， 为啥要费力搞CI？ 的确, 想用TravisCI来自动部署Hexo博客程序，需要不少设置（瞎折腾），为了给大伙信心，列举一些优点： 优点1：直接在线编辑文件，立即生效 假设你已经发表了一篇文章，过了几天你在朋友机器上浏览发现有几个明显的错别字，对于有强迫症的，这是不能容忍的。 但你手头又没有完整的hexo+nodejs+git的开发环境，重新下载git，node，hexo配置会花费不少时间，特别不划算。 如果按照这篇完整折腾完，你可以直接用浏览器访问github个人项目仓库，直接编辑那篇post的原md文件，前后2分钟改完。 稍等片刻，你的博客就自动更新了。 优点2：自动部署，同时部署到多个地方 在gitcafe是被收购之前，很多同学（包括我）都是托管在上面的，国内访问速度比Github快很多。配合DNS根据IP位置可以自动选择导到gitcafe, 还是github，甚至你还可以部署到七牛云的静态网站。利用Travis CI可同时更新多个仓库。 比如我的博客现在有两个站：一个部署在码云，一个部署在github。都需要我自己手动部署。 注：最后发现码云并不支持。emmmmm 优点3：部署快捷方便 手动deploy需要推送public整个folder到github上，当后期网站文章、图片较多时候，对于天朝的网络，有时候连接github 就是不顺畅，经常要傻等不少上传时间。有了CI，你可以只提交post文件里单独的md文件即可，很快很爽，谁用谁知道。 优点4：bigger than biggerbuild icon 你的项目Readme里面可以显示CI build图标，很酷有没有？另外通过设置，可以在当build失败时自动发邮件提醒你。上面的图标，如果登陆后你在Github项目里，直接点击图标，会跳转到你当前项目build的log界面，很方便。 当然有了CI，你可以做很多事情，如自动运行单元测试，成功后再deploy等等。很多项目里的持续集成基本也是这个道理。 How to use Travis CI to deploy hexo blog?原博主是使用private项目演示的，虽然我也就用学校邮箱加入了Github的Education plan 但是为了通用性，我本次做了public的实践。 步骤：准备Travis CI账号, 传送门：public项目, private项目, 在登陆成功后. 如果发现没有Repositories，点击上图加号会看到自己的目录然后把想要持续集成的仓库开关打开，会自动hook到Github。 注意: 这里需要打开的是自己的 准备Github Personal Access Token。在Github的setting页面，左侧面板选择Developer settings然后Personal access tokens, 右上角点击Generate new token。生成token时候需要确定访问scope，这里我们选择第一个repo即可。重要：生成的token只有第一次可见，一定要保存下来备用。 (可选操作：除非你对于Travis CI保存你的密钥不信任，才需要做，否则可以直接跳过省事)推荐直接跳过：有点奇怪原博主加了密又自行设置了环境变量保存token。加密在最后都没有用到。。。 准备Travis命令行工具，需要依赖ruby环境。对于Windows环境，可以使用这里的安装包，安装完成后可用ruby -v检查。 请安装2.4.x版本。 注意不要安装2.5版本。暂时不支持。会报错: 1ERROR: Error installing travis: The last version of ffi (&gt;= 1.3.0) to support your Ruby &amp; RubyGems was 1.9.18. Try installing it with `gem install ffi -v 1.9.18` and then running the current command again ffi requires Ruby version &lt; 2.5, &gt;= 2.0. The current ruby version is 2.5.0. 安装命令行工具，参考这里官方文档： 1gem install travis 验证travis安装成功 用Github的用户名/密码登录这个命令行工具。 1travis login 登陆成功后，开始加密，参考下面代码： 1234travis encrypt -r &lt;github name&gt;/&lt;github repo&gt; GH_Token=XXX#sample：travis encrypt -r mtianyan/hexoBlog-Github GH_TOKEN=33ba3948bxxx 把输出的secure:&quot;xxxx&quot;保存。可以在文件的 123env: global: - secure: `你加密后的` travis-ci会自动对于你的密钥进行解密。当做加密前的环境变量:GH_TOKEN 配置.travis.yml（如果没有，新建)添加环境变量来保护自己的github密钥。 点击more options 选择 setting. 找到环境变量值。将自己在github得到的个人token值 作为value，name可以自行定义Travis_Token 下面gulp的压缩安装，参考我Hexo中教程的方法。如果不需要压缩，可自行替换为hexo g 我个人的.travis.yml 可供参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748language: node_js #设置语言node_js: stable #设置相应的版本cache: apt: true directories: - node_modules # 缓存不经常更改的内容before_install: - export TZ=&apos;Asia/Shanghai&apos; # 更改时区install: - npm install #安装hexo及插件script: - hexo clean #清除 - hexo g &amp;&amp; gulp #生成after_script: - git clone https://$&#123;GH_REF&#125; .deploy_git # GH_REF是最下面配置的仓库地址 - cd .deploy_git - git checkout master - cd ../ - mv .deploy_git/.git/ ./public/ # 这一步之前的操作是为了保留master分支的提交记录，不然每次git init的话只有1条commit - cd ./public - git config user.name &quot;mtianyan&quot; #修改name - git config user.email &quot;1147727180@qq.com&quot; #修改email - git add . - git commit -m &quot;Travis CI Auto Builder at `date +&quot;%Y-%m-%d %H:%M&quot;`&quot; # 提交记录包含时间 跟上面更改时区配合 - git push --force --quiet &quot;https://$&#123;Travis_Token&#125;@$&#123;GH_REF&#125;&quot; master:master #Travis_Token是在Travis中配置环境变量的名称branches: only: - master #只监测master分支，master是我的分支的名称，可根据自己情况设置env: global: - GH_REF: github.com/mtianyan/mtianyan.github.io.git #设置GH_REF，注意更改yourname# configure notifications (email, IRC, campfire etc)# please update this section to your needs!# https://docs.travis-ci.com/user/notifications/notifications: email: - 1147727180@qq.com - mtianyan@outlook.com on_success: change on_failure: always 这里需要注意的是：GH_REF，这个地址其实就是你github上存放静态博客最终文件的仓库地址，末尾加上.git。 集成build icon，在Travis CI控制台里，点击那个icon，选择markdown的样式，然后放到项目Readme里即可。 大功告成后的使用。你可以在本地hexo源码目录内，使用命令: 123git add .git commit -am &quot;实验&quot;git push -u origin master 来将自己的博客源码提交至github中的源码仓库 CI检测到你的源码更新改动会自动帮你编译git push 到你的静态页面存放仓库 在commit记录中会显示bulid的状态。 注意：有点慢。毕竟他要在Ci的服务器上根据你的配置文件安装下载，然后使用它的虚拟机进行部署。]]></content>
      <categories>
        <category>运维环境配置</category>
      </categories>
      <tags>
        <tag>环境搭建</tag>
        <tag>Hexo</tag>
        <tag>TravisCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶学习笔记]]></title>
    <url>%2Fpost%2F1d8f9861.html</url>
    <content type="text"><![CDATA[进阶的基础是学会入门 Python零基础入门课程学习完之后。我继续复习进阶知识。课程知识+个人总结以及知识点标注与相关难点探究。 [√] 慕课网廖雪峰老师的: python进阶 课程详细介绍Python强大的函数式编程和面向对象编程，掌握Python高级程序设计的方法。 打好基础再来进阶在基础入门笔记中我们学到了以下知识: 安装Python环境 变量和数据类型：Python内置的基本类型 List和Tuple：顺序的集合类型 条件判断和循环：控制程序流程 Dict和Set：根据key访问的集合类型 函数：定义和调用函数 切片：如何对list进行切片 迭代：如何用for循环迭代集合类型 列表生成式:如何快速生成列表 注：我个人对于自认为重要的知识点都进行了知识点关键字标记。如果想复习的小伙伴可以ctrl + f 输入关键字 知识点 进行查看。对于重要以及较难的编程题目我进行了关键字 天涯的标记。 进阶课程中将会学到的知识： 函数式编程：注意与函数编程区别 模块：模块的使用 面向对象：概念，属性，方法，基础，多态。 定制类：利用Python的特殊方法定制类 学习目标：掌握函数式编程 掌握面向对象编程 能够编写模块化的程序 Python函数式编程讲解Python函数式编程概念，高阶函数的概念和实际用法，以及装饰器函数的原理和实现方式。 函数式编程简介函数: function,入门课程学习过的函数式：functional, 一种编程范式 函数式编程 是一种抽象的编程模式 不同语言的抽象层次不同: 上图中语言越往上越高级，抽象程度越高。越往上抽象就逐渐向数学接近 知识点: 函数式编程的特点: 把计算视为函数而非指令(这样不贴近计算机而是贴进计算) 纯函数式编程: 不需要变量，没有副作用，测试简单。(函数任意执行多少次结果确定) 支持高阶函数，代码简洁 知识点: python支持的函数式编程 不是纯函数式编程：允许有变量 支持高阶函数：函数也可以作为变量传入 支持闭包：有了闭包就能返回函数 有限度的支持匿名函数。 高阶函数高阶函数： 变量可以指向函数 demo: 直接输入abs返回的是一个函数对象。而变量f 通过赋值运算符指向abs函数对象然后此时f这个函数名就可以实现调用abs() 知识点: 函数名其实就是指向函数的一个变量(一个很普通的变量) abs 就是一个普通的变量名。通过赋值运算符指向了python内置的abs函数对象。我们可以把它当做普通变量处理。 知识点: 高阶函数高在自己能接受函数作为参数 高阶函数: 能接收函数作为参数的函数。 变量可以指向函数 函数的参数可以接受变量 所以一个函数可以接收另一个函数作为参数。这时我们把能接收函数做参数的函数成为高阶函数。 demo: 接受abs函数 定义一个函数，接收x,y,f三个参数。 其中x,y是数值，f是函数 def add(x,y,f) return f(x) + f(y) 只要我们理解: 变量可以指向函数,函数可以作为一个变量传递给另一个函数就可以了。 待探究: 如果刚才我们一直没有关闭窗口。也就是我们的abs变量还指向的是len函数对象。 解决方案: 暂无待续 把函数作为参数上一节我们讲了高阶函数的概念，并编写了一个简单的高阶函数： 12def add(x, y, f): return f(x) + f(y) 如果传入abs作为参数f的值： 1add(-5, 9, abs) 根据函数的定义，函数执行的代码实际上是： 1abs(-5) + abs(9) 由于参数 x, y 和 f都可以任意传入，如果 f传入其他函数，就可以得到不同的返回值。 编程任务 利用add(x,y,f)函数，计算： 注意: python中的sqrt函数在math包中。需要import进来import的相关知识后面会讲的。这里只需要用一下。 123456789import math# from math import sqrtdef add(x, y, f): return f(x) + f(y)print add(25, 9, math.sqrt)# 如果采用第二行import方法调用时如下print add(25, 9, sqrt) 运行结果: 18.0 map()函数map()是 Python 内置的高阶函数，它接收一个函数f和一个 list，并通过把函数f依次作用在list的每个元素上，得到一个新的list并返回这个list。 例如，对于list [1, 2, 3, 4, 5, 6, 7, 8, 9] 如果希望把list的每个元素都作平方，就可以用map()函数： 因此，我们只需要传入函数f(x)=x*x，就可以利用map()函数完成这个计算： 123def f(x): return x*xprint map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]) 输出结果： 1[1, 4, 9, 10, 25, 36, 49, 64, 81] 知识点； 注意：map()函数不改变原有的 list，而是返回一个新的 list。 利用map()函数，可以把一个 list 转换为另一个 list，只需要传入转换函数。 由于list包含的元素可以是任何类型，因此，map() 不仅仅可以处理只包含数值的 list，事实上它可以处理包含任意类型的 list，只要传入的函数f可以处理这种数据类型。 编程任务 假设用户输入的英文名字不规范，没有按照首字母大写，后续字母小写的规则，请利用map()函数. 把一个list（包含若干不规范的英文名字）变成一个包含规范英文名字的list： 输入：[&#39;adam&#39;, &#39;LISA&#39;, &#39;barT&#39;]输出：[&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;] 实现代码: 123def format_name(s): return s[0].upper() + s[1:].lower()print map(format_name, ['adam', 'LISA', 'barT']) 运行结果: 1[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] reduce()函数reduce()函数也是Python内置的一个高阶函数。reduce()函数接收的参数和 map()类似，一个函数f，一个list，但知识点：行为和 map()不同，reduce()传入的函数 f 必须接收两个参数，reduce()对list的每个元素反复调用函数f，并返回最终结果值。 例如，编写一个f函数，接收x和y，返回x和y的和： 12def f(x, y): return x + y 调用 reduce(f, [1, 3, 5, 7, 9])时，reduce函数将做如下计算： 12345先计算头两个元素：f(1, 3)，结果为4；再把结果和第3个元素计算：f(4, 5)，结果为9；再把结果和第4个元素计算：f(9, 7)，结果为16；再把结果和第5个元素计算：f(16, 9)，结果为25；由于没有更多的元素了，计算结束，返回结果25。 上述计算实际上是对 list 的所有元素求和。虽然Python内置了求和函数sum()，但是，利用reduce()求和也很简单。 知识点：reduce()还可以接收第3个可选参数，作为计算的初始值。如果把初始值设为100，计算： 1reduce(f, [1, 3, 5, 7, 9], 100) 结果将变为125，因为第一轮计算是： 计算初始值和第一个元素：f(100, 1)，结果为101。 编程任务 Python内置了求和函数sum()，但没有求积的函数，请利用recude()来求积： 12输入：[2, 4, 5, 7, 12]输出：2*4*5*7*12的结果 实现代码: 1234def prod(x, y): return x*yprint reduce(prod, [2, 4, 5, 7, 12]) 运行结果： 13360 filter()函数filter()函数是 Python 内置的另一个有用的高阶函数，filter()函数接收一个函数 f 和一个list，这个函数 f 的作用是对每个元素进行判断，返回 True或 False，filter() 知识点：根据判断结果自动过滤掉不符合条件的元素，返回由符合条件元素组成的新list。 例如，要从一个list [1, 4, 6, 7, 9, 12, 17]中删除偶数，保留奇数，首先，要编写一个判断奇数的函数： 12def is_odd(x): return x % 2 == 1 然后，利用filter()过滤掉偶数： 12filter(is_odd, [1, 4, 6, 7, 9, 12, 17])结果：[1, 7, 9, 17] 利用filter()，可以完成很多有用的功能，例如，删除 None 或者空字符串： 123def is_not_empty(s): return s and len(s.strip()) &gt; 0filter(is_not_empty, ['test', None, '', 'str', ' ', 'END']) 结果： 1[&apos;test&apos;, &apos;str&apos;, &apos;END&apos;] 知识点: 注意: s.strip(rm) 删除 s 字符串中开头、结尾处的 rm 序列的字符。 当rm为空时，默认删除空白符（包括’\n’, ‘\r’, ‘\t’, ‘ ‘)，如下： 12a = &apos; 123&apos;a.strip() 结果： ‘123’ 123a=&apos;\t\t123\r\n&apos;a.strip()结果：&apos;123&apos; 编程任务 请利用filter()过滤出1~100中平方根是整数的数，即结果应该是： 1[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 实现代码: 123456import mathdef is_sqr(x): return math.sqrt(x)%1==0print filter(is_sqr, range(1, 101)) 注: sqrt函数是开根号的函数，返回值为浮点数。在math包中。 math.sqrt(4) 结果是 2.0 运行结果: 1[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 自定义排序函数Python内置的 sorted()函数可对list进行排序： 1&gt;&gt;&gt;sorted([36, 5, 12, 9, 21]) 运行结果： 1[5, 9, 12, 21, 36] 知识点: 但 sorted()也是一个高阶函数，它可以接收一个比较函数来实现自定义排序，比较函数的定义是，传入两个待比较的元素 x, y，如果 x应该排在y的前面，返回-1，如果x 应该排在 y 的后面，返回 1。如果 x 和 y 相等，返回 0。 因此，如果我们要实现倒序排序，只需要编写一个reversed_cmp函数： 123456def reversed_cmp(x, y): if x &gt; y: return -1 if x &lt; y: return 1 return 0 解析上面函数: 当x&gt;y时，return-1.向sorted表明x应排在y的前面。也就是越大的排越靠前(俗称逆序) 其他情况同理可得 这样，调用 sorted() 并传入 reversed_cmp就可以实现倒序排序： 12&gt;&gt;&gt; sorted([36, 5, 12, 9, 21], reversed_cmp)[36, 21, 12, 9, 5] 知识点：sorted()也可以对字符串进行排序，字符串默认按照ASCII大小来比较： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'])['Credit', 'Zoo', 'about', 'bob'] ‘Zoo’排在’about’之前是因为’Z’的ASCII码比’a’小。 编程任务对字符串排序时，有时候忽略大小写排序更符合习惯。请利用sorted()高阶函数，实现忽略大小写排序的算法。 12输入：[&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;]输出：[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;] 实现代码: 1234def cmp_ignore_case(s1, s2): return cmp(s1.lower(), s2.lower())print sorted(['bob', 'about', 'Zoo', 'Credit'], cmp_ignore_case) 运行结果： 1[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;] 返回函数Python的函数不但可以返回int、str、list、dict等数据类型，还可以返回函数！ 解析：因为前面我们讲过Python中的函数为高阶函数。也就是可以把函数作为一个参数使用。既然是一个参数(变量)，那么可以被用来返回也不奇怪了。 例如，定义一个函数 f()，我们让它返回一个函数 g，可以这样写： 1234567def f(): print 'call f()...' # 定义函数g: def g(): print 'call g()...' # 返回函数g: return g 仔细观察上面的函数定义，我们在函数f内部又定义了一个函数 g。由于函数 g也是一个对象，函数名 g 就是一个指向函数 g 的变量，所以，最外层函数 f 可以返回变量 g，也就是函数 g 本身。 调用函数 f，我们会得到 f 返回的一个函数： 123456&gt;&gt;&gt; x = f() # 调用f()call f()...&gt;&gt;&gt; x # 变量x是f()返回的函数：&lt;function g at 0x1037bf320&gt;&gt;&gt;&gt; x() # x指向函数，因此可以调用call g()... # 调用x()就是执行g()函数定义的代码 知识点: 图中x = f()对于f()进行调用的同时将返回值存入了x中。因此x存放上了f函数的返回值: g函数。因为x指向一个函数对象。所以我们可以调用该函数对象。 请注意区分返回函数和返回值： 1234def myabs(): return abs # 返回函数def myabs2(x): return abs(x) # 返回函数调用的结果，返回值是一个数值 知识点: 返回函数可以把一些计算延迟执行。例如，如果定义一个普通的求和函数： 12def calc_sum(lst): return sum(lst) 调用calc_sum()函数时，将立刻计算并得到结果： 12&gt;&gt;&gt; calc_sum([1, 2, 3, 4])10 但是，如果返回一个函数，就可以延迟计算： 1234def calc_sum(lst): def lazy_sum(): return sum(lst) return lazy_sum 调用calc_sum()并没有计算出结果，而是返回函数: 123&gt;&gt;&gt; f = calc_sum([1, 2, 3, 4])&gt;&gt;&gt; f&lt;function lazy_sum at 0x1037bfaa0&gt; 对返回的函数进行调用时，才计算出结果: 12&gt;&gt;&gt; f()10 由于可以返回函数，我们在后续代码里就可以决定到底要不要调用该函数。 编程任务 请编写一个函数calc_prod(lst)，它接收一个list，返回一个函数，返回函数可以计算参数的乘积。 实现代码: 1234567def calc_prod(lst): def prod(): return reduce((lambda x, y : x * y),lst) return prodf = calc_prod([1, 2, 3, 4])print f() 解析: reduce((lambda x, y : x * y),lst) reduce为一个高阶函数，传入两个参数。 参数一为(lambda x, y : x * y)这是一个lambda函数。表示传入参数x,y。然后进行x * y运算。 参数二: 一个可迭代对象。 过程: 1 * 2得到结果 2 2 * 3得到结果 6 6 * 4得到结果 24 运行结果： 124 闭包在函数内部定义的函数和外部定义的函数是一样的，只是他们无法被外部访问： 123456def g(): print 'g()...'def f(): print 'f()...' return g 知识点；将 g 的定义移入函数 f 内部，防止其他代码调用 g： 12345def f(): print 'f()...' def g(): print 'g()...' return g 但是，考察上一小节定义的 calc_sum 函数： 1234def calc_sum(lst): def lazy_sum(): return sum(lst) return lazy_sum 知识点 注意: 发现没法把 lazy_sum 移到 calc_sum 的外部，因为它引用了 calc_sum 的参数 lst。也就是在calc_sum函数外部定义的lazy_sum函数，就没办法接收传入calc_sum函数的参数。 知识点:python闭包的定义 像这种内层函数引用了外层函数的变量（参数也算变量），然后返回内层函数本身的情况，称为闭包（Closure）。 闭包的特点是返回的函数还引用了外层函数的局部变量 如被返回的lazy_sum 函数还引用了传入calc_sum函数的变量(局部) 所以，要知识点: 正确使用闭包，就要确保引用的局部变量在函数返回后不能变。举例如下： 希望一次返回3个函数，分别计算1x1,2x2,3x3: 1234567891011def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count()print f1(), f2(), f3() 你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果全部都是 9。 知识点: 原因就是当count()函数返回了3个函数时，这3个函数所引用的变量 i 的值已经变成了3。由于f1、f2、f3并没有被调用，所以，此时他们并未计算 i*i，当 f1 被调用时： 个人解析： f1, f2, f3 = count()调用count()函数, 进入count()函数内部。 fs = []创建一个空的集合 for i in range(1, 4): 循环执行三次代码块 1234for i in range(1, 4): def f(): return i*i fs.append(f) 第一次执行时，定义一个f()函数，该函数此时被直接调用时的返回值为 i*i ;然后将f函数作为一个元素加入空列表fs中。验证代码如下 123456789101112def count(): fs = [] for i in range(1, 4): def f(): return i*i print "我现在的返回值：%d",f() fs.append(f) return fsf1, f2, f3 = count()print f1(), f2(), f3() 然后继续执行。直到把三个函数都当做元素存入fs。至此f1, f2, f3 = count()运行完毕 print f1(), f2(), f3() 调用这三个函数。此时f()被调用。执行i*i,但是现在的i 值为3.所以三个值都为9. 12&gt;&gt;&gt; f1()9 # 因为f1现在才计算i*i，但现在i的值已经变为3 因此，返回函数不要引用任何循环变量，或者后续会发生变化的变量。 编程任务(天涯) 返回闭包不能引用循环变量，请改写count()函数，让它正确返回能计算1x1、2x2、3x3的函数。 实现代码: 12345678910def count(): fs = [] for i in range(1, 4): def f(i): return lambda : i*i fs.append(f(i)) return fsf1, f2, f3 = count()print f1(), f2(), f3() 解析：往fs中添加的元素不在是f函数本身。此时添加的对象是f函数的调用也就是他被调用后的返回值lambda : i*i 一个lambda函数对象。 lambda : i*i时它使用的变量是当次循环传入的i值。而不是循环变量i 解析二： 考察下面的函数 f: 1234def f(j): def g(): return j*j return g 它可以正确地返回一个闭包g，g所引用的变量j不是循环变量，因此将正常执行。 使用下一个函数来将循环变量在单词循环之时持久化。 参考代码: 123456789101112def count(): fs = [] for i in range(1, 4): def f(j): def g(): return j*j return g r = f(i) fs.append(r) return fsf1, f2, f3 = count()print f1(), f2(), f3() 运行结果： 11 4 9 附加解法: 123456789101112def count(): fs = [] for i in range(1, 4): def f(): return i*i print "我现在的返回值：%d",f() fs.append(f()) return fsf1, f2, f3 = count()print f1, f2, f3 匿名函数高阶函数可以接收函数做参数，有些时候，我们不需要显式地定义函数，直接传入匿名函数更方便。 在Python中，对匿名函数提供了有限支持。还是以map()函数为例，计算 f(x)=x的平方 时，除了定义一个f(x)的函数外，还可以直接传入匿名函数： 12&gt;&gt;&gt; map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])[1, 4, 9, 16, 25, 36, 49, 64, 81] 通过对比可以看出，匿名函数 lambda x: x * x实际上就是： 12def f(x): return x * x 关键字lambda 表示匿名函数，冒号前面的 x 表示函数参数。 知识点: 匿名函数有个限制，就是只能有一个表达式，不写return，返回值就是该表达式的结果。 使用匿名函数，可以不必定义函数名，直接创建一个函数对象，很多时候可以简化代码： 12&gt;&gt;&gt; sorted([1, 3, 9, 5, 0], lambda x,y: -cmp(x,y))[9, 5, 3, 1, 0] 注：cmp(x,y) 函数用于比较2个对象，如果 x &lt; y返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1。 前面我们知道排序函数。是接收到return-1.向sorted表明x应排在y的前面。也就是小的x排前面,那么此时是正序排列。而-cmp(x,y)则是逆序排列。 返回函数的时候，也可以返回匿名函数： 12345&gt;&gt;&gt; myabs = lambda x: -x if x &lt; 0 else x &gt;&gt;&gt; myabs(-1)1&gt;&gt;&gt; myabs(1)1 解析: 匿名函数本质上就是一个函数，它所抽象出来的东西是一组运算。Python lambda总共以下2种写法，一种是不含if…else…的，一种是包含if…else…的： 12lambda &lt;args&gt;: &lt;return-expression&gt;lambda &lt;args&gt;: &lt;return-expression&gt; if &lt;cond-expression&gt; else &lt;return-expression&gt; 更多学习资源: http://locatino.github.io/2015/08/03/playing-python-lambda/ 如果想要在匿名函数中做一些除了return-expression，其他事情（比如说print()）,应该怎么写？确实不太好写，不过我还是想出了一个非常投机取巧的办法： 1lambda x: True if x % 2 == 0 and print(x) == None else x") 上面这个函数不仅返回了x的值，并且还打印了x.这里我把需要执行的语句(print())转换成了逻辑表达式，而且是一个永远为真的表达式，它一定会执行，并且对业务的逻辑判断没有影响。前提是你必须准确的知道需要执行的语句的返回值，才能写出永远为真的表达式。 and语句的短路：所以一定要保证前面x%2 == 0为真。此时if x % 2 == 0 and print(x)返回print(x) 编程任务 利用匿名函数简化以下代码： 123def is_not_empty(s): return s and len(s.strip()) &gt; 0filter(is_not_empty, ['test', None, '', 'str', ' ', 'END']) 实现代码: 1print filter(lambda s:s and len(s.strip()) &gt; 0, ['test', None, '', 'str', ' ', 'END']) 既要满足s非空，又要满足s不是结束字符等。 注意: s.strip(rm) 删除 s 字符串中开头、结尾处的 rm 序列的字符。 当rm为空时，默认删除空白符（包括’\n’, ‘\r’, ‘\t’, ‘ ‘) 运行结果： 1[&apos;test&apos;, &apos;str&apos;, &apos;END&apos;] Python中的装饰器 在不改变原函数的情况下，想在运行时动态的增加新功能。可以极大地简化代码，避免每个函数编写重复性代码。 示例: 希望对下列函数调用增加log功能，打印出函数调用 123456def f1(x): return x*2def f2(): return x*xdef f3(): return x*x*x 方法一: 直接修改原函数 123456789def f1(x): print 'call f1()' return x*2def f2(): print 'call f2()' return x*xdef f3(): print 'call f3()' return x*x*x 高阶函数： 可以接收函数作为参数 可以返回函数 是否可以接收一个函数，对其包装，然后返回一个新函数。 方法二: 通过高阶函数返回新函数 1234567def f1(x): return x*2def new_fn(f): def fn(x): print 'call' + f.__name__ + '()' return f(x) return fn 代码解析：我们可以定义一个new_fn这样一个函数。接收f参数。返回一个fn新函数 在这个新函数内部首先我们用print语句打印出了一行日志。紧接着我们对原函数进行调用。返回原函数的结果。 至此我们就得到了一个新的函数。它即包含了原函数的调用（也就是返回原函数调用后的值，又有一行日志来增强原函数的功能。 这里的new_fn(f)这个高阶函数就是装饰器函数： 知识点： 调用装饰器函数: 12g1 = new_fn(f1)print g1(5) 向new_fn传入f1，然后会 先定义一个fn(x)函数。 进入fn(x)函数内部：执行第一行打印log，然后如果有对于fn(x)的调用会返回传入的f函数并为其传入参数x. 但是很明显：现在只是定义了这个函数fn(x),并没有进行调用。 所以此时g1 = new_fn(f1)的执行结果便是new_fn的返回值。返回fn对象，并被存入了g1变量中。此时对于g1的调用就是对于fn的调用。 print g1(5)实际就是调用了fn函数。 即打印了日志 得到了原函数的返回值return f(x) 方法2： 12f1 = new_fn(f1)print f1(5) 将f1作为参数传入的new_fn的返回值 fn 保存回f1本身因为函数名本身也是变量。所以我们就用这种方法彻底的隐藏了f1的原始定义函数。这时候f1就是装饰以后的函数。 注： _name_ 是函数的一个属性，取到函数名。 Python内置的@语法就是为了简化装饰器的调用。 123@new_fndef f1(x): return x*2 装饰器的作用 可以极大地简化代码，避免每个函数编写重复性代码 打印日志@log 检测性能@performance 数据库事务@transaction URL路由@post(&#39;/register&#39;) @post(&#39;/register&#39;)让这个函数来处理指定的url。 编写无参数decorator知识点：Python的 decorator 本质上就是一个高阶函数，它接收一个函数作为参数，然后，返回一个新函数。 使用 decorator 用Python提供的 @ 语法，这样可以避免手动编写 f = decorate(f) 这样的代码。 考察一个@log的定义： 12345def log(f): def fn(x): print 'call ' + f.__name__ + '()...' return f(x) return fn 对于阶乘函数，@log工作得很好： 1234@logdef factorial(n): return reduce(lambda x,y: x*y, range(1, n+1))print factorial(10) 结果： 12call factorial()...3628800 但是，对于参数不是一个的函数，调用将报错： 1234@logdef add(x, y): return x + yprint add(1, 2) 结果： 1234Traceback (most recent call last): File &quot;test.py&quot;, line 15, in &lt;module&gt; print add(1,2)TypeError: fn() takes exactly 1 argument (2 given) 因为 add() 函数需要传入两个参数，但是@log写死了只含一个参数的返回函数。 要让 @log 自适应任何参数定义的函数，可以利用Python的 *args 和 **kw，保证任意个数的参数总是能正常调用： 12345def log(f): def fn(*args, **kw): print 'call ' + f.__name__ + '()...' return f(*args, **kw) return fn 现在，对于任意函数，@log 都能正常工作。 注： *args表示任何多个无名参数，它是一个tuple **kw表示关键字参数，它是一个dict。 并且同时使用*args和**kwargs时，必须*args参数列要在**kw前 参考博客： http://blog.csdn.net/feisan/article/details/1729905 https://n3xtchen.github.io/n3xtchen/python/2014/08/08/python-args-and-kwargs Python里，带*的参数就是用来接受可变数量参数的。 12345def funcD(a, b, *c): print a print b print "length of c is: %d " % len(c) print c 调用funcD(1, 2, 3, 4, 5, 6)结果是 123412length of c is: 4(3, 4, 5, 6) 如果一个函数定义中的最后一个形参有 **前缀,所有正常形参之外的其他的关键字参数都将被放置在一个字典中传递给函数，比如： 1234def funcF(a, **b): print a for x in b: print x + ": " + str(b[x]) 调用funcF(100, c=&#39;你好&#39;, b=200)，执行结果 123100c: 你好b: 200 大家可以看到，b是一个dict对象实例，它接受了关键字参数b和c。 应用使用 Mysql Python 库时候,经常看到这个样子的代码，db_conf 一般都从配置文件读取，这时优雅的不定字典参数就派上用途了！ 123456789101112131415import mysql.connector db_conf = &#123; user='xx', password='yy', host='xxx.xxx.xxx.xxx', database='zz'&#125;cnx = mysql.connector.connect( user=db_conf['user'], password=db_conf['password'], host=db_conf['host'], database=db_conf['database'] ) 修改版： 1234567891011import mysql.connector db_conf = &#123; user='xx', password='yy', host='xxx.xxx.xxx.xxx', database='zz'&#125;cnx = mysql.connector.connect(**db_conf)... 编程任务(天涯) 请编写一个@performance，它可以打印出函数调用的时间。 实现代码: 12345678910111213141516import timedef performance(f): def log_time(x): t1= time.time() res = f(x) t2=time.time() print 'call %s() in %fs' % (f.__name__, (t2 - t1)) return res return log_time@performancedef factorial(n): return reduce(lambda x,y: x*y, range(1, n+1))print factorial(10) 解析： performance(f)是一个高阶函数，传入一个参数：函数对象,返回包装后的新函数。 log_time(x)是被高阶函数包裹的新函数。它复杂把旧函数的参数作为自己的参数。在自己内部执行想要额外添加的功能。然后调用原函数，将原函数调用后的返回值作为自己的返回值。 time模块(参考time模块内容) 运行结果： 12call factorial() in 0.004899s3628800 编写带参数decorator考察上一节的 @log 装饰器： 12345def log(f): def fn(x): print 'call ' + f.__name__ + '()...' return f(x) return fn 发现对于被装饰的函数，log打印的语句是不能变的（除了函数名）。 如果有的函数非常重要，希望打印出&#39;[INFO] call xxx()...&#39;，有的函数不太重要，希望打印出&#39;[DEBUG] call xxx()...&#39;，这时，log函数本身就需要传入&#39;INFO&#39;或&#39;DEBUG&#39;这样的参数，类似这样： 123@log('DEBUG')def my_func(): pass 把上面的定义翻译成高阶函数的调用，就是： 1my_func = log(&apos;DEBUG&apos;)(my_func) 解：log(&#39;DEBUG&#39;)(my_func)装饰器函数log(&#39;DEBUG&#39;)&#39;被调用传入my_func原函数对象 上面的语句看上去还是比较绕，再展开一下： 12log_decorator = log(&apos;DEBUG&apos;)my_func = log_decorator(my_func) 也就是装饰器函数本身是log(&#39;DEBUG&#39;),然后为该函数调用时传入参数my_func 上面的语句又相当于： 1234log_decorator = log(&apos;DEBUG&apos;)@log_decoratordef my_func(): pass 知识点：所以，带参数的log函数首先返回一个decorator函数，再让这个decorator函数接收my_func并返回新函数： 123456789101112def log(prefix): def log_decorator(f): def wrapper(*args, **kw): print '[%s] %s()...' % (prefix, f.__name__) return f(*args, **kw) return wrapper return log_decorator@log('DEBUG')def test(): passprint test() 执行结果： 12[DEBUG] test()...None 与上一节无参数做比较: 12345def log(f): def fn(x): print 'call ' + f.__name__ + '()...' return f(x) return fn 可以看出它在log(f)的上层又多加了一层函数外壳。使得带参数的log函数首先返回一个decorator函数。 知识点：无参数装饰器为两层结构。有参数装饰器为三层结构 对于这种3层嵌套的decorator定义，你可以先把它拆开： 12345678910# 标准decorator:def log_decorator(f): def wrapper(*args, **kw): print '[%s] %s()...' % (prefix, f.__name__) return f(*args, **kw) return wrapper# 返回decorator:def log(prefix): return log_decorator(f) 拆开以后会发现，调用会失败，因为在3层嵌套的decorator定义中，最内层的wrapper引用了最外层的参数prefix，所以，把一个闭包拆成普通的函数调用会比较困难。不支持闭包的编程语言要实现同样的功能就需要更多的代码。 那么我们如何拆开才能保证正确呢？ 1未完待续 编程任务(天涯)上一节的@performance只能打印秒，请给 @performace增加一个参数，允许传入&#39;s&#39;或&#39;ms&#39;： 123@performance('ms')def factorial(n): return reduce(lambda x,y: x*y, range(1, n+1)) 实现代码(天涯): 12345678910111213141516171819import timedef performance(unit): def perf_decorator(f): def wrapper(*args, **kw): t1 = time.time() r = f(*args, **kw) t2 = time.time() t = (t2 - t1)*1000 if unit =='ms' else (t2 - t1) print 'call %s() in %f %s'%(f.__name__, t, unit) return r return wrapper return perf_decorator@performance('ms') def factorial(n): return reduce(lambda x,y: x*y, range(1, n+1))print factorial(10) 最外层输入：参数unit，返回：接收了参数的装饰器函数对象。 次外层输入：参数f 函数对象。返回: 被包装过的函数对象wrapper 最内层输入：参数为*args, **kw。进行额外的处理加返回：返回f函数调用的返回值。 人肉运行： 12345perf_decorator = performance(&apos;ms&apos;)factorial = perf_decorator(factorial(10))factorial = wrapperfactorial(10) = wrapper(10)# 然后执行 print 并返回 f(*args, **kw)的值 运行结果： 12call factorial() in 6.043911 ms3628800 完善decorator@decorator可以动态实现函数功能的增加，但是，经过@decorator“改造”后的函数，和原函数相比，除了功能多一点外，有没有其它不同的地方？ 在没有decorator的情况下，打印函数名： 123def f1(x): passprint f1.__name__ 输出： f1 有decorator的情况下，再打印函数名： 123456789def log(f): def wrapper(*args, **kw): print 'call...' return f(*args, **kw) return wrapper@logdef f2(x): passprint f2.__name__ 输出： wrapper 可见，由于decorator返回的新函数函数名已经不是&#39;f2&#39;，而是@log内部定义的&#39;wrapper&#39;。这对于那些依赖函数名的代码就会失效。decorator还改变了函数的__doc__等其它属性。知识点: 如果要让调用者看不出一个函数经过了@decorator的“改造”，就需要把原函数的一些属性复制到新函数中： 1234567def log(f): def wrapper(*args, **kw): print 'call...' return f(*args, **kw) wrapper.__name__ = f.__name__ wrapper.__doc__ = f.__doc__ return wrapper 这样写decorator很不方便，因为我们也很难把原函数的所有必要属性都一个一个复制到新函数上，所以Python内置的functools可以用来自动化完成这个“复制”的任务： 1234567import functoolsdef log(f): @functools.wraps(f) def wrapper(*args, **kw): print 'call...' return f(*args, **kw) return wrapper 最后需要指出，由于我们把原函数签名改成了(*args, **kw)，因此，无法获得原函数的原始参数信息。即便我们采用固定参数来装饰只有一个参数的函数： 123456def log(f): @functools.wraps(f) def wrapper(x): print 'call...' return f(x) return wrapper 也可能改变原函数的参数名，因为新函数的参数名始终是 &#39;x&#39;，原函数定义的参数名不一定叫 &#39;x&#39;。 编程任务请思考带参数的@decorator，@functools.wraps应该放置在哪： 123456def performance(unit): def perf_decorator(f): def wrapper(*args, **kw): ??? return wrapper return perf_decorator 实现代码: 123456789101112131415161718192021import time, functoolsdef performance(unit): def perf_decorator(f): @functools.wraps(f) def wrapper(*args,**kv): t0 = time.time() r = f(*args,**kv) t1 = time.time() t = (t1 - t0) if unit =='s' else (t1 - t0) * 1000 print 'call %s() in %s %s' % (f.__name__, t, unit) return r return wrapper return perf_decorator@performance('ms')def factorial(n): return reduce(lambda x,y: x*y, range(1, n+1))print factorial.__name__ tips: 最外层输入：参数unit，返回：接收了参数的装饰器函数对象。 次外层输入：参数f 函数对象。返回: 被包装过的函数对象wrapper 最内层输入：参数为*args, **kw。进行额外的处理加返回：返回f函数调用的返回值。 搞清每一层的输入输出就可以成功写出来 运行结果： 1factorial 偏函数当一个函数有很多参数时，调用者就需要提供多个参数。如果减少参数个数，就可以简化调用者的负担。 比如，int()函数可以把字符串转换为整数，当仅传入字符串时，int()函数默认按十进制转换： 12&gt;&gt;&gt; int('12345')12345 但int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换： 1234&gt;&gt;&gt; int('12345', base=8)5349&gt;&gt;&gt; int('12345', 16)74565 知识点：假设要转换大量的二进制字符串，每次都传入int(x, base=2)非常麻烦，于是，我们想到，可以定义一个int2()的函数，默认把base=2传进去： 12def int2(x, base=2): return int(x, base) 这样，我们转换二进制就非常方便了： 1234&gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 functools.partial就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2： 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2(&apos;1000000&apos;)64&gt;&gt;&gt; int2(&apos;1010101&apos;)85 所以，functools.partial可以把一个参数多的函数变成一个参数少的新函数，少的参数需要在创建时指定默认值，这样，新函数调用的难度就降低了。 知识点：偏函数是为一个多参数函数设定参数的默认值，可以降低函数调用的难度。 编程任务 在第7节中，我们在sorted这个高阶函数中传入自定义排序函数就可以实现忽略大小写排序。请用functools.partial把这个复杂调用变成一个简单的函数： 1sorted_ignore_case(iterable) 实现代码: 12345import functoolssorted_ignore_case = functools.partial(sorted,key=str.lower)print sorted_ignore_case(['bob', 'about', 'Zoo', 'Credit']) 运行结果： 1[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;] functools.partial 传入两个参数，一个是原函数。一个是原函数包含的默认参数。 官方文档对于sorted的说明： key specifies a function of one argument that is used to extract a comparison key from each list element:key=str.lower. The default value isNone. 课外知识： sort 与 sorted 区别： sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。 list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted方法返回的是一个新的 list，而不是在原来的基础上进行的操作。 sorted 语法： sorted(iterable[, cmp[, key[, reverse]]]) 参数说明： iterable – 可迭代对象。 cmp – 比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出，此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。 key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。 reverse – 排序规则，reverse = True 降序 ， reverse = False 升序（默认）。 Python的模块如何编写和导入模块，以及如何安装并使用第三方模块 模块和包。概念：模块和包。 当我们的代码越来越多的时候: 将所有代码放入一个py文件:难以维护 我们如果将代码分拆放入多个py文件,好处: - 同一个名字变量互不影响 存在于两个文件中的同名变量，函数互不影响。 这时我们称文件a为模块a,b为模块b. 知识点： 模块的名字就是python文件的名字。 引用其他模块方法： 当模块变多之后他们很容易重名。 路人甲与路人乙都写了util.py,产生了冲突。 同名的模块放入不同的包中。就可以了。此时即使有同名的模块，但是他们的完整名字变成了p1.util 和 p2.util，就不冲突了。 引用完整模块 全部都要写完整名称。p1.util p1.util.f(2,10) 包就是文件夹 模块就是xxx.py文件 包可以有多级 区分包和普通目录 包下面必须有一个__init__.py这样一个特殊的文件。每一个包的每一层目录都要有这个文件。 即使这个文件是空文件，也必须让这个文件存在。这样Python才能识别这是一个包。 导入模块要使用一个模块，我们必须首先导入该模块。Python使用import语句导入一个模块。例如，导入系统自带的模块 math： 1import math 你可以认为math就是一个指向已导入模块的变量，通过该变量，我们可以访问math模块中所定义的所有公开的函数、变量和类： 12345&gt;&gt;&gt; math.pow(2, 0.5) # pow是函数1.4142135623730951&gt;&gt;&gt; math.pi # pi是变量3.141592653589793 注意：pow()方法返回 x的y次方 的值。 内置的 pow() 方法 1pow(x, y[, z]) 函数是计算x的y次方，如果z在存在，则再对结果进行取模，其结果等效于pow(x,y) %z但是会更有效率一点。 如果我们只希望导入用到的math模块的某几个函数，而不是所有函数，可以用下面的语句： 1from math import pow, sin, log 这样，可以直接引用 pow, sin, log 这3个函数，但math的其他函数没有导入进来： 1234&gt;&gt;&gt; pow(2, 10)1024.0&gt;&gt;&gt; sin(3.14)0.0015926529164868282 如果遇到名字冲突怎么办？比如math模块有一个log函数，logging模块也有一个log函数，如果同时使用，如何解决名字冲突？ 如果使用import导入模块名，由于必须通过模块名引用函数名，因此不存在冲突： 123import math, loggingprint math.log(10) # 调用的是math的log函数logging.log(10, &apos;something&apos;) # 调用的是logging的log函数 如果使用 from...import 导入 log 函数，势必引起冲突。这时，可以给函数起个“别名”来避免冲突： 1234from math import logfrom logging import log as logger # logging的log现在变成了loggerprint log(10) # 调用的是math的loglogger(10, 'import from logging') # 调用的是logging的log 编程任务 Python的os.path模块提供了 isdir() 和 isfile()函数，请导入该模块，并调用函数判断指定的目录和文件是否存在。 注意: 可以在本机上测试是否存在相应的文件夹和文件。 实现代码: 123import osprint os.path.isdir(r'C:\Windows')print os.path.isfile(r'C:\Windows\notepad.exe') 运行结果： 12TrueTrue 动态导入模块如果导入的模块不存在，Python解释器会报 ImportError 错误： 1234&gt;&gt;&gt; import somethingTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ImportError: No module named something 有的时候，两个不同的模块提供了相同的功能，比如 StringIO 和 cStringIO 都提供了StringIO这个功能。 这是因为Python是动态语言，解释执行，因此Python代码运行速度慢。 如果要提高Python代码的运行速度，最简单的方法是把某些关键函数用 C 语言重写，这样就能大大提高执行速度。 同样的功能，StringIO 是纯Python代码编写的，而 cStringIO 部分函数是 C 写的，因此 cStringIO运行速度更快。 利用ImportError错误，我们经常在Python中动态导入模块： 1234try: from cStringIO import StringIOexcept ImportError: from StringIO import StringIO 上述代码先尝试从cStringIO导入，如果失败了（比如cStringIO没有被安装），再尝试从StringIO导入。这样，如果cStringIO模块存在，则我们将获得更快的运行速度，如果cStringIO不存在，则顶多代码运行速度会变慢，但不会影响代码的正常执行。 try 的作用是捕获错误，并在捕获到指定错误时执行 except 语句。 这里先用一下，更多内容在Python错误与异常处理中讲。 编程任务 利用import ... as ...，还可以动态导入不同名称的模块。Python 2.6/2.7提供了json 模块，但Python 2.5以及更早版本没有json模块，不过可以安装一个simplejson模块，这两个模块提供的函数签名和功能都一模一样。 试写出导入json 模块的代码，能在Python 2.5/2.6/2.7都正常运行。 实现代码: 123456try: import jsonexcept ImportError: import simplejsonprint json.dumps(&#123;'python':2.7&#125;) 运行结果： 12 注：Python的Json模块序列化与反序列化的过程分别是 encoding和 decoding encoding(编码)：把一个Python对象编码转换成Json字符串 decoding(解码)：把Json格式字符串解码转换成Python对象 对于简单数据类型string、unicode、int、float、list、tuple、dict，可以直接处理。 json.dumps方法对简单数据类型encoding 知识点联系对比： 在Python入门中接触的Unicode编码中。 decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(&#39;gb2312&#39;)，表示将gb2312编码的字符串str1转换成unicode编码。 encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gb2312编码 使用futurePython的新版本会引入新的功能，但是，实际上这些功能在上一个老版本中就已经存在了。要“试用”某一新的特性，就可以通过导入__future__模块的某些功能来实现。 例如，Python 2.7的整数除法运算结果仍是整数： 12&gt;&gt;&gt; 10 / 33 python2.7下： 但是，Python 3.x已经改进了整数的除法运算，/除将得到浮点数，//除才仍是整数： 1234&gt;&gt;&gt; 10 / 33.3333333333333335&gt;&gt;&gt; 10 // 33 要在Python 2.7中引入3.x的除法规则，导入__future__的division： 123&gt;&gt;&gt; from __future__ import division&gt;&gt;&gt; print 10 / 33.3333333333333335 当新版本的一个特性与旧版本不兼容时，该特性将会在旧版本中添加到__future__中，以便旧的代码能在旧版本中测试新特性。 编程任务 在Python 3.x中，字符串统一为unicode，不需要加前缀 u，而以字节存储的str则必须加前缀 b。请利用__future__的unicode_literals在Python 2.7中编写unicode字符串。 实现代码: 1234from __future__ import unicode_literalss = 'am I an unicode?'print isinstance(s, unicode) 运行结果： 1True 安装第三方模块虽然内置了许多模块，但是很多有用的第三方包需要我们自己下载。 两种方法安装第三方模块 easy_install pip install(推荐，因为pip内置) 我推荐安装的Anaconda也已经内置了pip 安装体验。 pip install web.py cmd下进入python命令行就可以导入web.py了 查找第三方模块名字： https://pypi.python.org/pypi 在这里可以搜索。 Python面向对象编程如何创建类和实例，如何定义类的属性和方法。 面向对象简介面向对象编程是一种程序设计范式: 把程序看做不同对象的相互调用。 是对于现实世界建立的对象模型。 面向对象编程的基本思想(类和实例): 类：用于定于抽象类型（比如人、汽车、花等抽象的一类事物） 实例：根据类的定义被创建出来的 类的定义： 12class Person: pass 通过类创建具体的实例，通过类名与一个类似于函数调用。 123xiaoming = Person()xiaojun = Person()xiaohong = Person() 比如人的实例有小明、小胡等具体的人，他们有年龄、性别和爱好等不同的属性和方法 面向对象最重要的思想就是：数据封装 在类中把数据封装起来。不同的实例它具有相同的数据类型，但是他拥有不同的属性。 定义类并创建实例在Python中，类通过 class 关键字定义。以 Person 为例，定义一个Person类如下： 12class Person(object): pass 按照 Python 的编程习惯，类名以大写字母开头，紧接着是(object)，表示该类是从哪个类继承下来的。类的继承将在后面的章节讲解，现在我们只需要简单地从object类继承。 有了Person类的定义，就可以创建出具体的xiaoming、xiaohong等实例。创建实例使用 类名+()，类似函数调用的形式创建： 12xiaoming = Person()xiaohong = Person() 编程任务 请练习定义Person类，并创建出两个实例，打印实例，再比较两个实例是否相等。 实现代码: 123456789class Person: passxiaoming = Person()xiaohong = Person()print xiaomingprint xiaohongprint xiaoming==xiaohong 运行结果： 123&lt;__main__.Person instance at 0x7f434c37d050&gt;&lt;__main__.Person instance at 0x7f434c3813f8&gt;False 创建实例属性虽然可以通过Person类创建出xiaoming、xiaohong等实例，但是这些实例看上除了地址不同外，没有什么其他不同。在现实世界中，区分xiaoming、xiaohong要依靠他们各自的名字、性别、生日等属性。 如何让每个实例拥有各自不同的属性？由于Python是动态语言，对每一个实例，都可以直接给他们的属性赋值. 例如，给xiaoming这个实例加上name、gender和birth属性： 1234xiaoming = Person()xiaoming.name = 'Xiao Ming'xiaoming.gender = 'Male'xiaoming.birth = '1990-1-1' 给xiaohong加上的属性不一定要和xiaoming相同： 1234xiaohong = Person()xiaohong.name = 'Xiao Hong'xiaohong.school = 'No. 1 High School'xiaohong.grade = 2 实例的属性可以像普通变量一样进行操作： 1xiaohong.grade = xiaohong.grade + 1 编程任务 请创建包含两个 Person 类的实例的 list，并给两个实例的 name 赋值，然后按照 name 进行排序。 实现代码: 123456789101112131415161718class Person(object): passp1 = Person()p1.name = 'Bart'p2 = Person()p2.name = 'Adam'p3 = Person()p3.name = 'Lisa'L1 = [p1, p2, p3]L2 = sorted(L1,key=lambda x:x.name)print L2[0].nameprint L2[1].nameprint L2[2].name 运行结果： 123AdamBartLisa python3下该代码只需要print加上括号。也可以正常运行。 初始化实例属性虽然我们可以自由地给一个实例绑定各种属性，但是，现实世界中，一种类型的实例应该拥有相同名字的属性。例如，Person类应该在创建的时候就拥有name、gender 和 birth 属性，怎么办？ 在定义 Person 类时，可以为Person类添加一个特殊的__init__()方法被自动调用，我们就能在此为每个实例都统一加上以下属性： 12345class Person(object): def __init__(self, name, gender, birth): self.name = name self.gender = gender self.birth = birth __init__() 方法的第一个参数必须是 self（也可以用别的名字，但建议使用习惯用法），后续参数则可以自由指定，和定义函数没有任何区别。 相应地，创建实例时，就必须要提供除 self 以外的参数： 12xiaoming = Person('Xiao Ming', 'Male', '1991-1-1')xiaohong = Person('Xiao Hong', 'Female', '1992-2-2') 有了__init__()方法，每个Person实例在创建时，都会有 name、gender 和birth 这3个属性，并且，被赋予不同的属性值，访问属性使用.操作符： 1234print xiaoming.name# 输出 'Xiao Ming'print xiaohong.birth# 输出 '1992-2-2' 知识点：要特别注意的是，初学者定义__init__()方法常常忘记了 self 参数： 12345678&gt;&gt;&gt; class Person(object):... def __init__(name, gender, birth):... pass... &gt;&gt;&gt; xiaoming = Person('Xiao Ming', 'Male', '1990-1-1')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: __init__() takes exactly 3 arguments (4 given) 这会导致创建失败或运行不正常，因为第一个参数name被Python解释器传入了实例的引用，从而导致整个方法的调用参数位置全部没有对上。 编程任务 请定义Person类的__init__方法，除了接受 name、gender 和 birth 外，还可接受任意关键字参数，并把他们都作为属性赋值给实例。 实现代码: 123456789101112class Person(object): def __init__(self, name, gender, birth,**kw): self.name = name self.gender = gender self.birth = birth self.__dict__.update(kw) # setattr(self, k, v)也可以 xiaoming = Person('Xiao Ming', 'Male', '1990-1-1', job='Student')print xiaoming.nameprint xiaoming.job 运行结果： 12Xiao MingStudent **kw是一个可变的存入dict的变量。 知识点: self.__dict__.update(kw) 解释器内部会将**kw拆分成对应的dict. setattr()方法接受3个参数：setattr(对象，属性，属性的值) setattr(self,k,v)相当于self.k = v kw.iteritems()历遍字典kw的所有key和value，分别匹配k，v 我们在运行期定义的属性和类定义时定义的属性都被放在了__dict__里。 推荐文章：http://hbprotoss.github.io/posts/python-descriptor.html 访问限制我们可以给一个实例绑定很多属性，如果有些属性不希望被外部访问到怎么办？ Python对属性权限的控制是通过属性名来实现的，如果一个属性由双下划线开头(__)，该属性就无法被外部访问。看例子： 123456789101112131415class Person(object): def __init__(self, name): self.name = name self._title = 'Mr' self.__job = 'Student'p = Person('Bob')print p.name# =&gt; Bobprint p._title# =&gt; Mrprint p.__job# =&gt; ErrorTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Person' object has no attribute '__job' 可见，只有以双下划线开头的__job不能直接被外部访问。 知识点： 但是，如果一个属性以__xxx__的形式定义，那它又可以被外部访问了，以”xxx“定义的属性在Python的类中被称为特殊属性，有很多知识点:预定义的特殊属性可以使用，通常我们不要把普通属性用__xxx__定义。 以单下划线开头的属性_xxx虽然也可以被外部访问，但是，按照习惯，他们不应该被外部访问。 编程任务 请给Person类的__init__方法中添加name和score参数，并把score绑定到__score属性上，看看外部是否能访问到。 实现代码: 123456789101112class Person(object): def __init__(self, name, score): self.name = name self.__score = scorep = Person('Bob', 59)print p.nametry : print p.__scoreexcept AttributeError: print 'attributeerror' 运行结果： 12Bobattributeerror 创建类属性类是模板，而实例则是根据类创建的对象。 知识点: 绑定在一个实例上的属性不会影响其他实例，但是，类本身也是一个对象，如果在类上绑定一个属性，则所有实例都可以访问类的属性，并且，所有实例访问的类属性都是同一个！也就是说，实例属性每个实例各自拥有，互相独立，而类属性有且只有一份。 定义类属性可以直接在 class 中定义： 1234class Person(object): address = 'Earth' def __init__(self, name): self.name = name 因为类属性是直接绑定在类上的，所以，知识点： 访问类属性不需要创建实例，就可以直接访问： 12print Person.address# =&gt; Earth 对一个实例调用类的属性也是可以访问的，所有实例都可以访问到它所属的类的属性： 123456p1 = Person('Bob')p2 = Person('Alice')print p1.address# =&gt; Earthprint p2.address# =&gt; Earth 由于Python是动态语言，类属性也是可以动态添加和修改的： 12345Person.address = &apos;China&apos;print p1.address# =&gt; &apos;China&apos;print p2.address# =&gt; &apos;China&apos; 因为类属性只有一份，所以，当Person类的address改变时，所有实例访问到的类属性都改变了。 编程任务 请给 Person 类添加一个类属性 count，每创建一个实例，count 属性就加 1，这样就可以统计出一共创建了多少个 Person 的实例。 实现代码: 1234567891011121314class Person(object): count = 0 def __init__(self,name): self.name=name Person.count=Person.count+1p1 = Person('Bob')print Person.countp2 = Person('Alice')print Person.countp3 = Person('Tim')print Person.count 运行结果： 123123 类属性和实例属性名字冲突怎么办修改类属性会导致所有实例访问到的类属性全部都受影响，但是，如果在实例变量上修改类属性会发生什么问题呢？ 123456789101112131415class Person(object): address = 'Earth' def __init__(self, name): self.name = namep1 = Person('Bob')p2 = Person('Alice')print 'Person.address = ' + Person.addressp1.address = 'China'print 'p1.address = ' + p1.addressprint 'Person.address = ' + Person.addressprint 'p2.address = ' + p2.address 结果如下： 1234Person.address = Earthp1.address = ChinaPerson.address = Earthp2.address = Earth 我们发现，在设置了 p1.address = &#39;China&#39; 后，p1访问 address 确实变成了 China，但是，Person.address和p2.address仍然是Earth，怎么回事？ 知识点: 原因是 p1.address = &#39;China&#39;并没有改变 Person 的 address，而是给p1这个实例绑定了实例属性address ，对p1来说，它有一个实例属性address（值是’China’），而它所属的类Person也有一个类属性address，所以: 访问 p1.address 时，优先查找实例属性，返回’China’。 访问 p2.address 时，p2没有实例属性address，但是有类属性address，因此返回’Earth’。 知识点：当实例属性和类属性重名时，实例属性优先级高，它将屏蔽掉对类属性的访问。 当我们把 p1 的 address 实例属性删除后，访问 p1.address 就又返回类属性的值 &#39;Earth&#39;了： 123del p1.addressprint p1.address# =&gt; Earth 可见，千万不要在实例上修改类属性，它实际上并没有修改类属性，而是给实例绑定了一个实例属性。 编程任务 请把上节的 Person 类属性 count 改为 __count，再试试能否从实例和类访问该属性。 实现代码: 12345678910111213141516class Person(object): __count = 0 def __init__(self, name): self.name = name Person.__count += 1 print Person.__countp1 = Person('Bob')p2 = Person('Alice')try: print Person.__countexcept AttributeError: print 'attributeerror' 运行结果： 12312attributeerror 定义实例方法一个实例的私有属性就是以__开头的属性，无法被外部访问，那这些属性定义有什么用？ 虽然私有属性无法从外部访问，但是，从类的内部是可以访问的。除了可以定义实例的属性外，还可以定义实例的方法。 实例的方法就是在类中定义的函数，它的第一个参数永远是 self，指向调用该方法的实例本身，其他参数和一个普通函数是完全一样的： 1234567class Person(object): def __init__(self, name): self.__name = name def get_name(self): return self.__name\ get_name(self) 就是一个实例方法，它的第一个参数是self。__init__(self, name)其实也可看做是一个特殊的实例方法。 调用实例方法必须在实例上调用： 123p1 = Person('Bob')print p1.get_name() # self不需要显式传入# =&gt; Bob 在实例方法内部，可以访问所有实例属性，这样，如果外部需要访问私有属性，可以通过方法调用获得，这种数据封装的形式除了能保护内部数据一致性外，还可以简化外部调用的难度。 编程任务 请给 Person 类增加一个私有属性 __score，表示分数，再增加一个实例方法 get_grade()，能根据 __score 的值分别返回 A-优秀, B-及格, C-不及格三档。 实现代码: 1234567891011121314151617181920212223class Person(object): def __init__(self, name, score): self.__name = name self.__score = score def get_grade(self): score = self.__score if score &gt;85: return 'A' elif score &gt;60: return 'B' elif score &lt;60: return 'C'p1 = Person('Bob', 90)p2 = Person('Alice', 65)p3 = Person('Tim', 48)print p1.get_grade()print p2.get_grade()print p3.get_grade() 注：可以在访问数据时顺便通过get_grade做一些其他操作。 运行结果： 123ABC 方法也是属性我们在 class中定义的实例方法其实也是属性，它实际上是一个函数对象： 123456class Person(object): def __init__(self, name, score): self.name = name self.score = score def get_grade(self): return 'A' 12345p1 = Person('Bob', 90)print p1.get_grade# =&gt; &lt;bound method Person.get_grade of &lt;__main__.Person object at 0x109e58510&gt;&gt;print p1.get_grade()# =&gt; A 知识点: 也就是说，p1.get_grade 返回的是一个函数对象，但这个函数是一个绑定到实例的函数，p1.get_grade() 才是方法调用。 知识点；因为方法也是一个属性，所以，它也可以动态地添加到实例上，只是需要用 types.MethodType() 把一个函数变为一个方法： 12345678910111213141516171819202122import typesdef fn_get_grade(self): if self.score &gt;= 80: return 'A' if self.score &gt;= 60: return 'B' return 'C'class Person(object): def __init__(self, name, score): self.name = name self.score = scorep1 = Person('Bob', 90)p1.get_grade = types.MethodType(fn_get_grade, p1, Person)print p1.get_grade()# =&gt; Ap2 = Person('Alice', 65)print p2.get_grade()# ERROR: AttributeError: 'Person' object has no attribute 'get_grade'# 因为p2实例并没有绑定get_grade 知识点: types.MethodType传入三个参数，(方法，对象，类)实现将函数变为对象的一个方法。 给一个实例动态添加方法并不常见，直接在class中定义要更直观。 编程任务 由于属性可以是普通的值对象，如 str，int等，也可以是方法，还可以是函数，大家看看下面代码的运行结果，请想一想 p1.get_grade 为什么是函数而不是方法： 123456789class Person(object): def __init__(self, name, score): self.name = name self.score = score self.get_grade = lambda: 'A'p1 = Person('Bob', 90)print p1.get_gradeprint p1.get_grade() 注： p1.get_grade是属性，只不过这里的属性是一个函数对象，即f p1.get_grade()是方法，前面的p1就是调用这个方法的对象 实现代码: 12345678910class Person(object): def __init__(self, name, score): self.name = name self.score = score self.get_grade = lambda: 'A'p1 = Person('Bob', 90)print p1.get_gradeprint p1.get_grade() 运行结果： 12 at 0x7fa7202fb5f0&gt;A 定义类方法和属性类似，方法也分实例方法和类方法。 在class中定义的全部是实例方法，实例方法第一个参数 self 是实例本身。 要在class中定义类方法，需要这么写： 123456789101112class Person(object): count = 0 @classmethod def how_many(cls): return cls.count def __init__(self, name): self.name = name Person.count = Person.count + 1print Person.how_many()p1 = Person('Bob')print Person.how_many() 知识点: 通过标记一个 @classmethod，该方法将绑定到 Person 类上，而非类的实例。类方法的第一个参数将传入类本身，通常将参数名命名为 cls，上面的 cls.count实际上相当于 Person.count。 知识点: 因为是在类上调用，而非实例上调用，因此类方法无法获得任何实例变量，只能获得类的引用。 编程任务 如果将类属性 count 改为私有属性__count，则外部无法读取__score，但可以通过一个类方法获取，请编写类方法获得__count值。 实现代码: 12345678910111213141516class Person(object): __count = 0 @classmethod def how_many(cls): return cls.__count def __init__(self, name): self.name = name Person.__count = Person.__count + 1 print Person.how_many()p1 = Person('Bob')print Person.how_many() 运行结果： 1201 Python类的继承如何判断实例类型，多态以及如何获取对象信息。 什么是继承继承： 新类不必从头编写， 可以直接从现有类继承，就自动拥有了现有类的所有功能， 新类只需要编写现有类缺少的新功能即可。 继承优点： 复用已有代码 自动拥有了现有类的所有功能 只需要编写缺少的新功能 会形成一棵继承树 继承的特点： 子类和父类是is关系 如果一个实例是一个子类，则它也是一个父类；如果实例是父类，则它不是子类。is关系指的是：小狗是鸟，却不能说动物是小狗 has关系指的是：学生有一本书，不能说学生是一本书 两个has关系的类不能继承，只能以属性组合到类中. 123class Student(Person): def __init__(self , bookName): self.book=Book(bookName) 继承特点： 总是从某个类继承，没有合适的类时使用object类继承 调用super().__init__方法（用来初始化父类），如果不用父类的属性有可能没有被正确初始化。 Student类从父类继承name和gender： super(Student,self).__init__(name,gender)调用这一句来初始化父类。 继承一个类如果已经定义了Person类，需要定义新的Student和Teacher类时，可以直接从Person类继承： 1234class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender 定义Student类时，只需要把额外的属性加上，例如score： 1234class Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = score 知识点：一定要用 super(Student, self).__init__(name, gender)去初始化父类，否则，继承自 Person的 Student 将没有 name 和 gender。 函数super(Student, self)将返回当前类继承的父类，即 Person ，然后调用__init__()方法，注意self参数已在super()中传入，在init()中将隐式传递，不需要写出（也不能写）。 编程任务 请参考 Student 类，编写一个 Teacher类，也继承自 Person。 实现代码: 123456789101112131415class Person(object): def __init__(self, name, gender): self.name = name self.gender = genderclass Teacher(Person): def __init__(self, name, gender, course): super(Teacher, self).__init__(name, gender) self.course = course t = Teacher('Alice', 'Female', 'English')print t.nameprint t.course 运行结果： 12AliceEnglish 判断类型函数isinstance()可以判断一个变量的类型，既可以用在Python内置的数据类型如str、list、dict，也可以用在我们自定义的类，它们本质上都是数据类型。 假设有如下的 Person、Student 和 Teacher 的定义及继承关系如下： 123456789101112131415161718class Person(object): def __init__(self, name, gender): self.name = name self.gender = genderclass Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = scoreclass Teacher(Person): def __init__(self, name, gender, course): super(Teacher, self).__init__(name, gender) self.course = coursep = Person('Tim', 'Male')s = Student('Bob', 'Male', 88)t = Teacher('Alice', 'Female', 'English') 当我们拿到变量 p、s、t 时，可以使用 isinstance 判断类型： 123456&gt;&gt;&gt; isinstance(p, Person)True # p是Person类型&gt;&gt;&gt; isinstance(p, Student)False # p不是Student类型&gt;&gt;&gt; isinstance(p, Teacher)False # p不是Teacher类型 这说明在继承链上，一个父类的实例不能是子类类型，因为子类比父类多了一些属性和方法。 我们再考察 s： 123456&gt;&gt;&gt; isinstance(s, Person)True # s是Person类型&gt;&gt;&gt; isinstance(s, Student)True # s是Student类型&gt;&gt;&gt; isinstance(s, Teacher)False # s不是Teacher类型 s是Student类型，不是Teacher类型，这很容易理解。但是，s 也是Person类型，因为Student继承自Person，虽然它比Person多了一些属性和方法，但是，把 s 看成Person的实例也是可以的。 这说明在一条继承链上，一个实例可以看成它本身的类型，也可以看成它父类的类型。 编程任务 请根据继承链的类型转换，依次思考 t是否是 Person，Student，Teacher，object类型，并使用isinstance()判断来验证您的答案。 实现代码: 123456789101112131415161718192021222324class Person(object): def __init__(self, name, gender): self.name = name self.gender = genderclass Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = scoreclass Teacher(Person): def __init__(self, name, gender, course): super(Teacher, self).__init__(name, gender) self.course = courset = Teacher('Alice', 'Female', 'English')print isinstance(t, Person)print isinstance(t, Student)print isinstance(t, Teacher)print isinstance(t, object) 运行结果： 1234TrueFalseTrueTrue 多态类具有继承关系，并且子类类型可以向上转型看做父类类型，如果我们从 Person 派生出Student和Teacher ，并都写了一个whoAmI() 方法： 1234567891011121314151617181920class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def whoAmI(self): return 'I am a Person, my name is %s' % self.nameclass Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = score def whoAmI(self): return 'I am a Student, my name is %s' % self.nameclass Teacher(Person): def __init__(self, name, gender, course): super(Teacher, self).__init__(name, gender) self.course = course def whoAmI(self): return 'I am a Teacher, my name is %s' % self.name 在一个函数中，如果我们接收一个变量 x，则无论该 x 是 Person、Student还是 Teacher，都可以正确打印出结果： 12345678910def who_am_i(x): print x.whoAmI()p = Person('Tim', 'Male')s = Student('Bob', 'Male', 88)t = Teacher('Alice', 'Female', 'English')who_am_i(p)who_am_i(s)who_am_i(t) 运行结果： 123I am a Person, my name is TimI am a Student, my name is BobI am a Teacher, my name is Alice 知识点：这种行为称为多态。也就是说，方法调用将作用在 x的实际类型上。s 是Student类型，它实际上拥有自己的 whoAmI()方法以及从 Person继承的 whoAmI方法，但调用 s.whoAmI()总是先查找它自身的定义，如果没有定义，则顺着继承链向上查找，直到在某个父类中找到为止。 由于Python是动态语言，所以，传递给函数 who_am_i(x)的参数 x 不一定是 Person 或 Person 的子类型。任何数据类型的实例都可以，只要它有一个whoAmI()的方法即可： 123class Book(object): def whoAmI(self): return 'I am a book' 知识点：这是动态语言和静态语言（例如Java）最大的差别之一。动态语言调用实例方法，不检查类型，只要方法存在，参数正确，就可以调用。 编程任务 Python提供了open()函数来打开一个磁盘文件，并返回 File对象。File对象有一个read()方法可以读取文件内容： 例如，从文件读取内容并解析为JSON结果： 123import jsonf = open('/path/to/file.json', 'r')print json.load(f) 由于Python的动态特性，json.load()并不一定要从一个File对象读取内容。任何对象，只要有read()方法，就称为File-like Object，都可以传给json.load()。 请尝试编写一个File-like Object，把一个字符串 r&#39;[&quot;Tim&quot;, &quot;Bob&quot;, &quot;Alice&quot;]&#39;包装成 File-like Object 并由 json.load() 解析。 实现代码: 123456789101112import jsonclass Students(object): def __init__(self, strlist): self.strlist = strlist def read(self): return(self.strlist)s = Students('["Tim", "Bob", "Alice"]')print json.load(s) 运行结果： 1[u&apos;Tim&apos;, u&apos;Bob&apos;, u&apos;Alice&apos;] 多重继承除了从一个父类继承外，Python允许从多个父类继承，称为多重继承。 多重继承的继承链就不是一棵树了，它像这样： 12345678910111213141516171819class A(object): def __init__(self, a): print 'init A...' self.a = aclass B(A): def __init__(self, a): super(B, self).__init__(a) print 'init B...'class C(A): def __init__(self, a): super(C, self).__init__(a) print 'init C...'class D(B, C): def __init__(self, a): super(D, self).__init__(a) print 'init D...' 看下图: 像这样，D 同时继承自 B和 C，也就是 D 拥有了 A、B、C的全部功能。多重继承通过 super()调用__init__()方法时，A 虽然被继承了两次，但__init__()只调用一次： 12345&gt;&gt;&gt; d = D('d')init A...init C...init B...init D... 多重继承的目的是从两种继承树中分别选择并继承出子类，以便组合功能使用。 举个例子，Python的网络服务器有TCPServer、UDPServer、UnixStreamServer、UnixDatagramServer，而服务器运行模式有 多进程ForkingMixin 和 多线程ThreadingMixin两种。 要创建多进程模式的 TCPServer： 12class MyTCPServer(TCPServer, ForkingMixin) pass 要创建多线程模式的 UDPServer： 12class MyUDPServer(UDPServer, ThreadingMixin): pass 如果没有多重继承，要实现上述所有可能的组合需要 4x2=8 个子类。 编程任务1234567891011+-Person +- Student +- Teacher是一类继承树；+- SkillMixin +- BasketballMixin +- FootballMixin是一类继承树。 通过多重继承，请定义“会打篮球的学生”和“会踢足球的老师”。 实现代码: 12345678910111213141516171819202122232425262728293031class Person(object): passclass Student(Person): passclass Teacher(Person): passclass SkillMixin(object): passclass BasketballMixin(SkillMixin): def skill(self): return 'basketball'class FootballMixin(SkillMixin): def skill(self): return 'football'class BStudent(BasketballMixin,Student): passclass FTeacher(FootballMixin,Teacher): passs = BStudent()print s.skill()t = FTeacher()print t.skill() 运行结果： 12basketballfootball 获取对象信息(知识点)拿到一个变量，除了用isinstance()判断它是否是某种类型的实例外，还有没有别的方法获取到更多的信息呢？ 例如，已有定义： 1234567891011class Person(object): def __init__(self, name, gender): self.name = name self.gender = genderclass Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = score def whoAmI(self): return 'I am a Student, my name is %s' % self.name 首先可以用 type() 函数获取变量的类型，它返回一个 Type 对象： 12345&gt;&gt;&gt; type(123)&lt;type 'int'&gt;&gt;&gt;&gt; s = Student('Bob', 'Male', 88)&gt;&gt;&gt; type(s)&lt;class '__main__.Student'&gt; 其次，可以用 dir() 函数获取变量的所有属性： 12345&gt;&gt;&gt; dir(123) # 整数也有很多属性...['__abs__', '__add__', '__and__', '__class__', '__cmp__', ...]&gt;&gt;&gt; dir(s)['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'gender', 'name', 'score', 'whoAmI'] 对于实例变量，dir()返回所有实例属性，包括__class__这类有特殊意义的属性。注意到方法whoAmI也是 s 的一个属性。 如何去掉__xxx__这类的特殊属性，只保留我们自己定义的属性？回顾一下filter()函数的用法。 dir()返回的属性是字符串列表，如果已知一个属性名称，要获取或者设置对象的属性，就需要用 getattr() 和 setattr( )函数了： 123456789101112131415&gt;&gt;&gt; getattr(s, 'name') # 获取name属性'Bob'&gt;&gt;&gt; setattr(s, 'name', 'Adam') # 设置新的name属性&gt;&gt;&gt; s.name'Adam'&gt;&gt;&gt; getattr(s, 'age') # 获取age属性，但是属性不存在，报错：Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'age'&gt;&gt;&gt; getattr(s, 'age', 20) # 获取age属性，如果属性不存在，就返回默认值20：20 编程任务对于Person类的定义： 12345class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender` 希望除了 name和gender 外，可以提供任意额外的关键字参数，并绑定到实例，请修改 Person 的 __init__()定 义，完成该功能。 实现代码: 12345678910class Person(object): def __init__(self, name, gender, **kw): for k,v in kw.items(): setattr(self, k, v)p = Person('Bob', 'Male', age=18, course='Python')print p.ageprint p.course 运行结果： 1218Python Python的特殊方法(魔术方法)以及如何利用特殊方法定制类，实现各种强大的功能。 什么是特殊方法？demo： 比较print的结果： 12&gt;&gt;&gt; print lst[1,2,3] 对于一个Person类的实例进行print，会得到一个对象地址。 python如何把任意变量变成str? python能够将任意变量变成str是因为任何数据类型的实例都有一个特殊方法：__str__()思考：Person类并没有定义__str__()，为什么能调用？ 对应调用list和Person实例的__str__()方法。 如果我们给Person类加上特殊方法__str__()，我们就可以根据自己需要打印出Person实例来。 用于print的str 用于len的len 用于cmp的cmp python的特殊方法特点： 特殊方法定义在class中 不需要直接调用 python的某些函数或操作符会自动调用对应的特殊方法。 Python定义的一部分特殊方法: 正确实现特殊方法： 只需编写用到的特殊方法 有关联性的特殊方法都必须实现 如： 123__getattr____setattr____delattr__ 这三个方法需要同时编写 __str__和__repr__如果要把一个类的实例变成 str，就需要实现特殊方法__str__()： 123456class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __str__(self): return '(Person: %s, %s)' % (self.name, self.gender) 现在，在交互式命令行下用 print 试试： 123&gt;&gt;&gt; p = Person('Bob', 'male')&gt;&gt;&gt; print p(Person: Bob, male) 但是，如果直接敲变量 p： 12&gt;&gt;&gt; p&lt;main.Person object at 0x10c941890&gt; 似乎__str__()不会被调用。 知识点: 因为 Python 定义了__str__()和__repr__()两种方法，__str__()用于显示给用户，而__repr__()用于显示给开发人员。 有一个偷懒的定义__repr__的方法： 1234567class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __str__(self): return '(Person: %s, %s)' % (self.name, self.gender) __repr__ = __str__ 编程任务请给Student 类定义__str__和__repr__方法，使得能打印出&lt;Student: name, gender, score&gt;： 1234class Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = score 实现代码: 1234567891011121314151617class Person(object): def __init__(self, name, gender): self.name = name self.gender = genderclass Student(Person): def __init__(self, name, gender, score): super(Student, self).__init__(name, gender) self.score = score def __str__(self): return '(Student: %s, %s, %s)' % (self.name, self.gender, self.score)s = Student('Bob', 'male', 88)print s 运行结果： 1(Student: Bob, male, 88) __cmp__对 int、str 等内置数据类型排序时，Python的 sorted() 按照默认的比较函数 cmp排序，但是，如果对一组 Student 类的实例排序时，就必须提供我们自己的特殊方法 __cmp__()： 123456789101112131415class Student(object): def __init__(self, name, score): self.name = name self.score = score def __str__(self): return '(%s: %s)' % (self.name, self.score) __repr__ = __str__ def __cmp__(self, s): if self.name &lt; s.name: return -1 elif self.name &gt; s.name: return 1 else: return 0 上述 Student 类实现了__cmp__()方法，__cmp__用实例自身self和传入的实例s进行比较，如果 self 应该排在前面，就返回 -1，如果 s应该排在前面，就返回1，如果两者相当，返回 0。 Student类实现了按name进行排序： 123&gt;&gt;&gt; L = [Student('Tim', 99), Student('Bob', 88), Student('Alice', 77)]&gt;&gt;&gt; print sorted(L)[(Alice: 77), (Bob: 88), (Tim: 99)] 注意: 如果list不仅仅包含 Student 类，则 __cmp__ 可能会报错： 12L = [Student('Tim', 99), Student('Bob', 88), 100, 'Hello']print sorted(L) 请思考如何解决。(未完待续) 编程任务 请修改 Student 的 __cmp__ 方法，让它按照分数从高到底排序，分数相同的按名字排序。 实现代码: 12345678910111213141516171819202122232425262728293031323334class Student(object): def __init__(self, name, score): self.name = name self.score = score def __str__(self): return '(%s: %s)' % (self.name, self.score) __repr__ = __str__ def __cmp__(self, s): if self.score&gt;s.score: return -1 elif self.score&lt;s.score: return 1 elif self.name&lt;s.name: return -1 elif self.nae&gt;s.name: return 1 else: return 0L = [Student('Tim', 99), Student('Bob', 88), Student('Alice', 99)]print sorted(L) 运行结果： 1[(Alice: 99), (Tim: 99), (Bob: 88)] __len__如果一个类表现得像一个list，要获取有多少个元素，就得用 len() 函数。 要让 len() 函数工作正常，类必须提供一个特殊方法__len__()，它返回元素的个数。 例如，我们写一个 Students 类，把名字传进去： 12345class Students(object): def __init__(self, *args): self.names = args def __len__(self): return len(self.names) 只要正确实现了__len__()方法，就可以用len()函数返回Students实例的“长度”： 123&gt;&gt;&gt; ss = Students(&apos;Bob&apos;, &apos;Alice&apos;, &apos;Tim&apos;)&gt;&gt;&gt; print len(ss)3 编程任务(天涯)斐波那契数列是由 0, 1, 1, 2, 3, 5, 8...构成。 请编写一个Fib类，Fib(10)表示数列的前10个元素，print Fib(10)可以打印出数列的前 10 个元素，len(Fib(10))可以正确返回数列的个数10。 实现代码: 12345678910111213141516171819class Fib(object): def __init__(self, num): self.num = num self.fibo = [0,1] i = 2 while i &lt; self.num: self.fibo.append(self.fibo[i-2] + self.fibo[i-1]) i = i + 1 def __str__(self): return str(self.fibo) def __len__(self): return len(self.fibo)f = Fib(10)print fprint len(f) 运行结果： 12[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]10 数学运算Python 提供的基本数据类型 int、float以做整数和浮点的四则运算以及乘方等运算。 但是，四则运算不局限于int和float，还可以是有理数、矩阵等。 要表示有理数，可以用一个Rational类来表示： 1234class Rational(object): def __init__(self, p, q): self.p = p self.q = q p、q 都是整数，表示有理数 p/q。 如果要让Rational进行+运算，需要正确实现__add__： 123456789class Rational(object): def __init__(self, p, q): self.p = p self.q = q def __add__(self, r): return Rational(self.p * r.q + self.q * r.p, self.q * r.q) def __str__(self): return '%s/%s' % (self.p, self.q) __repr__ = __str__ 现在可以试试有理数加法： 1234&gt;&gt;&gt; r1 = Rational(1, 3)&gt;&gt;&gt; r2 = Rational(1, 2)&gt;&gt;&gt; print r1 + r25/6 编程任务(天涯) Rational类虽然可以做加法，但无法做减法、乘方和除法，请继续完善Rational类，实现四则运算。 提示： 123减法运算：__sub__乘法运算：__mul__除法运算：__div__ 实现代码: 12345678910111213141516171819202122232425262728293031323334353637383940def gcs(a,b,c=1): if 0==a%2 and 0==b%2: return gcs(a/2,b/2,c*2); s = abs(a-b) m = min(a,b) if s == m: return m*c return gcs(s,m,c) class Rational(object): def __init__(self, p, q): self.p = p self.q = q def __add__(self, r): return Rational(self.p * r.q + self.q * r.p, self.q * r.q) def __sub__(self, r): return Rational(self.p * r.q - self.q * r.p, self.q * r.q) def __mul__(self, r): return Rational(self.p * r.p, self.q * r.q) def __div__(self, r): return Rational(self.p * r.q , self.q * r.p) def __str__(self): c = gcs(self.p, self.q) return '%s/%s' % (self.p/c, self.q/c) __repr__ = __str__r1 = Rational(1, 2)r2 = Rational(1, 4)print r1 + r2print r1 - r2print r1 * r2print r1 / r2 推荐阅读评论区：https://www.imooc.com/code/6253 运行结果： 12343/41/41/82/1 类型转换Rational类实现了有理数运算，但是，如果要把结果转为 int 或 float 怎么办？ 考察整数和浮点数的转换： 1234&gt;&gt;&gt; int(12.34)12&gt;&gt;&gt; float(12)12.0 如果要把 Rational 转为 int，应该使用： 12r = Rational(12, 5)n = int(r) 要让int()函数正常工作，只需要实现特殊方法__int__(): 123456class Rational(object): def __init__(self, p, q): self.p = p self.q = q def __int__(self): return self.p // self.q 结果如下： 1234&gt;&gt;&gt; print int(Rational(7, 2))3&gt;&gt;&gt; print int(Rational(1, 3))0 同理，要让float()函数正常工作，只需要实现特殊方法__float__()。 编程任务 请继续完善Rational，使之可以转型为float。 实现代码: 12345678910111213class Rational(object): def __init__(self, p, q): self.p = p self.q = q def __int__(self): return self.p // self.q def __float__(self): return float(self.p)/self.qprint float(Rational(7, 2))print float(Rational(1, 3)) 运行结果： 123.50.333333333333 @property考察 Student 类： 1234class Student(object): def __init__(self, name, score): self.name = name self.score = score 当我们想要修改一个 Student 的 scroe 属性时，可以这么写： 12s = Student(&apos;Bob&apos;, 59)s.score = 60 但是也可以这么写： 1s.score = 1000 显然，直接给属性赋值无法检查分数的有效性。 如果利用两个方法： 12345678910class Student(object): def __init__(self, name, score): self.name = name self.__score = score def get_score(self): return self.__score def set_score(self, score): if score &lt; 0 or score &gt; 100: raise ValueError(&apos;invalid score&apos;) self.__score = score 这样一来，s.set_score(1000) 就会报错。 这种使用 get/set 方法来封装对一个属性的访问在许多面向对象编程的语言中都很常见。 知识点: 但是写 s.get_score() 和 s.set_score() 没有直接写 s.score 来得直接。有没有两全其美的方法？—-有。 因为Python支持高阶函数，在函数式编程中我们介绍了装饰器函数，可以用装饰器函数把 get/set 方法“装饰”成属性调用： 123456789101112class Student(object): def __init__(self, name, score): self.name = name self.__score = score @property def score(self): return self.__score @score.setter def score(self, score): if score &lt; 0 or score &gt; 100: raise ValueError('invalid score') self.__score = score 注意: 第一个score(self)是get方法，用@property装饰，第二个score(self, score)是set方法，用@score.setter装饰，@score.setter是前一个@property装饰后的副产品。 现在，就可以像使用属性一样设置score了： 12345678&gt;&gt;&gt; s = Student('Bob', 59)&gt;&gt;&gt; s.score = 60&gt;&gt;&gt; print s.score60&gt;&gt;&gt; s.score = 1000Traceback (most recent call last): ...ValueError: invalid score 说明对 score 赋值实际调用的是 set方法。 编程任务 如果没有定义set方法，就不能对“属性”赋值，这时，就可以创建一个只读“属性”。请给Student类加一个grade属性，根据 score计算 A（&gt;=80）、B、C（&lt;60）。 实现代码: 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/env python# -*- coding: utf-8 -*-# @Date : 2018-01-04 07:44:12# @Author : Your Name (you@example.org)# @Link : http://example.org# @Version : $Id$class Student(object): def __init__(self, name, score): self.name = name self.__score = score @property def score(self): return self.__score @score.setter def score(self, score): if score &lt; 0 or score &gt; 100: raise ValueError('invalid score') self.__score = score @property def grade(self): if self.score &gt;= 80: return "A" elif self.score &gt;=60: return "B" else: return "C"s = Student('Bob', 59)print s.grades.score = 60print s.grades.score = 99print s.grade @property,可以将python定义的函数“当做”属性访问，从而提供更加友好访问方式注意self.score 不要写成score 会报错。 运行结果： 123CBA __slots__由于Python是动态语言，任何实例在运行期都可以动态地添加属性。 知识点：如果要限制添加的属性，例如，Student类只允许添加 name、gender和score 这3个属性，就可以利用Python的一个特殊的__slots__来实现。 顾名思义，__slots__是指一个类允许的属性列表： 123456class Student(object): __slots__ = ('name', 'gender', 'score') def __init__(self, name, gender, score): self.name = name self.gender = gender self.score = score 现在，对实例进行操作： 1234567&gt;&gt;&gt; s = Student(&apos;Bob&apos;, &apos;male&apos;, 59)&gt;&gt;&gt; s.name = &apos;Tim&apos; # OK&gt;&gt;&gt; s.score = 99 # OK&gt;&gt;&gt; s.grade = &apos;A&apos;Traceback (most recent call last): ...AttributeError: &apos;Student&apos; object has no attribute &apos;grade&apos; __slots__的目的是限制当前类所能拥有的属性，如果不需要添加任意动态的属性，使用__slots__也能节省内存。 编程任务 假设Person类通过__slots__定义了name和gender，请在派生类Student中通过_slots__继续添加score的定义，使Student类可以实现name、gender和score 3个属性。 实现代码: 123456789101112131415161718192021class Person(object): __slots__ = ('name', 'gender') def __init__(self, name, gender): self.name = name self.gender = genderclass Student(Person): __slots__ = ('score') def __init__(self,name,gender,score): self.name = name self.score = score self.gender = gender s = Student('Bob', 'male', 59)s.name = 'Tim's.score = 99print s.score 运行结果： 199 __call__在Python中，函数其实是一个对象： 12345&gt;&gt;&gt; f = abs&gt;&gt;&gt; f.__name__'abs'&gt;&gt;&gt; f(-123)123 由于 f 可以被调用，所以，f 被称为可调用对象。 所有的函数都是可调用对象。 一个类实例也可以变成一个可调用对象，只需要实现一个特殊方法__call__()。 我们把 Person 类变成一个可调用对象： 12345678class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __call__(self, friend): print 'My name is %s...' % self.name print 'My friend is %s...' % friend 现在可以对 Person 实例直接调用： 1234&gt;&gt;&gt; p = Person(&apos;Bob&apos;, &apos;male&apos;)&gt;&gt;&gt; p(&apos;Tim&apos;)My name is Bob...My friend is Tim... 单看 p(&#39;Tim&#39;)你无法确定 p 是一个函数还是一个类实例，所以，在Python中，函数也是对象，对象和函数的区别并不显著。 编程任务 改进一下前面定义的斐波那契数列： 12class Fib(object): ??? 请加一个__call__方法，让调用更简单： 123&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; print f(10)[0, 1, 1, 2, 3, 5, 8, 13, 21, 34] 实现代码: 12345678910class Fib(object): def __call__(self,num): a,b,c = 0,1,[] for n in range(num): c.append(a) a,b = b,a+b return cf = Fib()print f(10) 运行结果： 1[0, 1, 1, 2, 3, 5, 8, 13, 21, 34] 课程总结Python的函数式编程： 函数式与函数的区别。 支持高阶函数： 函数本身可作为变量传入另一个函数。 抽象度很高的模型。 内置函数map sorted reduce filter中都有高阶函数应用。 闭包：利用闭包来返回函数 匿名函数：限制,匿名函数只能有一个表达式：表达式的结果就是匿名函数的返回值。 装饰器：高阶函数的应用，可以把一个函数装饰成另一个函数。以便能在运行时动态增加函数功能。 Python的模块和包： 模块的目的：避免名字冲突。 包可以看成具有目录层次的模块(__init__.py) 引用模块 引用future添加新版本中的特性。 Python的面向对象编程: 类和实例(类是模板，实例是根据模板创健的对象) 属性和方法。(可以把属性看成是一个方法？)，绑定到实例的函数对象。 区分类属性和实例属性(优先级，尽量不要重名) 类的继承中： 继承的概念和目的(代码复用) 多态（从一个父类派生出多个子类。可以使子类具有各自不同的行为。） 多重继承 定制类： 定制类的目的：为了让我们编写的类应用到普通函数中。len() cmp() 特殊方法 类型转换(把任意类型转化为string类型，int类型) __call__方法：把实例变成一个可以跟函数一样调用的对象。 任何数据类型的实例都有一个特殊方法__str__()类似于toString 特殊方法的特征： 特殊方法定义在class中 不需要直接调用 Python的某些函数或者操作符会调用对应的特殊方法 正确实现特殊方法： 只需要编写用到的特殊方法 有关联性的特殊方法都必须实现： 123如 __getattr__ __setattr__ __delattr__ python中__str__和__repr__如果要把一个类的实例变成 str，就需要实现特殊方法__str__()： 123456class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __str__(self): return '(Person: %s, %s)' % (self.name, self.gender) __str__()用于显示给用户，而__repr__()用于显示给开发人员。 有一个偷懒的定义__repr__的方法： 12#正常定义完__str__后__repr__ = __str__ #把__str__赋予__repr__ 特殊方法之 __cmp__对一组 Student 类的实例排序时，就必须提供我们自己的特殊方法 __cmp__()： 1234567def __cmp__(self, s): if self.name &lt; s.name: return -1 elif self.name &gt; s.name: return 1 else: return 0 上述 Student 类实现了__cmp__()方法，__cmp__用实例自身self和传入的实例 s进行比较. 在class中定义__init__(self),__float__(self)方法：数据类型转换 python中 @propertyget/set 方法来封装对一个属性的访问用装饰器函数把 get/set 方法“装饰”成属性调用 @property和@method.setter是搭配使用的，@property对应get方法（相当于只读不可更改），@method.setter对应set方法。 python中__slots__任何实例在运行期都可以动态地添加属性如果要限制添加的属性，例如，Student类只允许添加 name、gender和score 这3个属性，就可以利用Python的一个特殊的__slots__来实现。 1__slots__ = (&apos;name&apos;, &apos;gender&apos;, &apos;score&apos;) 顾名思义，__slots__是指一个类允许的属性列表 子类继续父类的场合：__slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的。除非在子类中也定义__slots__，此时，子类允许定义的属性就是自身的__slots__加上父类中定义的__slots__。 下一步进阶： IO：文件和Socket 多任务：进程和线程 数据库 Web开发]]></content>
      <categories>
        <category>python从入门到精通</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>学习笔记</tag>
        <tag>进阶学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python零基础入门笔记]]></title>
    <url>%2Fpost%2F7a862501.html</url>
    <content type="text"><![CDATA[复习是为了更好的学习更新的知识。 学习python有一年多了，希望通过学习笔记来复习了，也能让后来者少走一点弯路。在课程笔记的同时加入了一部分自己的经验补充。 [√] 廖雪峰老师在慕课网的课程: Python入门 Python的初次体验python语言介绍全世界有几百种编程语言，但是流行的只有十几种，python就是其中一种。荷兰人龟叔于1989年圣诞节创立。 特点：优雅，明确，简单。 适合的领域： web网站和各种网络服务； 系统工具和脚本； 作为胶水语言把其他语言开发的模块包装起来方便使用。 Python是一门高级语言，所以不适合贴近硬件的代码: 比如驱动程序（首选C） 移动开发，有各自的语言，（objectC，swift/java） 游戏开发（首选C/C++）。 Python实际应用： YouTube，豆瓣，搜狐邮箱；Openstack开源云计算平台。Google，Yahoo，NASA。 语言之间的对比： C编译为机器码；JAVA编译为字节码；python为解释执行。 缺点： 运行慢，Python源码不能加密。 Python版本的选择博主建议选择安装环境篇的进阶版：2.7版本与3.x版本共存。 3.x版本建议选择Python 3.5.1 |Anaconda 4.1.0 (64-bit)以后如果要使用python进行TensorFlow windows版的配置可以省下时间。 windows下安装python参考：搭建Python开发环境 第一个python程序cmd下输入python。进入交互式环境。 命令行模式启动python:python 命令行模式执行python文件python 目录/xxx.py 命令行模式关闭python：exit() 注意：不要使用word，或者windows下自带的记事本来进行代码编写。 推荐使用： 轻量级：sublime Text 或 editplus 重量级(较大工程) : pycharm Professional 2.7版本专属： print &#39;hello,world!&#39; 3.x版本(2.7版本也可以正常运行)： print (&quot;hello,world!&quot;) Python变量和数据类型 讲解Python基本的数据类型.包括整数、浮点数、字符串和布尔类型，以及变量的概念和基本的数据运算。 数据类型整数在Python程序中，整数的表示方法和数学上的写法一模一样. 例如：1，100，-8080，0，等等。十六进制用0x前缀和0-9，a-f表示. 例如：0xff00，0xa5b4c3d2，等等。 浮点数浮点数也就是小数，之所以称为浮点数: 因为按照科学记数法表示时，一个浮点数的小数点位置是可变的 比如，1.23x10^9和12.3x10^8是相等的。 浮点数可以用数学写法: 如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x10^9就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。 整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差。 知识点：python2与3不同整除 python2.7下：/ 和 // 都是整数除法。 例: 1/2结果为0.后面小数部分会直接去除掉。 python3.x下： / 为浮点数除法(如：1/2=0.5) //为整数除法(如: 1//2 = 0） 12345a = 1 b = 2print a+b#python2.7下想要浮点数除法就得使用类型转换。print float(a)/b 字符串字符串是以’’或””括起来的任意文本，比如’abc’，”xyz”等等。请注意，’’或””本身只是一种表示方式，不是字符串的一部分. 因此，字符串’abc’只有a，b，c这3个字符。 布尔值布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来。 布尔值可以用and、or和not运算。 and运算是与运算，只有所有都为 True，and运算结果才是 True。 or运算是或运算，只要其中有一个为 True，or 运算结果就是 True。 not运算是非运算，它是一个单目运算符，把 True 变成 False，False 变成 True。 空值空值是Python里一个特殊的值，用None表示。 None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 编程小任务： 计算十进制整数 45678 和十六进制整数 0x12fd2 之和。 请用字符串表示出Learn Python in imooc。 请计算以下表达式的布尔值（注意==表示判断是否相等）：12100 &lt; 990xff == 255 题目答案： 1234print 45678+0x12fd2print "Learn Python in imooc" print 100&lt;99 print 0xff == 255 运行结果： 1234123456Learn Python in imoocFalseTrue print语句print语句可以向屏幕上输出指定的文字。比如输出’hello, world’，用代码实现如下： 1print 'hello, world' 注意： 当我们在Python交互式环境下编写代码时，&gt;&gt;&gt;是Python解释器的提示符，不是代码的一部分。 当我们在文本编辑器中编写代码时，千万不要自己添加 &gt;&gt;&gt;。 print语句也可以跟上多个字符串，用逗号,隔开，就可以连成一串输出： 1print 'The quick brown fox', 'jumps over', 'the lazy dog' 运行结果： 1The quick brown fox jumps over the lazy dog print会依次打印每个字符串，知识点：遇到逗号,会输出一个空格. print也可以打印整数，或者计算结果： 1234&gt;&gt;&gt; print 300300 #运行结果&gt;&gt;&gt; print 100 + 200300 #运行结果 漂亮做法： 12&gt;&gt;&gt; print '100 + 200 =', 100 + 200100 + 200 = 300 #运行结果 注意: 对于100 + 200，Python解释器自动计算出结果300.但是，’100 + 200 =’是字符串而非数学公式，Python把它视为字符串. 编程任务：请用两种方式打印出 hello, python.实现代码： 123#input codeprint 'hello, python.'print 'hello,','python.' 运行结果： 12hello, python.hello, python. 注释Python的注释以#开头，后面的文字直到行尾都算注释 12345# 这一行全部都是注释...print 'hello' # 这也是注释# 暂时不想运行下面一行代码:# print 'hello, python.' 注释还有一个巧妙的用途，就是一些代码我们不想运行，但又不想删除，就可以用注释暂时屏蔽掉： 编程任务:将代码编辑器中的 “print ‘hello’” 语句修改成注释语句 实现代码： 1# print 'hello' 注释：多行注释1234'''下面是一行被注释代码下面是两行被注释代码''' 什么是变量在Python中，变量的概念基本上和初中代数的方程变量是一致的。 例如，对于方程式y=x*x ，x就是变量。 当x=2时，计算结果是4。当x=5时，计算结果是25。 只是在计算机程序中，变量不仅可以是数字，还可以是任意数据类型。 在Python程序中，变量是用一个变量名表示。 知识点：变量名必须是大小写英文、数字和下划线 _ 的组合，且不能用数字开头。比如： 12a = 1t_007 = 'T007' 变量a是一个整数。变量t_007是一个字符串。 在Python中，等号=是赋值语句，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量，例如： 1234a = 123 # a是整数print aa = 'imooc' # a变为字符串print a 知识点: 这种变量本身类型不固定的语言称之为动态语言，与之对应的是静态语言。 静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。例如Java是静态语言，赋值语句如下（// 表示注释）： 123//这些是java代码int a = 123; // a是整数类型变量a = "mooc"; // 错误：不能把字符串赋给整型变量 和静态语言相比，动态语言更灵活，就是这个原因。请不要把赋值语句的等号等同于数学的等号。比如下面的代码： 12x = 10x = x + 2 如果从数学上理解x = x + 2那无论如何是不成立的. 在程序中，赋值语句先计算右侧的表达式x + 2，得到结果12，再赋给变量x。由于x之前的值是10，重新赋值后，x的值变成12。 最后，知识点: 理解变量在计算机内存中的表示也非常重要。当我们写：a = ‘ABC’时，Python解释器干了两件事情： 在内存中创建了一个’ABC’的字符串； 在内存中创建了一个名为a的变量，并把它指向’ABC’。 也可以把一个变量a赋值给另一个变量b，这个操作实际上是把变量b指向变量a所指向的数据，例如下面的代码： 1234a = 'ABC'b = aa = 'XYZ'print b 最后一行打印出变量b的内容到底是’ABC’呢还是’XYZ’？如果从数学意义上理解，就会错误地得出b和a相同，也应该是’XYZ’，但实际上b的值是’ABC’，让我们一行一行地执行代码，就可以看到到底发生了什么事： 执行a = &#39;ABC&#39;，解释器创建了字符串 &#39;ABC&#39;和变量 a，并把a指向 &#39;ABC&#39;： 执行b = a，解释器创建了变量 b，并把b指向 a 指向的字符串&#39;ABC&#39;： 执行a = &#39;XYZ&#39;，解释器创建了字符串&#39;XYZ&#39;，并把a的指向改为’XYZ’，但b并没有更改： 所以，最后打印变量b的结果自然是’ABC’了。 编程任务： 等差数列可以定义为每一项与它的前一项的差等于一个常数，可以用变量 x1 表示等差数列的第一项，用 d 表示公差，请计算数列 1 4 7 10 13 16 19 … 前 100 项的和。 实现代码: 1234567x1 = 1d = 3n = 100x100 = x1+(100-1)*ds2 = (x1+x100)*100/2s = n*x1+n*(n-1)*d/2print s,s2 等差数列公式： （首项+尾项）*项数/2 项数*首项+项数*(项数-1)*公差/2 运行结果： 114950 14950 定义字符串字符串可以用&#39;&#39;或者&quot;&quot;括起来表示。 如果字符串本身包含&#39;怎么办？比如我们要表示字符串 I&#39;m OK，这时，可以用&quot; &quot;括起来表示： 12"I'm OK"'Learn "Python" in imooc' 类似的，知识点: 如果字符串包含&quot;，我们就可以用&#39; &#39;括起来表示： 如果字符串既包含&#39;又包含&quot;怎么办？ 知识点：转义 这个时候，就需要对字符串的某些特殊字符进行转义，Python字符串用\进行转义。 要表示字符串 Bob said &quot;I&#39;m OK&quot;.由于 &#39; 和&quot;会引起歧义，因此，我们在它前面插入一个\表示这是一个普通字符，不代表字符串的起始，因此，这个字符串又可以表示为 12'Bob said \"I\'m OK\".'# 在要保留原状的字符串前面加上右斜杠 注意：转义字符 \不计入字符串的内容中。 常用的转义字符还有： \n表示换行 \t 表示一个制表符 \\ 表示 \ 字符本身 编程任务： 请将下面两行内容用Python的字符串表示并打印出来： 12 Python was started in 1989 by &quot;Guido&quot;. Python is free and easy to learn. 12s = 'Python was started in 1989 by"Guido".\nPython is free and easy to learn.'print s raw字符串与多行字符串如果一个字符串包含很多需要转义的字符，对每一个字符都进行转义会很麻烦。为了避免这种情况，我们可以在字符串前面加个前缀 r ，表示这是一个 raw 字符串，里面的字符就不需要转义了。例如： 1r'\(~_~)/ \(~_~)/' 解释： 这个例子举得不是很好。可以看出raw加上之后。可能产生误会的\被修改为\\(\\ 表示 \ 字符本身) 不加上r 只有\和(并没有合成转义字符。 加上r。\需要被转义，经过转义后显示出来还是自己。 知识点: 个人小题(r的强大作用) 上图效果可以看出r的强大作用。 但是r&#39;我是一段字符&#39;表示法不能表示多行字符串(r&#39;&#39;&#39;一段字符&#39;&#39;&#39;)，也不能表示包含&#39;和 &quot;的字符串（为什么？） 因为如果r&#39;mtian&#39;yan&#39; r遇到左边第一个&#39;,会继续往后找闭合的标志&#39;然后找到mtian的地方。它任务结束了。代码继续往下执行。当扫到yan这里他就会报错。 ???(更深层待续) 或者r&quot;mtian&quot;yan&quot; 或导致r提前结束掉。后面的就无法继续匹配到对应的。 知识点: 多行字符串，可以用&#39;&#39;&#39;...&#39;&#39;&#39;表示： 12345'''Line 1Line 2Line 3'''#上面这个字符串的表示方法和下面的是完全一样的：'Line 1\nLine 2\nLine 3' 还可以在多行字符串前面添加 r ，把这个多行字符串也变成一个raw字符串： 123r'''Python is created by "Guido".It is free and easy to learn.Let's start learn Python in imooc!''' 编程任务：请把下面的字符串用r&#39;&#39;&#39;...&#39;&#39;&#39;的形式改写，并用print打印出来： 1&apos;\&quot;To be, or not to be\&quot;: that is the question.\nWhether it\&apos;s nobler in the mind to suffer.&apos; 12print r'''"To be,or not to be":that is the question.Whether it's nobler in the mind to suffer.''' 知识点: Unicode字符串字符串还有一个编码问题。 因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），0 - 255被用来表示大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母 A 的编码是65，小写字母 z 的编码是122。 如果要表示中文，显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。 类似的，日文和韩文等其他语言也有这个问题。为了统一所有文字的编码，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 Unicode通常用两个字节表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为0就可以。 因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码，普通的字符串’ABC’在Python内部都是ASCII编码的。 Python在后来添加了对Unicode的支持，以Unicode表示的字符串用u’…’表示，比如： 123print u'中文'中文注意: 不加 u ，中文就不能正常显示。(这个应该是很早版本才会。笔者现在已经无法复现) 转载: http://blog.csdn.net/lxdcyh/article/details/4018054 字符串在Python内部的表示是unicode编码，因此，在做编码转换时，通常需要以unicode作为中间编码，即先将其他编码的字符串解码decode成unicode，再从unicode编码encode成另一种编码。 decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(&#39;gb2312&#39;)，表示将gb2312编码的字符串str1转换成unicode编码。 encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘gb2312’)，表示将unicode编码的字符串str2转换成gb2312编码 代码中字符串的默认编码与代码文件本身的编码一致。 如：s=’中文’ 如果是在utf8的文件中，该字符串就是utf8编码，如果是在gb2312的文件中，则其编码为gb2312。这种情况下，要进行编码转换，都需要先用decode方法将其转换成unicode编码，再使用encode方法将其转换成其他编码。通常，在没有指定特定的编码方式时，都是使用的系统默认编码创建的代码文件 如果字符串是这样定义：s=u’中文’ 则该字符串的编码就被指定为unicode了，即python的内部编码，而与代码文件本身的编码无关。因此，对于这种情况做编码转换，只需要直接使用encode方法将其转换成指定编码即可。 如果一个字符串已经是unicode了，再进行解码则将出错，因此通常要对其编码方式是否为unicode进行判断： 12isinstance(s, unicode) #用来判断是否为unicode 用非unicode编码形式的str来encode会报错 如何获得系统的默认编码？ 1234#!/usr/bin/env python#coding=utf-8import sysprint sys.getdefaultencoding() 该段程序在Win10(1079)上输出为：ascii 在某些IDE中，字符串的输出总是出现乱码，甚至错误，其实是由于IDE的结果输出控制台自身不能显示字符串的编码，而不是程序本身的问题。 如在UliPad(注:UliPad是wxPython的动力，导向和灵活的编程器)中运行如下代码： 12s=u&quot;中文&quot;print s 会提示：UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-1: ordinal not in range(128)。这是因为UliPad在控制台信息输出窗口是按照ascii编码输出的（系统的默认编码是ascii），而上面代码中的字符串是Unicode编码的，所以输出时产生了错误。 将最后一句改为：print s.encode(&#39;gb2312&#39;) 则能正确输出“中文”两个字。 若最后一句改为：print s.encode(&#39;utf8&#39;) 则输出：/xe4/xb8/xad/xe6/x96/x87，这是控制台信息输出窗口按照ascii编码输出utf8编码的字符串的结果。 unicode(str,&#39;gb2312&#39;)与str.decode(&#39;gb2312&#39;)是一样的，都是将gb2312编码的str转为unicode编码 使用str.__class__可以查看str的编码形式为str类型。 window默认编码gbk；linux默认编码utf8 原理说了半天，最后来个包治百病的吧：(天涯)：下面代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#!/usr/bin/env python #coding=utf-8def getCoding(strInput): ''' 获取编码格式 ''' if isinstance(strInput, unicode): return "unicode" try: strInput.decode("utf8") return 'utf8' except: pass try: strInput.decode("gbk") return 'gbk' except: pass def tran2UTF8(strInput): ''' 转化为utf8格式 ''' strCodingFmt = getCoding(strInput) if strCodingFmt == "utf8": return strInput elif strCodingFmt == "unicode": return strInput.encode("utf8") elif strCodingFmt == "gbk": return strInput.decode("gbk").encode("utf8")def tran2GBK(strInput): ''' 转化为gbk格式 ''' strCodingFmt = getCoding(strInput) if strCodingFmt == "gbk": return strInput elif strCodingFmt == "unicode": return strInput.encode("gbk") elif strCodingFmt == "utf8": return strInput.decode("utf8").encode("gbk")s = "中文"if isinstance(s, unicode): #s=u"中文" print s.encode('gb2312') print "我是Unicode编码的"elif getCoding(s) == "utf8": #s="中文" print s.decode('utf-8').encode('gb2312') print "我是utf-8编码的"else: print s.decode('gbk').encode('gbk') print "我是gbk编码的" 上图结果一：以utf-8格式保存的py文件。图二：以ascii格式保存的py文件。 编码检测包 chardet 知识点：因此，转码的时候一定要先搞明白，字符串str是什么编码，然后decode成unicode，然后再encode成其他编码 插入数据库报错的解决方案:UnicodeDecodeError: ‘ascii’ codec can’t decode byte 123import sysreload(sys)sys.setdefaultencoding(&apos;utf8&apos;) Unicode字符串除了多了一个 u 之外，与普通字符串没啥区别，转义字符和多行表示法仍然有效： 转义： 1234567891011u'中文\n日文\n韩文'#多行：u'''第一行第二行'''#raw+多行：ur'''Python的Unicode字符串支持"中文","日文","韩文"等多种语言''' 如果中文字符串在Python环境下遇到 UnicodeDecodeError，这是因为.py文件保存的格式有问题。可以在第一行添加注释 1234# -*- coding: utf-8 -*-#简洁版#coding=utf-8 目的是告诉Python解释器，用UTF-8编码读取源代码。然后用Notepad++ 另存为… 并选择UTF-8格式保存。 编程任务：用多行Unicode字符串表示下面的唐诗并打印： 静夜思 床前明月光，疑是地上霜。举头望明月，低头思故乡。 知识点: https://www.python.org/dev/peps/pep-0263/ python定义文件编码到底用哪种？ 12345# coding=&lt;encoding name&gt; #!/usr/bin/python# -*- coding: &lt;encoding name&gt; -*-#!/usr/bin/python# vim: set fileencoding=&lt;encoding name&gt; : 这些都可以只要第一二行能满足如下正则表达式 1^[ \t\v]*#.*?coding[:=][ \t]*([-_.a-zA-Z0-9]+) 12345678910# -*- coding: utf-8 -*-# This Python file uses the following encoding: utf-8# 花式标明print '''静夜思床前明月光，疑是地上霜。举头望明月，低头思故乡。''' 如果不标明文件编码或找不到。python会默认你是ASCII 整数和浮点数Python支持对整数和浮点数直接进行四则混合运算，运算规则和数学上的四则运算规则完全一致。 基本的运算： 1231 + 2 + 3 # ==&gt; 64 * 5 - 6 # ==&gt; 147.5 / 8 + 2.1 # ==&gt; 3.0375 使用括号可以提升优先级，这和数学运算完全一致，注意只能使用小括号，但是括号可以嵌套很多层： 12(1 + 2) * 3 # ==&gt; 9(2.2 + 3.3) / (1.5 * (9 - 0.3)) # ==&gt; 0.42145593869731807 和数学运算不同的地方是，Python的整数运算结果仍然是整数，浮点数运算结果仍然是浮点数： 121 + 2 # ==&gt; 整数 31.0 + 2.0 # ==&gt; 浮点数 3.0 但是整数和浮点数混合运算的结果就变成浮点数了： 11 + 2.0 # ==&gt; 浮点数 3.0 为什么要区分整数运算和浮点数运算呢？ 这是因为整数运算的结果永远是精确的，而浮点数运算的结果不一定精确，因为计算机内存再大，也无法精确表示出无限循环小数，比如 0.1 换成二进制表示就是无限循环小数。 那整数的除法运算遇到除不尽的时候，结果难道不是浮点数吗？我们来试一下： 111 / 4 # ==&gt; 2 令很多初学者惊讶的是，Python的整数除法，即使除不尽，结果仍然是整数，余数直接被扔掉。不过，Python提供了一个求余的运算 % 可以计算余数： 111 % 4 # ==&gt; 3 如果我们要计算 11 / 4 的精确结果，按照“整数和浮点数混合运算的结果是浮点数”的法则，把两个数中的一个变成浮点数再运算就没问题了： 111.0 / 4 # ==&gt; 2.75 编程任务：请计算 2.5 + 10 / 4 ,并解释计算结果为什么不是期望的 5.0 ? 请修复上述运算，使得计算结果是 5.0 1print 2.5 + 10.0 / 4 运行结果： 15.0 布尔类型我们已经了解了Python支持布尔类型的数据，布尔类型只有True和False两种值，但是布尔类型有以下几种运算： 与运算：只有两个布尔值都为 True 时，计算结果才为 True。 1234True and True # ==&gt; TrueTrue and False # ==&gt; FalseFalse and True # ==&gt; FalseFalse and False # ==&gt; False 或运算：只要有一个布尔值为 True，计算结果就是 True。 1234True or True # ==&gt; TrueTrue or False # ==&gt; TrueFalse or True # ==&gt; TrueFalse or False # ==&gt; False 非运算：把True变为False，或者把False变为True： 12not True # ==&gt; Falsenot False # ==&gt; True 布尔运算在计算机中用来做条件判断，根据计算结果为True或者False，计算机可以自动执行不同的后续代码。 在Python中，布尔类型还可以与其他数据类型做 and、or和not运算，请看下面的代码： 知识点：Python把0、空字符串’’和None看成 False，其他数值和非空字符串都看成 True。短路运算 12a = Trueprint a and 'a=T' or 'a=F' 计算结果不是布尔类型，而是字符串 &#39;a=T&#39;，这是为什么呢？ 因为Python把0、空字符串’’和None看成 False，其他数值和非空字符串都看成 True，所以： True and ‘a=T’ 计算结果是 ‘a=T’继续计算 ‘a=T’ or ‘a=F’ 计算结果还是 ‘a=T’要解释上述结果，又涉及到 and 和 or 运算的一条重要法则：短路计算。 在计算 a and b时，如果 a 是 False，则根据与运算法则，整个结果必定为 False，因此返回 a；如果 a 是 True，则整个计算结果必定取决与 b，因此返回 b。 在计算 a or b 时，如果 a 是 True，则根据或运算法则，整个计算结果必定为 True，因此返回 a；如果 a 是 False，则整个计算结果必定取决于 b，因此返回 b。 所以Python解释器在做布尔运算时，只要能提前确定计算结果，它就不会往后算了，直接返回结果。 编码任务：请运行如下代码，并解释打印的结果： 1234a = 'python'print 'hello,', a or 'world'b = ''print 'hello,', b or 'world' 1234567# -*- coding: utf-8 -*-a = 'python'print 'hello,', a or 'world'#a为非空，则输出ab = ''#b为空，输出worldprint 'hello,', b or 'world' 运行结果： 12hello, pythonhello, world Python集合类型:list和tuple创建listPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。 比如，列出班里所有同学的名字，就可以用一个list表示： 12&gt;&gt;&gt; ['Michael', 'Bob', 'Tracy']['Michael', 'Bob', 'Tracy'] list是数学意义上的有序集合，也就是说，list中的元素是按照顺序排列的。 构造list非常简单，按照上面的代码，直接用 [ ]把list的所有元素都括起来，就是一个list对象。通常，我们会把list赋值给一个变量，这样，就可以通过变量来引用list： 123&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates # 打印classmates变量的内容['Michael', 'Bob', 'Tracy'] 由于Python是动态语言，所以list中包含的元素并不要求都必须是同一种数据类型，我们完全可以在list中包含各种数据： 1&gt;&gt;&gt; L = [&apos;Michael&apos;, 100, True] 一个元素也没有的list，就是空list： 1&gt;&gt;&gt; empty_list = [] 编程任务 假设班里有3名同学：Adam，Lisa和Bart，他们的成绩分别是 95.5，85 和 59，请按照 名字, 分数, 名字, 分数… 的顺序按照分数从高到低用一个list表示，然后打印出来。 12L = ['Adam', 95.5,'Lisa', 85, 'Bart', 59]print L 运行结果: 1[&apos;Adam&apos;, 95.5, &apos;Lisa&apos;, 85, &apos;Bart&apos;, 59] 注：list本身就是有序的。所以直接打印即可。 Python按照索引访问list由于list是一个有序集合，所以，我们可以用一个list按分数从高到低表示出班里的3个同学： 1&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 那我们如何从list中获取指定第 N 名的同学呢？方法是通过索引来获取list中的指定元素。 需要特别注意的是，索引从 0 开始，也就是说，第一个元素的索引是0，第二个元素的索引是1，以此类推。 因此，要打印第一名同学的名字，用 L[0]: 12345678910&gt;&gt;&gt; print L[0]Adam#要打印第二名同学的名字，用 L[1]:&gt;&gt;&gt; print L[1]Lisa#要打印第三名同学的名字，用 L[2]:&gt;&gt;&gt; print L[2]Bart 要打印第四名同学的名字，用 L[3]: 1234&gt;&gt;&gt; print L[3]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;IndexError: list index out of range 报错了！IndexError意思就是索引超出了范围，因为上面的list只有3个元素，有效的索引是 0，1，2。 所以，使用索引时，千万注意不要越界。 编程任务 三名同学的成绩可以用一个list表示：L = [95.5, 85, 59] 请按照索引分别打印出第一名、第二名、第三名，同时测试 print L[3]。 实现代码： 12345L = [95.5,85,59]print L[0]print L[1]print L[2]print L[3] 运行结果： 1234567Traceback (most recent call last): File &quot;index.py&quot;, line 5, in print L[3]IndexError: list index out of range95.58559 知识点：正序从0开始，逆序从-1开始是最好一个list内容。 当索引数字为负数时，表示逆序读出List中的内容，记住List的最后一个空间的编号为-1开始 倒序访问list我们还是用一个list按分数从高到低表示出班里的3个同学： 1&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 这时，老师说，请分数最低的同学站出来。 要写代码完成这个任务，我们可以先数一数这个 list，发现它包含3个元素，因此，最后一个元素的索引是2： 12&gt;&gt;&gt; print L[2]Bart Bart同学是最后一名，俗称倒数第一，所以，我们可以用 -1 这个索引来表示最后一个元素： 12&gt;&gt;&gt; print L[-1]Bart Bart同学表示躺枪。 类似的，倒数第二用 -2 表示，倒数第三用 -3 表示，倒数第四用 -4 表示： 123456789&gt;&gt;&gt; print L[-2]Lisa&gt;&gt;&gt; print L[-3]Adam&gt;&gt;&gt; print L[-4]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list index out of rangeL[-4] 报错了，因为倒数第四不存在，一共只有3个元素。 使用倒序索引时，也要注意不要越界。 编程任务 三名同学的成绩可以用一个list表示：L = [95.5, 85, 59] 请按照倒序索引分别打印出倒数第一、倒数第二、倒数第三。 实现代码： 12345L = [95.5, 85, 59]print L[-1]print L[-2]print L[-3]print L[-4] 运行结果： 1234567Traceback (most recent call last): File &quot;index.py&quot;, line 5, in print L[-4]IndexError: list index out of range598595.5 list添加新元素(append insert)现在，班里有3名同学： 1&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 今天，班里转来一名新同学 Paul，如何把新同学添加到现有的 list 中呢？ 第一个办法是用 list 的 append() 方法，把新同学追加到 list 的末尾： 1234&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;]&gt;&gt;&gt; L.append(&apos;Paul&apos;)&gt;&gt;&gt; print L[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;] append()总是把新的元素添加到 list 的尾部。 如果 Paul 同学表示自己总是考满分，要求添加到第一的位置，怎么办？ 方法是用list的 insert()方法，它接受两个参数，第一个参数是索引号，第二个参数是待添加的新元素： 1234&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;]&gt;&gt;&gt; L.insert(0, &apos;Paul&apos;)&gt;&gt;&gt; print L[&apos;Paul&apos;, &apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] L.insert(0, &#39;Paul&#39;) 的意思是，’Paul’将被添加到索引为 0 的位置上（也就是第一个），而原来索引为 0 的Adam同学，以及后面的所有同学，都自动向后移动一位。 编程任务 假设新来一名学生Paul，Paul 同学的成绩比Bart好，但是比Lisa差，他应该排到第三名的位置，请用代码实现。 代码实现: 123L = ['Adam', 'Lisa', 'Bart']L.insert(2,'paul')print L 运行结果: 1['Adam', 'Lisa', 'paul', 'Bart'] 正向第三名索引号为2.倒数第三名索引号为-3 list删除元素(pop)Paul同学刚来几天又要转走了，那么我们怎么把Paul 从现有的list中删除呢？ 如果Paul同学排在最后一个，我们可以用list的pop()方法删除： 12345&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;]&gt;&gt;&gt; L.pop()&apos;Paul&apos;&gt;&gt;&gt; print L[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] pop()方法总是删掉list的最后一个元素，并且它还返回这个元素，所以我们执行 L.pop() 后，会打印出 ‘Paul’。 如果Paul同学不是排在最后一个怎么办？比如Paul同学排在第三： 1&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Paul&apos;, &apos;Bart&apos;] 要把Paul踢出list，我们就必须先定位Paul的位置。由于Paul的索引是2，因此，用 pop(2)把Paul删掉： 1234&gt;&gt;&gt; L.pop(2)&apos;Paul&apos;&gt;&gt;&gt; print L[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 两种方式：直接pop()默认删除第一个，括号内指定参数：索引，删除索引位置上。 编码任务1L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Paul&apos;, &apos;Bart&apos;] Paul的索引是2，Bart的索引是3，如果我们要把Paul和Bart都删掉，请解释下面的代码为什么不能正确运行： 12L.pop(2)L.pop(3) 怎样调整代码可以把Paul和Bart都正确删除掉？ 解释：因为语句是按顺序执行的删除了Paul之后。索引号3已经越界。我们要删除的Bart已经变成2了。 知识点：这教育我们删除list时要秉着从前到后顺序。 List替换元素假设现在班里仍然是3名同学： &gt;&gt;&gt; L = [&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;] 现在，Bart同学要转学走了，碰巧来了一个Paul同学，要更新班级成员名单，我们可以先把Bart删掉，再把Paul添加进来。 另一个办法是直接用Paul把Bart给替换掉： 123&gt;&gt;&gt; L[2] = &apos;Paul&apos;&gt;&gt;&gt; print LL = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Paul&apos;] 对list中的某一个索引赋值，就可以直接用新的元素替换掉原来的元素，list包含的元素个数保持不变。 由于Bart还可以用 -1 做索引，因此，下面的代码也可以完成同样的替换工作： &gt;&gt;&gt; L[-1] = &#39;Paul&#39; 编程任务 班里的同学按照分数排名是这样的：L = [‘Adam’, ‘Lisa’, ‘Bart’]但是，在一次考试后，Bart同学意外取得第一，而Adam同学考了倒数第一。 请通过对list的索引赋值，生成新的排名。 实现代码： 1234L = ['Adam', 'Lisa', 'Bart']L[0]='Bart'L[-1]='Adam'print L 运行结果： 1[&apos;Bart&apos;, &apos;Lisa&apos;, &apos;Adam&apos;] 创建tupletuple是另一种有序的列表，中文翻译为“ 元组 ”。tuple 和 list 非常类似，但是，知识点：tuple一旦创建完毕，就不能修改了。 同样是表示班里同学的名称，用tuple表示如下： 1&gt;&gt;&gt; t = (&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;) 创建tuple和创建list唯一不同之处是用( )替代了[ ]。 现在，这个 t 就不能改变了，tuple没有 append()方法，也没有insert()和pop()方法。所以，新同学没法直接往 tuple 中添加，老同学想退出 tuple 也不行。 获取 tuple 元素的方式和 list 是一模一样的，我们可以正常使用 t[0]，t[-1]等索引方式访问元素，但是不能赋值成别的元素，不信可以试试： 1234&gt;&gt;&gt; t[0] = &apos;Paul&apos;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &apos;tuple&apos; object does not support item assignment 编程任务 创建一个tuple，顺序包含0 - 9这10个数。 实现代码： 12t = (0,1,2,3,4,5,6,7,8,9)print t 运行结果： 1(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) 创建单元素tupletuple和list一样，可以包含 0 个、1个和任意多个元素。 包含多个元素的 tuple，前面我们已经创建过了。 包含 0 个元素的 tuple，也就是空tuple，直接用 ()表示： 123&gt;&gt;&gt; t = ()&gt;&gt;&gt; print t() 创建包含1个元素的 tuple 呢？来试试： 123&gt;&gt;&gt; t = (1)&gt;&gt;&gt; print t1 好像哪里不对！t 不是 tuple ，而是整数1。为什么呢？ 知识点：单元素tuple的()被当做优先级。(1)变成整数1.单元素括号结尾加, 因为()既可以表示tuple，又可以作为括号表示运算时的优先级，结果 (1) 被Python解释器计算出结果 1，导致我们得到的不是tuple，而是整数 1。 正是因为用()定义单元素的tuple有歧义，所以 Python 规定，单元素 tuple 要多加一个逗号,，这样就避免了歧义： 123&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; print t(1,) Python在打印单元素tuple时，也自动添加了一个,，为了更明确地告诉你这是一个tuple。 多元素 tuple 加不加这个额外的,效果是一样的： 123&gt;&gt;&gt; t = (1, 2, 3,)&gt;&gt;&gt; print t(1, 2, 3) 编程任务下面代码为什么没有创建出包含一个学生的 tuple： 12t = ('Adam')print t 请修改代码，确保 t 是一个tuple。 因为单元素tuple的括号被当做是优先级标志。要加上额外,标识这是一个元组。 实现代码： 12t = ('Adam',)print t 运行结果: 1(&apos;Adam&apos;,) “可变”的tuple(指向不变。指向的东西可以变)前面我们看到了tuple一旦创建就不能修改。现在，我们来看一个可变的tuple： 1&gt;&gt;&gt; t = (&apos;a&apos;, &apos;b&apos;, [&apos;A&apos;, &apos;B&apos;]) 注意到 t 有 3 个元素：&#39;a&#39;，&#39;b&#39;和一个list：[&#39;A&#39;, &#39;B&#39;]。list作为一个整体是tuple的第3个元素。list对象可以通过 t[2] 拿到： 12345&gt;&gt;&gt; L = t[2]# 然后，我们把list的两个元素改一改：&gt;&gt;&gt; L[0] = 'X'&gt;&gt;&gt; L[1] = 'Y' 再看看tuple的内容： 12&gt;&gt;&gt; print t(&apos;a&apos;, &apos;b&apos;, [&apos;X&apos;, &apos;Y&apos;]) 不是说tuple一旦定义后就不可变了吗？怎么现在又变了？ 别急，我们先看看定义的时候tuple包含的3个元素： 当我们把list的元素’A’和’B’修改为’X’和’Y’后，tuple变为： 表面上看，tuple的元素确实变了，但其实变的不是 tuple 的元素，而是list的元素。 tuple一开始指向的list并没有改成别的list，所以，tuple所谓的“不变”是说，tuple的每个元素，指向永远不变。即指向’a’，就不能改成指向’b’，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的！ 理解了指向不变后，要创建一个内容也不变的tuple怎么做？那就必须保证tuple的每一个元素本身也不能变。 编程任务： 定义了tuple：t = (‘a’, ‘b’, [‘A’, ‘B’]) 由于 t 包含一个list元素，导致tuple的内容是可变的。能否修改上述代码，让tuple内容不可变？ 解答：将里面的list替换成一个不可变的元素。比如tuple。 实现代码: 12t = (&apos;a&apos;, &apos;b&apos;, (&apos;A&apos;, &apos;B&apos;))print t 运行结果： 1(&apos;a&apos;, &apos;b&apos;, (&apos;A&apos;, &apos;B&apos;)) Python的条件判断和循环语句if语句计算机之所以能做很多自动化的任务，因为它可以自己做条件判断。 比如，输入用户年龄，根据年龄打印不同的内容，在Python程序中，可以用if语句实现： 12345age = 20if age &gt;= 18: print 'your age is', age print 'adult'print 'END' 注意: Python代码的缩进规则。具有相同缩进的代码被视为代码块，上面的3，4行 print 语句就构成一个代码块（但不包括第5行的print）。如果 if 语句判断为 True，就会执行这个代码块。 知识点: 缩进请严格按照Python的习惯写法：4个空格，不要使用Tab，更不要混合Tab和空格，否则很容易造成因为缩进引起的语法错误。 注意: if 语句后接表达式，然后用:表示代码块开始。 如果你在Python交互环境下敲代码，还要特别留意缩进，并且退出缩进需要多敲一行回车： 12345&gt;&gt;&gt; age = 20&gt;&gt;&gt; if age &gt;= 18:... print &apos;your age is&apos;, age... print &apos;adult&apos;... 12your age is 20adult 编程任务 如果成绩达到60分或以上，视为passed。 假设Bart同学的分数是75，请用if语句判断是否能打印出 passed: 实现代码: 123score = 75if score&gt;=60: print 'passed' 运行结果: 1passed if-else当 if 语句判断表达式的结果为 True 时，就会执行 if 包含的代码块： 12if age &gt;= 18: print 'adult' 如果我们想判断年龄在18岁以下时，打印出 ‘teenager’，怎么办？ 方法是再写一个 if: 12if age &lt; 18: print 'teenager' 或者用 not 运算： 12if not age &gt;= 18: print 'teenager' 细心的同学可以发现，这两种条件判断是“非此即彼”的，要么符合条件1，要么符合条件2，因此，完全可以用一个 if ... else ... 语句把它们统一起来： 1234if age &gt;= 18: print 'adult'else: print 'teenager' 利用 if ... else ...语句，我们可以根据条件表达式的值为 True 或者 False ，分别执行 if 代码块或者 else 代码块。 注意: else 后面有个:。 编程任务 如果成绩达到60分或以上，视为passed，否则视为failed。 假设Bart同学的分数是55，请用if语句打印出 passed 或者 failed: 实现代码： 12345score = 55if score&gt;=60: print 'passed'else: print 'failed' 运行结果: 1failed if-elif-else有的时候，一个 if … else … 还不够用。比如，根据年龄的划分： 条件1：18岁或以上：adult 条件2：6岁或以上：teenager 条件3：6岁以下：kid 我们可以用一个 if age &gt;= 18 判断是否符合条件1，如果不符合，再通过一个 if 判断 age &gt;= 6 来判断是否符合条件2，否则，执行条件3： 1234567if age &gt;= 18: print &apos;adult&apos;else: if age &gt;= 6: print &apos;teenager&apos; else: print &apos;kid&apos; 这样写出来，我们就得到了一个两层嵌套的 if … else … 语句。这个逻辑没有问题，但是，如果继续增加条件，比如3岁以下是 baby： 12345678910if age &gt;= 18: print &apos;adult&apos;else: if age &gt;= 6: print &apos;teenager&apos; else: if age &gt;= 3: print &apos;kid&apos; else: print &apos;baby&apos; 这种缩进只会越来越多，代码也会越来越难看。 要避免嵌套结构的 if … else …，我们可以用 if … 多个elif … else … 的结构，一次写完所有的规则： 12345678if age &gt;= 18: print &apos;adult&apos;elif age &gt;= 6: print &apos;teenager&apos;elif age &gt;= 3: print &apos;kid&apos;else: print &apos;baby&apos; elif 意思就是 else if。这样一来，我们就写出了结构非常清晰的一系列条件判断。 特别注意: 这一系列条件判断会从上到下依次判断，如果某个判断为 True，执行完对应的代码块，后面的条件判断就直接忽略，不再执行了。 请思考下面的代码： 1234567age = 8if age &gt;= 6: print &apos;teenager&apos;elif age &gt;= 18: print &apos;adult&apos;else: print &apos;kid&apos; 当 age = 8 时，结果正确，但 age = 20 时，为什么没有打印出 adult？ 如果要修复，应该如何修复？ 知识点解答: 因为当age=20.第一个条件&gt;=6满足就短路了。因此我们在设置条件应该从严格到松泛. 1234567age = 20if age &gt;= 18: print &apos;teenager&apos;elif age &gt;= 6: print &apos;adult&apos;else: print &apos;kid&apos; 编程任务 如果按照分数划定结果： 90分或以上：excellent 80分或以上：good 60分或以上：passed 60分以下：failed 请编写程序根据分数打印结果。 实现代码: 12345678910score = 85if score&gt;=90: print 'excellent'elif score&gt;=80: print 'good'elif score&gt;=60: print 'passed'else: print 'failed' 运行结果: 1good for循环list或tuple可以表示一个有序集合。如果我们想依次访问一个list中的每一个元素呢？比如 list： 1234L = ['Adam', 'Lisa', 'Bart']print L[0]print L[1]print L[2] 如果list只包含几个元素，这样写还行，如果list包含1万个元素，我们就不可能写1万行print。 这时，循环就派上用场了。 Python的 for 循环就可以依次把list或tuple的每个元素迭代出来： 123L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;]for name in L: print name 注意: name 这个变量是在 for 循环中定义的(这是一个临时变量名字可自定义)，意思是，依次取出list中的每一个元素，并把元素赋值给 name，然后执行for循环体（就是缩进的代码块）。 这样一来，遍历一个list或tuple就非常容易了。 编程任务 班里考试后，老师要统计平均成绩，已知4位同学的成绩用list表示如下：L = [75, 92, 59, 68] 请利用for循环计算出平均成绩。 实现代码: 12345L = [75, 92, 59, 68]sum = 0.0for x in L: sum =sum+xprint sum / 4 运行结果： 173.5 while循环和 for 循环不同的另一种循环是 while 循环，while 循环不会迭代 list 或 tuple 的元素，而是根据表达式判断循环是否结束。 比如要从 0 开始打印不大于 N 的整数： 12345N = 10x = 0while x &lt; N: print x x = x + 1 while循环每次先判断 x &lt; N，如果为True，则执行循环体的代码块,否则，退出循环。 在循环体内，x = x + 1 会让 x 不断增加，最终因为 x &lt; N 不成立而退出循环。 如果没有这一个语句，while循环在判断 x &lt; N 时总是为True，就会无限循环下去，变成死循环，所以要特别留意while循环的退出条件。 编程任务 利用while循环计算100以内奇数的和。 实现代码: 123456sum = 0x = 1while x&lt;=100: sum=sum+x x=x+2print sum 知识点: 奇数只需要从1开始不断加2都是奇数。 运行结果： 12500 break退出循环用for 循环或者 while 循环时，如果要在循环体内直接退出循环，可以使用 break 语句。 比如计算1至100的整数和，我们用while来实现： 12345678sum = 0x = 1while True: sum = sum + x x = x + 1 if x &gt; 100: breakprint sum 咋一看， while True 就是一个死循环，但是在循环体内，我们还判断了 x &gt; 100 条件成立时，用break语句退出循环，这样也可以实现循环的结束。 编程任务 利用 while True 无限循环配合 break 语句，计算 1 + 2 + 4 + 8 + 16 + … 的前20项的和。 实现代码: 12345678910sum = 0x = 1n = 1while True: sum =sum+x x =2*x n =n+1 if n &gt;20: breakprint sum 运行结果: 11048575 continue继续循环在循环过程中，可以用break退出当前循环，还可以用continue跳过后续循环代码，继续下一次循环。 假设我们已经写好了利用for循环计算平均分的代码： 1234567L = [75, 98, 59, 81, 66, 43, 69, 85]sum = 0.0n = 0for x in L: sum = sum + x n = n + 1print sum / n 现在老师只想统计及格分数的平均分，就要把 x &lt; 60 的分数剔除掉，这时，利用continue，可以做到当 x &lt; 60的时候，不继续执行循环体的后续代码，直接进入下一次循环： 12345for x in L: if x &lt; 60: continue sum = sum + x n = n + 1 coutinue: 跳过下面的代码。开始下一次循环。 编程任务 对已有的计算 0 - 100 的while循环进行改造，通过增加 continue 语句，使得只计算奇数的和： 12345678sum = 0x = 1while True: sum = sum + x x = x + 1 if x &gt; 100: breakprint sum 思路: if判断到是偶数，continue跳过。 实现代码: 1234567891011sum = 0x = 0while True: x = x + 1 if x &gt; 100: break if x%2==0: continue sum = sum+x print sum 运行结果: 12500 多重循环(嵌套循环)在循环内部，还可以嵌套循环，我们来看一个例子： 123for x in [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]: for y in [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;]: print x + y x 每循环一次，y就会循环 3 次，这样，我们可以打印出一个全排列： 123456789A1A2A3B1B2B3C1C2C3 编程任务 对100以内的两位数，请使用一个两重循环打印出所有十位数数字比个位数数字小的数，例如，23（2 &lt; 3）。 代码实现。 123456tens_place = [1,2,3,4,5,6,7,8,9]ones_place = [0,1,2,3,4,5,6,7,8,9]for x in tens_place: for y in ones_place: if x&lt;y: print x*10 + y 运行结果： 12345678910121314151617181923略 重要的数据类型Dict和Set什么是dict我们已经知道，list 和 tuple 可以用来表示顺序集合，例如，班里同学的名字： 1[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 或者考试的成绩列表： 1[95, 85, 59] 但是，要根据名字找到对应的成绩，用两个 list 表示就不方便。 如果把名字和分数关联起来，组成类似的查找表： 123&apos;Adam&apos; ==&gt; 95&apos;Lisa&apos; ==&gt; 85&apos;Bart&apos; ==&gt; 59 给定一个名字，就可以直接查到分数。 Python的 dict 就是专门干这件事的。用 dict 表示名字-成绩的查找表如下： 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 我们把名字称为key，对应的成绩称为value，dict就是通过 key来查找 value。 花括号 {} 表示这是一个dict，然后按照 key: value, 写出来即可。最后一个 key: value 的逗号可以省略。 知识点： 区别小课堂 单元素的tuple必须在后面多加一个逗号。 dict最后的逗号可以省略 由于dict也是集合，len() 函数可以计算任意集合的大小： 12&gt;&gt;&gt; len(d)3 知识点：注意: 一个 key-value 算一个，因此，dict大小为3。 编程任务 新来的Paul同学成绩是 75 分，请编写一个dict，把Paul同学的成绩也加进去。 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 实现代码: 1234567d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59, &apos;Paul&apos;: 75 &#125; 访问dict我们已经能创建一个dict，用于表示名字和成绩的对应关系： 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 那么，如何根据名字来查找对应的成绩呢？ 可以简单地使用 d[key] 的形式来查找对应的 value，这和 list 很像，不同之处是，list 必须使用索引返回对应的元素，而dict使用key： 1234567&gt;&gt;&gt; print d[&apos;Adam&apos;]95&gt;&gt;&gt; print d[&apos;Paul&apos;]Traceback (most recent call last): File &quot;index.py&quot;, line 11, in &lt;module&gt; print d[&apos;Paul&apos;]KeyError: &apos;Paul&apos; 注意: 通过 key 访问 dict 的value，只要 key 存在，dict就返回对应的value。如果key不存在，会直接报错：KeyError。 知识点：避免 KeyError 发生，有两个办法： 是先判断一下 key 是否存在，用 in 操作符： 12if &apos;Paul&apos; in d: print d[&apos;Paul&apos;] 如果 ‘Paul’ 不存在，if语句判断为False，自然不会执行 print d[‘Paul’] ，从而避免了错误。 是使用dict本身提供的一个get方法，在Key不存在的时候，返回None： 1234&gt;&gt;&gt; print d.get(&apos;Bart&apos;)59&gt;&gt;&gt; print d.get(&apos;Paul&apos;)None 编程任务根据如下dict：12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 请打印出：Adam: 95Lisa: 85Bart: 59 实现代码: 12345678d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125;print &apos;Adam:&apos;,d[&apos;Adam&apos;]print &apos;Lisa:&apos;,d.get(&apos;Lisa&apos;)print &apos;Bart:&apos;,d[&apos;Bart&apos;] 运行结果： 123Adam: 95Lisa: 85Bart: 59 dict的特点知识点：dict查找速度快。list查找速度随着元素增加而逐渐下降。缺点：内存占用大。list慢但内存占用小。 dict的第一个特点是查找速度快，无论dict有10个元素还是10万个元素，查找速度都一样。而list的查找速度随着元素增加而逐渐下降。 不过dict的查找速度快不是没有代价的，dict的缺点是占用内存大，还会浪费很多内容，list正好相反，占用内存小，但是查找速度慢。 由于dict是按 key 查找，所以，在一个dict中，key不能重复。 dict的第二个特点就是存储的key-value序对是没有顺序的！这和list不一样： 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 当我们试图打印这个dict时： 12&gt;&gt;&gt; print d&#123;&apos;Lisa&apos;: 85, &apos;Adam&apos;: 95, &apos;Bart&apos;: 59&#125; 打印的顺序不一定是我们创建时的顺序，而且，不同的机器打印的顺序都可能不同，这说明 知识点:dict内部是无序的，不能用dict存储有序的集合。 知识点：dict的第三个特点是作为 key 的元素必须不可变，Python的基本类型如字符串、整数、浮点数都是不可变的，都可以作为 key。 但是list是可变的，就不能作为 key。 可以试试用list作为key时会报什么样的错误。 不可变这个限制仅作用于key，value是否可变无所谓： 12345&#123; &apos;123&apos;: [1, 2, 3], # key 是 str，value是list 123: &apos;123&apos;, # key 是 int，value 是 str (&apos;a&apos;, &apos;b&apos;): True # key 是 tuple，并且tuple的每个元素都是不可变对象，value是 boolean&#125; 最常用的key还是字符串，因为用起来最方便。 编程任务 请设计一个dict，可以根据分数来查找名字，已知成绩如下： 123Adam: 95,Lisa: 85,Bart: 59. 实现代码: 12345d = &#123; 95:&apos;Adam&apos;, 85:&apos;Lisa&apos;, 59:&apos;Bart&apos;&#125; 运行结果：无 更新dictdict是可变的，也就是说，我们可以随时往dict中添加新的 key-value。比如已有dict： 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 要把新同学’Paul’的成绩 72 加进去，用赋值语句： 1&gt;&gt;&gt; d[&apos;Paul&apos;] = 72 再看看dict的内容： 12&gt;&gt;&gt; print d&#123;&apos;Lisa&apos;: 85, &apos;Paul&apos;: 72, &apos;Adam&apos;: 95, &apos;Bart&apos;: 59&#125; 如果 key 已经存在，则赋值会用新的 value 替换掉原来的 value： 123&gt;&gt;&gt; d[&apos;Bart&apos;] = 60&gt;&gt;&gt; print d&#123;&apos;Lisa&apos;: 85, &apos;Paul&apos;: 72, &apos;Adam&apos;: 95, &apos;Bart&apos;: 60&#125; 编程任务 请根据Paul的成绩 72 更新下面的dict：123456789d = &#123; 95: &apos;Adam&apos;, 85: &apos;Lisa&apos;, 59: &apos;Bart&apos;&#125; 实现代码: 1234567d = &#123; 95: 'Adam', 85: 'Lisa', 59: 'Bart'&#125;d[72] = 'Paul'print d 运行结果： 1&#123;72: &apos;Paul&apos;, 59: &apos;Bart&apos;, 85: &apos;Lisa&apos;, 95: &apos;Adam&apos;&#125; 遍历dict由于dict也是一个集合，所以，遍历dict和遍历list类似，都可以通过 for 循环实现。 直接使用for循环可以遍历 dict 的 key： 1234567&gt;&gt;&gt; d = &#123; 'Adam': 95, 'Lisa': 85, 'Bart': 59 &#125;&gt;&gt;&gt; for key in d:... print key... LisaAdamBart 由于通过 key 可以获取对应的 value，因此，在循环体内，可以获取到value的值。 注：这里的key只是一个约定俗称的变量，可以改为其他名字。但是推荐用key。 编程任务 请用 for 循环遍历如下的dict，打印出 name: score 来。 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125; 实现代码： 1234567d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59&#125;for key in d: print key+&quot;:&quot;,d[key] 运行结果: 123Lisa: 85Adam: 95Bart: 59 什么是setdict的作用是建立一组 key 和一组 value 的映射关系，dict的key是不能重复的。 有的时候，我们只想要 dict 的 key，不关心 key 对应的 value，目的就是保证这个集合的元素不会重复，这时，set就派上用场了。 set 持有一系列元素，这一点和 list 很像，但是set的元素没有重复，而且是无序的，这点和 dict 的 key很像。 知识点: 创建 set 的方式是调用 set() 并传入一个 list，list的元素将作为set的元素： 12345&gt;&gt;&gt; s = set(['A', 'B', 'C'])可以查看 set 的内容：&gt;&gt;&gt; print sset(['A', 'C', 'B']) 请注意，上述打印的形式类似 list， 但它不是 list，仔细看还可以发现，打印的顺序和原始 list 的顺序有可能是不同的，因为set内部存储的元素是无序的。 因为set不能包含重复的元素，所以，当我们传入包含重复元素的 list 会怎么样呢？ 12345&gt;&gt;&gt; s = set([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;C&apos;])&gt;&gt;&gt; print sset([&apos;A&apos;, &apos;C&apos;, &apos;B&apos;])&gt;&gt;&gt; len(s)3 结果显示，set会自动去掉重复的元素，原来的list有4个元素，但set只有3个元素。 编程任务 请用set表示班里的4位同学：Adam, Lisa, Bart, Paul 实现代码: 12s = set(['Adam', 'Lisa', 'Bart', 'Paul'])print s 运行结果: 1set([&apos;Lisa&apos;, &apos;Paul&apos;, &apos;Adam&apos;, &apos;Bart&apos;]) 访问set由于set存储的是无序集合，所以我们没法通过索引来访问。 访问 set中的某个元素实际上就是判断一个元素是否在set中。 例如，存储了班里同学名字的set： 1&gt;&gt;&gt; s = set([&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;]) 我们可以用 in 操作符判断： Bart是该班的同学吗？ 12345678910&gt;&gt;&gt; &apos;Bart&apos; in sTrueBill是该班的同学吗？&gt;&gt;&gt; &apos;Bill&apos; in sFalsebart是该班的同学吗？&gt;&gt;&gt; &apos;bart&apos; in sFalse 知识点：大小写很重要，’Bart’ 和 ‘bart’被认为是两个不同的元素。 编程任务 由于上述set不能识别小写的名字，请改进set，使得 ‘adam’ 和 ‘bart’都能返回True。 既然大小写是不同的。那我们的set中就把大小写都包含。 实现代码: 123s = set(['Adam', 'Lisa', 'Bart', 'Paul','adam', 'lisa', 'bart', 'paul'])print 'adam' in sprint 'bart' in s 运行结果. 12TrueTrue set的特点set的内部结构和dict很像，唯一区别是不存储value，因此，判断一个元素是否在set中速度很快。 set存储的元素和dict的key类似，必须是不变对象，因此，任何可变对象是不能放入set中的。 最后，set存储的元素也是没有顺序的。 set的这些特点，可以应用在哪些地方呢？ 星期一到星期日可以用字符串&#39;MON&#39;, &#39;TUE&#39;, ... &#39;SUN&#39;表示。 假设我们让用户输入星期一至星期日的某天，如何判断用户的输入是否是一个有效的星期呢？ 可以用 if 语句判断，但这样做非常繁琐： 12345x = '???' # 用户输入的字符串if x!= 'MON' and x!= 'TUE' and x!= 'WED' ... and x!= 'SUN': print 'input error'else: print 'input ok' 注意：if 语句中的…表示没有列出的其它星期名称，测试时，请输入完整。 如果事先创建好一个set，包含&#39;MON&#39; ~ &#39;SUN&#39;： 1weekdays = set([&apos;MON&apos;, &apos;TUE&apos;, &apos;WED&apos;, &apos;THU&apos;, &apos;FRI&apos;, &apos;SAT&apos;, &apos;SUN&apos;]) 再判断输入是否有效，只需要判断该字符串是否在set中： 123456x = '???' # 用户输入的字符串if x in weekdays: print 'input ok'else: print 'input error'这样一来，代码就简单多了。 编程任务 月份也可以用set表示，请设计一个set并判断用户输入的月份是否有效。月份可以用字符串&#39;Jan&#39;, &#39;Feb&#39;, ...表示。 实现代码: 12345678910111213months = set(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul','Aug','Sep','Oct','Nov','Dec'])x1 = 'Feb'x2 = 'Sun'if x1 in months: print 'x1: ok'else: print 'x1: error'if x2 in months: print 'x2: ok'else: print 'x2: error' 运行结果: 12x1: okx2: error 遍历set由于 set 也是一个集合，所以，遍历 set 和遍历 list 类似，都可以通过 for 循环实现。 直接使用 for 循环可以遍历 set 的元素： 1234567&gt;&gt;&gt; s = set(['Adam', 'Lisa', 'Bart'])&gt;&gt;&gt; for name in s:... print name... LisaAdamBart 注意: 观察 for 循环在遍历set时，元素的顺序和list的顺序很可能是不同的，而且不同的机器上运行的结果也可能不同。 编程任务 请用 for 循环遍历如下的set，打印出 name: score 来。 1s = set([(&apos;Adam&apos;, 95), (&apos;Lisa&apos;, 85), (&apos;Bart&apos;, 59)]) 上面这个set中的每一个元素又是一个字典。 set([ ])是壳子。 (&#39;Adam&#39;, 95), (&#39;Lisa&#39;, 85), (&#39;Bart&#39;, 59)才是真正的内容 实现代码： 123s = set([('Adam', 95), ('Lisa', 85), ('Bart', 59)])for name,score in s: print name,':',score 运行结果: 123Lisa : 85Adam : 95Bart : 59 更新set(add remove)由于set存储的是一组不重复的无序元素，因此，更新set主要做两件事： 是把新的元素添加到set中 是把已有元素从set中删除。(前提是如果有) 添加元素时，用set的add()方法： 1234&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s.add(4)&gt;&gt;&gt; print sset([1, 2, 3, 4]) 如果添加的元素已经存在于set中，add()不会报错，但是不会加进去了： 1234&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s.add(3)&gt;&gt;&gt; print sset([1, 2, 3]) 删除set中的元素时，用set的remove()方法： 1234&gt;&gt;&gt; s = set([1, 2, 3, 4])&gt;&gt;&gt; s.remove(4)&gt;&gt;&gt; print sset([1, 2, 3]) 如果删除的元素不存在set中，remove()会报错： 12345&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s.remove(4)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;KeyError: 4 所以用add()可以直接添加，而remove()前需要判断。 编程任务 针对下面的set，给定一个list，对list中的每一个元素，如果在set中，就将其删除，如果不在set中，就添加进去。 12s = set([&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Paul&apos;])L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;] 实现代码： 12345678s = set(['Adam', 'Lisa', 'Paul'])L = ['Adam', 'Lisa', 'Bart', 'Paul']for name in L: if name in s: s.remove(name) else: s.add(name)print s 函数定义与调用什么是函数我们知道圆的面积计算公式为： S = πr² 当我们知道半径r的值时，就可以根据公式计算出面积。假设我们需要计算3个不同大小的圆的面积： 123456r1 = 12.34r2 = 9.08r3 = 73.1s1 = 3.14 * r1 * r1s2 = 3.14 * r2 * r2s3 = 3.14 * r3 * r3 当代码出现有规律的重复的时候，你就需要当心了，每次写3.14 * x * x不仅很麻烦，而且，如果要把3.14改成3.14159265359的时候，得全部替换。 有了函数，我们就不再每次写s = 3.14 * x * x，而是写成更有意义的函数调用 s = area_of_circle(x)，而函数 area_of_circle本身只需要写一次，就可以多次调用。 抽象是数学中非常常见的概念。举个例子： 计算数列的和，比如：1 + 2 + 3 + … + 100，写起来十分不方便，于是数学家发明了求和符号∑，可以把1 + 2 + 3 + … + 100记作： 123100∑nn=1 这种抽象记法非常强大，因为我们看到∑就可以理解成求和，而不是还原成低级的加法运算。 而且，这种抽象记法是可扩展的，比如： 123100∑(n²+1)n=1 还原成加法运算就变成了： (1 x 1 + 1) + (2 x 2 + 1) + (3 x 3 + 1) + ... + (100 x 100 + 1)可见，借助抽象，我们才能不关心底层的具体计算过程，而直接在更高的层次上思考问题。 写计算机程序也是一样，函数就是最基本的一种代码抽象的方式。 Python不但能非常灵活地定义函数，而且本身内置了很多有用的函数，可以直接调用。 编程任务 写一个函数 实现代码： 12s = area_of_circle(x)area_of_circle(x) 运行结果： 调用函数,内置函数Python内置了很多有用的函数，我们可以直接调用。 要调用一个函数，需要知道函数的名称和参数，比如求绝对值的函数 abs，它接收一个参数。 可以直接从Python的官方网站查看文档：http://docs.python.org/2/library/functions.html#abs 也可以在交互式命令行通过 help(abs)查看abs函数的帮助信息。 调用 abs 函数： 123456&gt;&gt;&gt; abs(100)100&gt;&gt;&gt; abs(-20)20&gt;&gt;&gt; abs(12.34)12.34 调用函数的时候，如果传入的参数数量不对，会报TypeError的错误，并且Python会明确地告诉你：abs()有且仅有1个参数，但给出了两个： 1234&gt;&gt;&gt; abs(1, 2)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: abs() takes exactly one argument (2 given) 如果传入的参数数量是对的，但参数类型不能被函数所接受，也会报TypeError的错误，并且给出错误信息：str是错误的参数类型： 1234&gt;&gt;&gt; abs(&apos;a&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: bad operand type for abs(): &apos;str&apos; 而比较函数 cmp(x, y) 就需要两个参数，如果 x&lt;y，返回 -1，如果 x==y，返回0，如果 x&gt;y，返回 1： 123456&gt;&gt;&gt; cmp(1, 2)-1&gt;&gt;&gt; cmp(2, 1)1&gt;&gt;&gt; cmp(3, 3)0 Python内置的常用函数还包括数据类型转换函数，比如 int()函数可以把其他数据类型转换为整数： 1234&gt;&gt;&gt; int(&apos;123&apos;)123&gt;&gt;&gt; int(12.34)12 str()函数把其他类型转换成 str： 1234&gt;&gt;&gt; str(123)&apos;123&apos;&gt;&gt;&gt; str(1.23)&apos;1.23&apos; 编程任务 sum()函数接受一个list作为参数，并返回list所有元素之和。请计算 1*1 + 2*2 + 3*3 + ... + 100*100。 实现代码： 1234567L = []L = []x = 1while x &lt;= 100: L.append(x * x) x = x + 1print sum(L) 运行结果: 1338350 编写函数在Python中，定义一个函数要使用 def 语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用 return语句返回。 我们以自定义一个求绝对值的 my_abs 函数为例： 12345def my_abs(x): if x &gt;= 0: return x else: return -x 请注意，函数体内部的语句在执行时，一旦执行到return时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。 知识点； 如果没有return语句，函数执行完毕后也会返回结果，只是结果为 None。return None可以简写为return。 编程任务 请定义一个 square_of_sum 函数，它接受一个list，返回list中每个元素平方的和。 实现代码: 12345678def square_of_sum(L): sum = 0 for x in L: sum = x*x+sum return sumprint square_of_sum([1, 2, 3, 4, 5])print square_of_sum([-5, 0, 5, 15, 25]) 运行结果: 1255900 函数之返回”多值”函数可以返回多个值吗？答案是肯定的。 比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移和角度，就可以计算出新的坐标： math包提供了sin()和 cos()函数，我们先用import引用它： 12345import mathdef move(x, y, step, angle): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny 这样我们就可以同时获得返回值： 123&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print x, y151.961524227 70.0 但其实这只是一种假象，Python函数返回的仍然是单一值： 123&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print r(151.96152422706632, 70.0) 知识点：用print打印返回结果，原来返回值是一个tuple！ 但是，在语法上，返回一个tuple可以省略括号，而多个变量可以同时接收一个tuple，按位置赋给对应的值，所以，知识点：Python的函数返回多值其实就是返回一个tuple，但写起来更方便。 编程任务 一元二次方程的定义是：ax² + bx + c = 0 请编写一个函数，返回一元二次方程的两个解。 注意：Python的math包提供了sqrt()函数用于计算平方根。 实现代码: 12345678import mathdef quadratic_equation(a, b, c): t = math.sqrt(b*b - 4*a*c) return (-b + t) / (2 * a),( -b - t )/ (2 * a)print quadratic_equation(2, 3, 0)print quadratic_equation(1, -6, 5) 运行结果: 12(0.0, -1.5)(5.0, 1.0) 递归函数在函数内部，可以调用其他函数。知识点: 如果一个函数在内部调用自身本身，这个函数就是递归函数。 举个例子，我们来计算阶乘 n! = 1 * 2 * 3 * ... * n，用函数 fact(n)表示，可以看出： fact(n) = n! = 1 * 2 * 3 * ... * (n-1) * n = (n-1)! * n = fact(n-1) * n所以，fact(n)可以表示为 n * fact(n-1)，只有n=1时需要特殊处理。 于是，fact(n)用递归的方式写出来就是： 1234def fact(n): if n==1: return 1 return n * fact(n - 1) 上面就是一个递归函数。可以试试： 123456&gt;&gt;&gt; fact(1)1&gt;&gt;&gt; fact(5)120&gt;&gt;&gt; fact(100)93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000 如果我们计算fact(5)，可以根据函数定义看到计算过程如下： 12345678910===&gt; fact(5)===&gt; 5 * fact(4)===&gt; 5 * (4 * fact(3))===&gt; 5 * (4 * (3 * fact(2)))===&gt; 5 * (4 * (3 * (2 * fact(1))))===&gt; 5 * (4 * (3 * (2 * 1)))===&gt; 5 * (4 * (3 * 2))===&gt; 5 * (4 * 6)===&gt; 5 * 24===&gt; 120 递归函数的优点是定义简单，逻辑清晰。知识点: 理论上，所有的递归函数都可以写成循环的方式，但循环的逻辑不如递归清晰。 知识点: 使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。可以试试计算 fact(10000)。 编程任务(天涯) 汉诺塔 (http://baike.baidu.com/view/191666.htm) 的移动也可以看做是递归函数。 我们对柱子编号为a, b, c，将所有圆盘从a移到c可以描述为： 如果a只有一个圆盘，可以直接移动到c； 如果a有N个圆盘，可以看成a有1个圆盘（底盘） + (N-1)个圆盘，首先需要把 (N-1) 个圆盘移动到 b，然后，将 a的最后一个圆盘移动到c，再将b的(N-1)个圆盘移动到c。 请编写一个函数，给定输入 n, a, b, c，打印出移动的步骤： 1move(n, a, b, c) 例如，输入 move(2, ‘A’, ‘B’, ‘C’)，打印出： 123A --&gt; BA --&gt; CB --&gt; C 实现代码： 12345678def move(n, a, b, c): if n ==1: print a, '--&gt;', c return move(n-1, a, c, b) print a, '--&gt;', c move(n-1, b, a, c)move(4, 'A', 'B', 'C') 运行结果: 123456789101112131415A --&gt; BA --&gt; CB --&gt; CA --&gt; BC --&gt; AC --&gt; BA --&gt; BA --&gt; CB --&gt; CB --&gt; AC --&gt; AB --&gt; CA --&gt; BA --&gt; CB --&gt; C 定义默认参数定义函数的时候，还可以有默认参数。 例如Python自带的 int() 函数，其实就有两个参数，我们既可以传一个参数，又可以传两个参数： 1234&gt;&gt;&gt; int(&apos;123&apos;)123&gt;&gt;&gt; int(&apos;123&apos;, 8)83 知识点: int()函数的第二个参数是转换进制，如果不传，默认是十进制 (base=10)，如果传了，就用传入的参数。 可见，函数的默认参数的作用是简化调用，你只需要把必须的参数传进去。但是在需要的时候，又可以传入额外的参数来覆盖默认参数值。 我们来定义一个计算 x 的N次方的函数: 123456def power(x, n): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 假设计算平方的次数最多，我们就可以把 n 的默认值设定为 2： 123456def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 这样一来，计算平方就不需要传入两个参数了： 12&gt;&gt;&gt; power(5)25 知识点: 由于函数的参数按从左到右的顺序匹配，所以默认参数只能定义在必需参数的后面： 123456# OK:def fn1(a, b=1, c=2): pass# Error:def fn2(a=1, b): pass 个人: 这里我们可以把自己想象成计算机。在自己感到为难不知道哪个是哪个的时候。那么恭喜你，计算机也不知道。 编程任务 请定义一个 greet()函数，它包含一个默认参数，如果没有传入，打印 &#39;Hello, world.&#39;，如果传入，打印 &#39;Hello, xxx.&#39; 实现代码: 12345def greet(x = 'World'): print 'Hello,'+x+'.'greet()greet('mtianyan') 运行结果: 12Hello,World.Hello,mtianyan. 知识点: 定义可变参数如果想让一个函数能接受任意个参数，我们就可以定义一个可变参数： 12def fn(*args): print args 可变参数的名字前面有个 * 号，我们可以传入0个、1个或多个参数给可变参数： 12345678&gt;&gt;&gt; fn()()&gt;&gt;&gt; fn('a')('a',)&gt;&gt;&gt; fn('a', 'b')('a', 'b')&gt;&gt;&gt; fn('a', 'b', 'c')('a', 'b', 'c') 可变参数也不是很神秘，Python解释器会把传入的一组参数组装成一个tuple传递给可变参数，因此，在函数内部，直接把变量 args 看成一个 tuple 就好了。 定义可变参数的目的也是为了简化调用。假设我们要计算任意个数的平均值，就可以定义一个可变参数： 12def average(*args): ... 这样，在调用的时候，可以这样写： 123456&gt;&gt;&gt; average()0&gt;&gt;&gt; average(1, 2)1.5&gt;&gt;&gt; average(1, 2, 2, 3, 4)2.4 编程任务 请编写接受可变参数的 average() 函数。 12345678910def average(*args): sum = 0.0 if len(args) == 0: return sum for x in args: sum = sum + x return sum / len(args)print average()print average(1, 2)print average(1, 2, 2, 3, 4) 运行结果: 1230.01.52.4 切片操作对list进行切片取一个list的部分元素是非常常见的操作。比如，一个list如下： 1&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;] 取前3个元素，应该怎么做？ 笨办法： 12&gt;&gt;&gt; [L[0], L[1], L[2]][&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 之所以是笨办法是因为扩展一下，取前N个元素就没辙了。 取前N个元素，也就是索引为0-(N-1)的元素，可以用循环： 1234567&gt;&gt;&gt; r = []&gt;&gt;&gt; n = 3&gt;&gt;&gt; for i in range(n):... r.append(L[i])... &gt;&gt;&gt; r[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 对这种经常取指定索引范围的操作，用循环十分繁琐，因此，Python提供了切片（Slice）操作符，能大大简化这种操作。 对应上面的问题，取前3个元素，用一行代码就可以完成切片： 123&gt;&gt;&gt; L[0:3][&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;]L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。 知识点： [0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。 如果第一个索引是0，还可以省略： 12&gt;&gt;&gt; L[:3][&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;] 也可以从索引1开始，取出2个元素出来： 12&gt;&gt;&gt; L[1:3][&apos;Lisa&apos;, &apos;Bart&apos;] 只用一个 : ，表示从头到尾： 12&gt;&gt;&gt; L[:][&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;] 因此，L[:]实际上复制出了一个新list。 知识点: 切片操作还可以指定第三个参数： 12&gt;&gt;&gt; L[::2][&apos;Adam&apos;, &apos;Bart&apos;] 第三个参数表示每N个取一个，上面的 L[::2] 会每两个元素取出一个来，也就是隔一个取一个。 把list换成tuple，切片操作完全相同，只是切片的结果也变成了tuple。 编程任务 range()函数可以创建一个数列： 12&gt;&gt;&gt; range(1, 101)[1, 2, 3, ..., 100] 请利用切片，取出： 前10个数； 3的倍数； 不大于50的5的倍数。 实现代码: 12345L = range(1, 101)print L[:10]print L[2::3]print L[4:50:5] 运行结果: 123[1, 2, 3, 4, 5, 6, 7, 8, 9, 10][3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99][5, 10, 15, 20, 25, 30, 35, 40, 45, 50] 倒序切片对于list，既然Python支持L[-1]取倒数第一个元素，那么它同样支持倒数切片，试试： 12345678910111213&gt;&gt;&gt; L = ['Adam', 'Lisa', 'Bart', 'Paul']&gt;&gt;&gt; L[-2:]['Bart', 'Paul']&gt;&gt;&gt; L[:-2]['Adam', 'Lisa']&gt;&gt;&gt; L[-3:-1]['Lisa', 'Bart']&gt;&gt;&gt; L[-4:-1:2]['Adam', 'Bart'] 记住倒数第一个元素的索引是-1。知识点：倒序切片包含起始索引，不包含结束索引。 编程任务 利用倒序切片对 1 - 100 的数列取出： 最后10个数； 最后10个5的倍数。 实现代码： 123L = range(1, 101)print L[-10:]print L[-46::5] 对字符串切片字符串 &#39;xxx&#39;和 Unicode字符串 u&#39;xxx&#39;也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串： 123456&gt;&gt;&gt; &apos;ABCDEFG&apos;[:3]&apos;ABC&apos;&gt;&gt;&gt; &apos;ABCDEFG&apos;[-3:]&apos;EFG&apos;&gt;&gt;&gt; &apos;ABCDEFG&apos;[::2]&apos;ACEG&apos; 在很多编程语言中，针对字符串提供了很多各种截取函数，其实目的就是对字符串切片。知识点：Python没有针对字符串的截取函数，只需要切片一个操作就可以完成，非常简单。 编程任务 字符串有个方法 upper() 可以把字符变成大写字母： 12&gt;&gt;&gt; &apos;abc&apos;.upper()&apos;ABC&apos; 但它会把所有字母都变成大写。请设计一个函数，它接受一个字符串，然后返回一个仅首字母变成大写的字符串。 提示：利用切片操作简化字符串操作。 实现代码: 123456def firstCharUpper(s): return s[0].upper() + s[1:]print firstCharUpper('hello')print firstCharUpper('sunday')print firstCharUpper('september') 运行结果： 123HelloSundaySeptember 各种迭代方式什么是迭代在Python中，如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。 在Python中，迭代是通过 for ... in 来完成的，而很多语言比如C或者Java，迭代list是通过下标完成的，比如Java代码： 123for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 可以看出，Python的for循环抽象程度要高于Java的for循环。 因为 Python 的 for循环不仅可以用在list或tuple上，还可以作用在其他任何可迭代对象上。 因此，迭代操作就是对于一个集合，无论该集合是有序还是无序，我们用 for 循环总是可以依次取出集合的每一个元素。 注意: 集合是指包含一组元素的数据结构，我们已经介绍的包括： 有序集合：list，tuple，知识点: str和unicode； 无序集合：set 无序集合并且具有 key-value 对：dict 而迭代是一个动词，它指的是一种操作，在Python中，就是 for 循环。 迭代与按下标访问数组最大的不同是，后者是一种具体的迭代实现方式，而前者只关心迭代结果，根本不关心迭代内部是如何实现的。 编程任务 请用for循环迭代数列 1-100 并打印出7的倍数。 实现代码: 123for i in range(1, 101): if i % 7 == 0: print i 运行结果: 1234567891011121314714212835424956637077849198 索引迭代知识点：Python中，迭代永远是取出元素本身，而非元素的索引。 对于有序集合，元素确实是有索引的。有的时候，我们确实想在 for 循环中拿到索引，怎么办？ 方法是使用 enumerate()函数： 12345678&gt;&gt;&gt; L = [&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;]&gt;&gt;&gt; for index, name in enumerate(L):... print index, &apos;-&apos;, name... 0 - Adam1 - Lisa2 - Bart3 - Paul 使用 enumerate()函数，我们可以在for循环中同时绑定索引index和元素name。但是，这不是 enumerate() 的特殊语法。实际上，enumerate() 函数把： 1[&apos;Adam&apos;, &apos;Lisa&apos;, &apos;Bart&apos;, &apos;Paul&apos;] 变成了类似： 1[(0, &apos;Adam&apos;), (1, &apos;Lisa&apos;), (2, &apos;Bart&apos;), (3, &apos;Paul&apos;)] 因此，迭代的每一个元素实际上是一个tuple： 1234for t in enumerate(L): index = t[0] name = t[1] print index, &apos;-&apos;, name 如果我们知道每个tuple元素都包含两个元素，for循环又可以进一步简写为： 12for index, name in enumerate(L): print index, &apos;-&apos;, name 这样不但代码更简单，而且还少了两条赋值语句。 可见，知识点: 索引迭代也不是真的按索引访问，而是由 enumerate() 函数自动把每个元素变成 (index, element) 这样的tuple，再迭代，就同时获得了索引和元素本身。 编程任务(天涯) zip()函数可以把两个 list 变成一个 list： 12&gt;&gt;&gt; zip([10, 20, 30], [&apos;A&apos;, &apos;B&apos;, &apos;C&apos;])[(10, &apos;A&apos;), (20, &apos;B&apos;), (30, &apos;C&apos;)] 在迭代 [&#39;Adam&#39;, &#39;Lisa&#39;, &#39;Bart&#39;, &#39;Paul&#39;]时，如果我们想打印出名次 - 名字（名次从1开始)，请考虑如何在迭代中打印出来。 提示：考虑使用zip()函数和range()函数 实现代码: 123L = ['Adam', 'Lisa', 'Bart', 'Paul']for index, name in zip(range(1, len(L)+1), L): print index, '-', name 运行结果: 12341 - Adam2 - Lisa3 - Bart4 - Paul 迭代dict的value迭代dict的value我们已经了解了dict对象本身就是可迭代对象，用 for 循环直接迭代 dict，可以每次拿到dict的一个key。 如果我们希望迭代 dict 对象的value，应该怎么做？ 知识点：values()把dict转换成一个包含所有value的listdict 对象有一个 values() 方法，这个方法把dict转换成一个包含所有value的list，这样，我们迭代的就是 dict的每一个 value： 12345678d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59 &#125;print d.values()# [85, 95, 59]for v in d.values(): print v# 85# 95# 59 如果仔细阅读Python的文档，还可以发现，dict除了values()方法外，还有一个 itervalues() 方法，用 itervalues() 方法替代 values() 方法，迭代效果完全一样： 12345678d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59 &#125;print d.itervalues()# &lt;dictionary-valueiterator object at 0x106adbb50&gt;for v in d.itervalues(): print v# 85# 95# 59 那这两个方法有何不同之处呢？ values() 方法实际上把一个 dict 转换成了包含 value 的list。 但是 itervalues() 方法不会转换，它会在迭代过程中依次从 dict 中取出 value，所以 itervalues() 方法比 values() 方法节省了生成 list 所需的内存。 打印 itervalues() 发现它返回一个 对象，这说明在Python中，for 循环可作用的迭代对象远不止 list，tuple，str，unicode，dict等，知识点: 任何可迭代对象都可以作用于for循环，而内部如何迭代我们通常并不用关心。 如果一个对象说自己可迭代，那我们就直接用 for 循环去迭代它，知识点: 可见，迭代是一种抽象的数据操作，它不对迭代对象内部的数据有任何要求。 编程任务 给定一个dict：d = { ‘Adam’: 95, ‘Lisa’: 85, ‘Bart’: 59, ‘Paul’: 74 } 请计算所有同学的平均分。 实现代码: 12345d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59, &apos;Paul&apos;: 74 &#125;sum = 0.0for v in d.itervalues(): sum = sum + vprint sum / len(d) 运行结果: 178.25 迭代dict的key和value我们了解了如何迭代 dict 的key和value，那么，在一个 for 循环中，能否同时迭代 key和value？答案是肯定的。 首先，我们看看 dict 对象的 items()方法返回的值： 123&gt;&gt;&gt; d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59 &#125;&gt;&gt;&gt; print d.items()[(&apos;Lisa&apos;, 85), (&apos;Adam&apos;, 95), (&apos;Bart&apos;, 59)] 可以看到，items() 方法把dict对象转换成了包含tuple的list，我们对这个list进行迭代，可以同时获得key和value： 123456&gt;&gt;&gt; for key, value in d.items():... print key, &apos;:&apos;, value... Lisa : 85Adam : 95Bart : 59 和 values()有一个 itervalues() 类似，items() 也有一个对应的 iteritems()，知识点： iteritems() 不把dict转换成list，而是在迭代过程中不断给出 tuple，所以， iteritems() 不占用额外的内存。 编程任务 请根据dict：d = { ‘Adam’: 95, ‘Lisa’: 85, ‘Bart’: 59, ‘Paul’: 74 } 打印出 name : score，最后再打印出平均分 average : score。 实现代码： 123456d = &#123; 'Adam': 95, 'Lisa': 85, 'Bart': 59, 'Paul': 74 &#125;sum = 0.0for k, v in d.iteritems(): sum = sum + v print k, ':', vprint 'average', ':', sum / len(d) 运行结果： 12345Lisa : 85Paul : 74Adam : 95Bart : 59average : 78.25 列表生成式:快速生成列表生成列表要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]，我们可以用range(1, 11)： 12&gt;&gt;&gt; range(1, 11)[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做？ 方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)... &gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 这种写法就是Python特有的列表生成式。利用列表生成式，可以以非常简洁的代码生成 list。 知识点: 写列表生成式时，把要生成的元素 x * x放到前面，后面跟 for 循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。 编程任务 请利用列表生成式生成列表 [1x2, 3x4, 5x6, 7x8, ..., 99x100] 提示：range(1, 100, 2)可以生成list [1, 3, 5, 7, 9,...] 实现代码： 1print [x * (x + 1) for x in range(1, 100, 2)] 运行结果： 1[2, 12, 30, 56, 90, 132, 182, 240, 306, 380, 462, 552, 650, 756, 870, 992, 1122, 1260, 1406, 1560, 1722, 1892, 2070, 2256, 2450, 2652, 2862, 3080, 3306, 3540, 3782, 4032, 4290, 4556, 4830, 5112, 5402, 5700, 6006, 6320, 6642, 6972, 7310, 7656, 8010, 8372, 8742, 9120, 9506, 9900] 复杂表达式使用for循环的迭代不仅可以迭代普通的list，还可以迭代dict。 假设有如下的dict： 1d = &#123; 'Adam': 95, 'Lisa': 85, 'Bart': 59 &#125; 完全可以通过一个复杂的列表生成式把它变成一个 HTML 表格： 12345tds = ['&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;' % (name, score) for name, score in d.iteritems()]print '&lt;table&gt;'print '&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;tr&gt;'print '\n'.join(tds)print '&lt;/table&gt;' 个人：&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt; 中： 第一个%s是name的填充位置。 第二个%s为score的填充位置。 有多少个name和score，会通过循环生成多少个。&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;tr&gt;设置表格头print ‘\n’.join(tds)。列表里的项通过\n连接成字符串。 注：字符串可以通过%进行格式化，用指定的参数替代 %s。字符串的join()方法可以把一个 list拼接成一个字符串。 把打印出来的结果保存为一个html文件，就可以在浏览器中看到效果了： 123456&lt;table border="1"&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;tr&gt;&lt;tr&gt;&lt;td&gt;Lisa&lt;/td&gt;&lt;td&gt;85&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Adam&lt;/td&gt;&lt;td&gt;95&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Bart&lt;/td&gt;&lt;td&gt;59&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 编程任务(天涯) 在生成的表格中，对于没有及格的同学，请把分数标记为红色。 提示：红色可以用 &lt;td style=&quot;color:red&quot;&gt; 实现。 实现代码: 12345678910d = &#123; &apos;Adam&apos;: 95, &apos;Lisa&apos;: 85, &apos;Bart&apos;: 59 &#125;def generate_tr(name, score): if score &lt; 60: return &apos;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td style=&quot;color:red&quot;&gt;%s&lt;/td&gt;&lt;/tr&gt;&apos; % (name, score) return &apos;&lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;&apos; % (name, score)tds = [generate_tr(name, score) for name, score in d.iteritems()]print &apos;&lt;table border=&quot;1&quot;&gt;&apos;print &apos;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;tr&gt;&apos;print &apos;\n&apos;.join(tds)print &apos;&lt;/table&gt;&apos; 运行结果: 条件过滤列表生成式的 for 循环后面还可以加上 if 判断。例如： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 如果我们只想要偶数的平方，不改动 range()的情况下，可以加上 if 来筛选： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 有了 if 条件，只有 if 判断为 True 的时候，才把循环的当前元素添加到列表中。 编程任务 请编写一个函数，它接受一个 list，然后把list中的所有字符串变成大写后返回，非字符串元素将被忽略。 提示： isinstance(x, str) 可以判断变量 x 是否是字符串； 字符串的 upper() 方法可以返回大写的字母。 实现代码: 123def toUppers(L): return [x.upper() for x in L if isinstance(x, str)]print toUppers(['Hello', 'world', 101]) 运行结果: 1[&apos;HELLO&apos;, &apos;WORLD&apos;] 多层表达式(知识点)for循环可以嵌套，知识点：因此，在列表生成式中，也可以用多层 for 循环来生成列表。 对于字符串 &#39;ABC&#39; 和 &#39;123&#39;，可以使用两层循环，生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in '123']['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'C1', 'C2', 'C3'] 翻译成循环代码就像下面这样： 1234L = []for m in 'ABC': for n in '123': L.append(m + n) 编程任务(天涯) 利用 3 层for循环的列表生成式，找出对称的 3 位数。例如，121 就是对称数，因为从右到左倒过来还是 121。 实现代码: 1print [100 * n1 + 10 * n2 + n3 for n1 in range(1, 10) for n2 in range(10) for n3 in range(10) if n1==n3] 运行结果： 1101, 111, 121, 131, 141, 151, 161, 171, 181, 191, 202, 212, 222, 232, 242, 252, 262, 272, 282, 292, 303, 313, 323, 333, 343, 353, 363, 373, 383, 393, 404, 414, 424, 434, 444, 454, 464, 474, 484, 494, 505, 515, 525, 535, 545, 555, 565, 575, 585, 595, 606, 616, 626, 636, 646, 656, 666, 676, 686, 696, 707, 717, 727, 737, 747, 757, 767, 777, 787, 797, 808, 818, 828, 838, 848, 858, 868, 878, 888, 898, 909, 919, 929, 939, 949, 959, 969, 979, 989, 999]]]></content>
      <categories>
        <category>python从入门到精通</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>零基础入门</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python开发环境搭建指南(Anaconda2,3共存)]]></title>
    <url>%2Fpost%2F230a7ad6.html</url>
    <content type="text"><![CDATA[工欲利器事必先善其器 对于Python环境搭建的指南，分为普通版和进阶版。推荐进阶版：Windows下Anaconda2(Python2)和Anaconda3(Python3)的共存 [√] 慕课网Python开发环境搭建: 配开发环境 Python开发环境搭建搭建环境分为两个版本： 基础版，供初学者快速安装体验。 进阶版, 供对于数据科学，机器学习有兴趣者安装。 推荐：安装进阶版，一步到位。 基础版：Windows下安装python环境(2.7 | 3.x) 官网： https://www.python.org/ 安装包下载。选择download下的windows。点击进入。 下图中红框为64位版本。32位版本可以选择Windows x86 executable installer 2.7版本的安装包下载： 安装python。点击下一步下一步进行默认安装即可。（跟平常装个qq啥的没两样） 安装完成的测试。win（即徽标键） + R 输入cmd ：打开命令行。输入python不报错的进入python控制台下。 进阶版：Windows下Anaconda2(Python2)和Anaconda3(Python3)的共存转载 原文地址：http://blog.csdn.net/infin1te/article/details/50445217 Anaconda是一个Python的科学计算发行版，包含了超过300个流行的用于科学、数学、工程和数据分析的Python Packages。由于Python有2和3两个版本，因此Anaconda也在Python2和Python3的基础上推出了两个发行版，即Anaconda2和Anaconda3。 以上文字摘自转载博客。通俗讲就是一个python的各种科学计算包的大合集版本。省去了自己安装大量基本包的过程。 Tips: 3.x版本建议选择Python 3.5.1 |Anaconda 4.1.0 (64-bit) 以后如果要使用python进行TensorFlow windows版的配置可以省下时间。 这是博主自入python坑以来找到的最好的共存方法，没有出过问任何题！！！ 这是博主自入python坑以来找到的最好的共存方法，没有出过问任何题！！！ 这是博主自入python坑以来找到的最好的共存方法，没有出过问任何题！！！ Linux下的python使用。 Linux 默认安装python，建议安装IPython； sudo apt-get install ipython安装Ipython（支持Tab键自动补齐） 使用Vim来创建.py文件 输入python即可查看当前版本 IPython是Python的交互式Shell，提供了代码自动补完，自动缩进，高亮显示，执行Shell命令等非常有用的特性。特别是它的代码补完功能. python文件类型(常识)python执行过程： .py文件 –&gt; python解释器 –&gt; 字节码文件 –&gt; python解释器 –&gt; 二进制文件 –&gt; 内存、运行 –&gt; 打印结果 字节码文件: .pyc转换方式： python -m py_compile xxx.py作用：提高程序的加载速度 .pyo(优化编译的.pyc文件)转换方式： python -O -m py_compile xxx.py作用：提高程序的运行速度 eclipse下的python环境安装。添加python开发环境到eclipse： 点击help——install New Software 点击add，弹出新窗口： Name:填PyDev Location:填 http://pydev.org/updates 确认后会出现 PyDev，勾选Pydev。 pydev for eclipse–&gt; next–&gt; accept–&gt;finish file–&gt; new–&gt; project –&gt;Pydev下 pydevProject python的dev下载地址：http://pydev.org/updates]]></content>
      <tags>
        <tag>Python</tag>
        <tag>零基础入门</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Next主题搭建博客安装美化及SEO优化指南]]></title>
    <url>%2Fpost%2Fa625fa82.html</url>
    <content type="text"><![CDATA[做博客最重要的是用心啦 最终成果当然是本站了。安装，美化，SEO推广，部署通通奉上。 持续集成自动部署已提上日程：已完成。 查看持续集成：《用TravisCI持续集成自动部署Hexo博客的个人实践》 Hexo + Next主题 搭建博客 “真”零基础安装配置最终成果: 本站 node.js官网下载: 下一步下一步安装法 node-v8.9.3-x64 C:\softEnvir\nodejs 验证安装: 设置npm下载目录123npm config set prefix D:\softEnvDown\nodejs\node_globalnpm config set cache D:\softEnvDown\nodejs\node_cachenpm config ls npm模块安装的默认全局路径已经改到了相应的文件夹中，但是 这时候用户还是无法require这些模块。因为电脑系统现在还不知道你把默认路径给改了，所以需要在win+R–&gt;“sysdm.cpl–&gt;高级–&gt;环境变量打开设置对话框。 首先在系统变量中新建，新建一个名为NODE_PATH的变量，变量值: 全局模块的文件夹中的node_modules文件夹的绝对路径. 即：D:\softEnvDown\nodejs\node_global\node_modules 配置文件目录：C:\Users\mtian\.npmrc 安装cnpmnpm install -g cnpm --registry=https://registry.npm.taobao.org Git安装Git-2.15.1.2-64-bit（同样官网下载: 下一步下一步安装法） C:\softEnvir\Git hexo-cli: 1.0.4安装cmd控制台下: `npm install -g hexo-cli` 初始化hexo博客目录D:/mtianBlog: hexo init hexoBlog 下载next主题及主题美化官方文档地址： http://theme-next.iissnan.com/ 下载主题next：v5.1.4。名字改为next 然后放入D:\mtianBlog\hexoBlog\themes 123hexo cleanhexo s --debug 主题美化。推荐阅读：https://www.jianshu.com/p/f054333ac9e6 部分节选： 圆形头像旋转。 打开\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 配置百度统计站长版。(官方文档有写) baidu_analytics: * 配置leancloud (官方文档有写) leancloud_visitors: enable: true app_id: * app_key: # 配置搜索(更推荐localSearch) 123456789algolia: applicationID: &apos;*&apos; apiKey: &apos;*&apos; adminApiKey: &apos;*&apos; indexName: &apos;*&apos; chunkSize: 5000$ export(windows 为 set) HEXO_ALGOLIA_INDEXING_KEY=***$ hexo algolia 您好！当我集成Algolia时候一直出错——上传不了记录，请问知道如何解决嘛？ 答案地址：https://github.com/iissnan/theme-next-docs/issues/162 使用gulp进行博文压缩（持续集成自动部署前提）在站点的根目录下执行以下命令： 12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 新建文件gulpfile.js: 内容为： 123456789101112131415161718192021222324252627282930313233var gulp = require(&apos;gulp&apos;);var minifycss = require(&apos;gulp-minify-css&apos;);var uglify = require(&apos;gulp-uglify&apos;);var htmlmin = require(&apos;gulp-htmlmin&apos;);var htmlclean = require(&apos;gulp-htmlclean&apos;);// 压缩 public 目录 cssgulp.task(&apos;minify-css&apos;, function() &#123; return gulp.src(&apos;./public/**/*.css&apos;) .pipe(minifycss()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 压缩 public 目录 htmlgulp.task(&apos;minify-html&apos;, function() &#123; return gulp.src(&apos;./public/**/*.html&apos;) .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest(&apos;./public&apos;))&#125;);// 压缩 public/js 目录 jsgulp.task(&apos;minify-js&apos;, function() &#123; return gulp.src(&apos;./public/**/*.js&apos;) .pipe(uglify()) .pipe(gulp.dest(&apos;./public&apos;));&#125;);// 执行 gulp 命令时执行的任务gulp.task(&apos;default&apos;, [ &apos;minify-html&apos;,&apos;minify-css&apos;,&apos;minify-js&apos;]); 部署hexo：npm install hexo-deployer-git --save 修改站点配置文件： 12345deploy: type: git repo: https://github.com/mtianyan/mtianyan.github.io branch: master message: new version first commit 需要使用的命令 123456789git config --global user.email &quot;1147727180@qq.com&quot;git config --global user.name &quot;mtianyan&quot;hexo cleanhexo g &amp;&amp; gulphexo d 将博客源码备份到github或码云(持续集成自动部署前提)12345git initgit add .git remote add origin https://github.com/mtianyan/hexoBlog-Github.gitgit commit -m &quot;first&quot;git push -u origin master 想要弄到码云替换第三行为下面： 1git remote add origin https://gitee.com/mtianyan/hexoBlog-mayun.git SEO优化指南URL唯一化:1npm install hexo-abbrlink --save 站点配置文件里添加: 1234permalink: post/:abbrlink.htmlabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex 安装sitemap站点地图生成插件12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 在站点配置文件或主题配置文件加入。 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 注意缩进 在hexo-site\source中新建文件robots.txt,内容如下，请自行替换 123456789101112131415User-agent: *Allow: /Allow: /archives/Allow: /categories/Allow: /tags/ Allow: /about/ Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: http://mtianyan.gitee.io/sitemap.xmlSitemap: http://mtianyan.gitee.io/baidusitemap.xml Allow后面的就是你的menu 可以在主题配置文件中找到。 前往链接：https://www.google.com/webmasters/添加你的网站。 下载验证文件放入hexo-site\source中 Tips: 站点配置文件忽略Google的验证文件。这样clean的时候就不会被删除了。 123skip_render: - README.md - google*****.html 打开站点地图：添加站点地图。 可以进入站点地图详情查看有没有报错。 robots.txt测试 确保0错误，0警告。 百度搜索资源平台http://ziyuan.baidu.com/ 点击用户中心。站点管理 进行 添加网站操作。 下载验证文件放入hexo-site\source中 验证完成之后点击域名进入控制台。 对于Robots文件进行验证。 点击链接提交，往下多翻点 选择sitemap：填入自己网站的sitemap地址。点击提交 查看状态。 页面关键字优化title 文件路径是your-hexo-site\themes\next\layout\index.swig 将文件中 1&#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125; 改为 1&#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;% endif %&#125;&#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125;]]></content>
      <categories>
        <category>运维环境配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next主题</tag>
        <tag>网站搭建</tag>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员装机必备爆款软件推荐与配置(windows版)]]></title>
    <url>%2Fpost%2F9237fc73.html</url>
    <content type="text"><![CDATA[做机也要做一只全能的机哦 值此新年来临之即，面对两百多个G的c盘。忍痛割爱将电脑系统重装，版本为(win10:1079)之后的所有电脑环境更新，专业软件安装均会记录在此文。 程序员装机必备爆款软件推荐与配置。 win10系统使用技巧 程序员装机必备爆款软件推荐与配置。编程软件篇sublimetextC:\software\Sublime Text 3 包管理地址: https://packagecontrol.io/installation Chinese Localization 汉化ctrl + shift + p 打开package install 安装插件Chinese Localization即可。 Theme - Afterglow 主题主题设置： Sublime Text -&gt; Preferences -&gt; Settings - User: 123456789101112&#123; &quot;color_scheme&quot;: &quot;Packages/Theme - Afterglow/Afterglow-twilight.tmTheme&quot;, &quot;ignored_packages&quot;: [ &quot;Vintage&quot; ], &quot;theme&quot;: &quot;Afterglow-blue.sublime-theme&quot;, &quot;tabs_small&quot;: true, &quot;sidebar_size_12&quot;: true, &quot;color_inactive_tabs&quot;: true, &quot;tabs_padding_small&quot;: true&#125; Markdown 插件必备 markdown preview markdown LivePrivew OmniMarkupPreviewer markdown_snippetshttps://praveenpuglia.com/github_markdown_snippets/ 自定义snippet设置(Tab补全) 加入快捷键pyb,快速创建代码块(Tab补全)+pyb 工具 -&gt; 插件开发 -&gt;新建代码片段 123456789&lt;snippet&gt; &lt;content&gt; 代码模板&lt;/content&gt; &lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt; &lt;tabTrigger&gt;pyb&lt;/tabTrigger&gt; &lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt; &lt;scope&gt;text.html.markdown&lt;/scope&gt;&lt;/snippet&gt; 加入新建hexo博文的快捷模板(tab补全)+hexopy 123456789101112131415161718192021222324&lt;snippet&gt; &lt;content&gt;---title: $&#123;1:&#125;date: $&#123;2:&#125;tags:- Python- $&#123;3:&#125;- $&#123;4:&#125; categories: $&#123;5:python从入门到精通&#125;copyright: true ---&#123;% cq %&#125; $&#123;6:&#125; &#123;% endcq %&#125;&#123;% note %&#125; $&#123;7:&#125; &#123;% endnote %&#125;\&lt;!--more--&gt;\&lt;/content&gt; &lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt; &lt;tabTrigger&gt;hexopy&lt;/tabTrigger&gt; &lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt; &lt;scope&gt;text.html.markdown&lt;/scope&gt;&lt;/snippet&gt; ctrl + shift +, sublime插入当前时间快捷键设置参考： https://stackoverflow.com/questions/11879481/can-i-add-date-time-for-sublime-snippet 加入直接插入当前时间的快捷键：ctrl + shift +, 新建插件: datetimestamp.py 1234567891011121314import datetime, getpassimport sublime, sublime_pluginclass AddDateTimeStampCommand(sublime_plugin.TextCommand): def run(self, edit): self.view.run_command("insert_snippet", &#123; "contents": "%s" % datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") &#125; )class AddDateStampCommand(sublime_plugin.TextCommand): def run(self, edit): self.view.run_command("insert_snippet", &#123; "contents": "%s" % datetime.datetime.now().strftime("%Y-%m-%d") &#125; )class AddTimeStampCommand(sublime_plugin.TextCommand): def run(self, edit): self.view.run_command("insert_snippet", &#123; "contents": "%s" % datetime.datetime.now().strftime("%H:%M:%S") &#125; ) 用户快捷键设置: 12345[&#123;&quot;keys&quot;: [&quot;shift+ctrl+,&quot;], &quot;command&quot;: &quot;add_date_time_stamp&quot; &#125;,&#123;&quot;keys&quot;: [&quot;shift+ctrl+.&quot;], &quot;command&quot;: &quot;add_date_stamp&quot; &#125;,&#123;&quot;keys&quot;: [&quot;shift+ctrl+/&quot;], &quot;command&quot;: &quot;add_time_stamp&quot; &#125;] sulimeTempel(新建文件模板)插件未完待续 设置python3环境默认python2环境为安装Python自动设置的。而python3环境则需要自行配置。 1234567&#123;&quot;encoding&quot;: &quot;utf-8&quot;, &quot;working_dir&quot;: &quot;$file_path&quot;, &quot;shell_cmd&quot;: &quot;D:\\softEnvDown\\Anaconda2\\envs\\py3\\python.exe -u \&quot;$file\&quot;&quot;, &quot;file_regex&quot;: &quot;^[ ]*File \&quot;(...*?)\&quot;, line ([0-9]*)&quot;, &quot;selector&quot;: &quot;source.python&quot;&#125; Python.sublime-settings用来替换Tab为4个空格。 1234&#123; &quot;tab_size&quot;: 4, &quot;translate_tabs_to_spaces&quot;: true&#125; Anaconda Python代码补全PEP8规范检查弹窗报错。 修改首选项-&gt;package setting -&gt; Anaconda -&gt; setting -default 123456 &quot;test_params&quot;: &#123; &quot;current_file_tests&quot;: &quot;&quot;, &quot;current_test&quot;: &quot;&quot;, &quot;project_tests&quot;: &quot;&quot; &#125;,&quot;swallow_startup_errors&quot;: true, Python PEP8 autoformatctrl + shift + R AutoFileNamecmder(替代windowscmd的神器)C:\greenSoft\cmder_mini cmder右键菜单添加，修改默认路径为桌面添加环境变量 在管理员权限下打开cmd，输入以下命令增加右键菜单 Cmder.exe /REGISTER ALL 选择Startup-Task，修改{cmd::Cmder}项，改为: cmd /k &quot;%ConEmuDir%\..\init.bat&quot; -new_console:d:C:\Users\mtian\Desktop 删除右键菜单(备用) 123456@echo off Reg delete &quot;HKEY_CLASSES_ROOT\Directory\Background\shell\Cmder&quot; /f pausecmd /k &quot;%ConEmuDir%\..\init.bat&quot; -new_console:%__CD_ 设置右键打开当前目录cmder Startup:specified named task shell::cmd cmd /k “%ConEmuDir%..\init.bat” -new_console:d:%CD% editplus安装C:\software\editplus 注册码：http://www.98key.com/editplus?sub=mtianyan VMware-workstationhttps://download3.vmware.com/software/wkst/file/VMware-workstation-full-14.1.0-7370693.exe xshell 5C:\software\Xshell 图床软件 mpicC:\greenSoft\MPic 2.2.1.3 java8u151http://www.oracle.com/technetwork/java/javase/downloads/ C:\softEnvir\java C:\softEnvir\java\jdk1.8.0_152\bin C:\softEnvir\java\jre1.8.0_152\bin eclipse-oxygen安装C:\software\eclipse\java-oxygen PyCharm 2017.3.2C:\Windows\System32\drivers\etc host文件 0.0.0.0 account.jetbrains.com 添加解释器： setting - Project Interpreter： 一直定位到python.exe点击确认。 普通软件chrome浏览器C:\Program Files (x86)\Google\Chrome\Application Shadowsocks &amp; kcptunoffice2016安装7zipC:\software\7-Zip Tim安装目录：C:\software\tim文件接收： C:\Users\mtian\Desktop聊天记录：C:\softData\QQuser C:\Users\mtian\OneDrive\Shadowsocks c盘绿色软件记录C:\greenSoft cmder pdf password 流程图 冰点文库 win10系统使用技巧win10开机启动项修改C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp 去除右键windows denferwin +r regedithttps://jingyan.baidu.com/article/4b52d702ae79affc5c774be3.html 编程环境Python相关环境anaconda下载： https://repo.continuum.io/archive/ Anaconda2: D:\softEnvDown\Anaconda2\ Anaconda3: D:\softEnvDown\Anaconda2\envs\py3 注意: py3不是自己创建的新文件夹，而是在路径上打字指定。 Ruby环境C:\softEnvir\Ruby24-x64]]></content>
      <categories>
        <category>运维环境配置</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- 系列完整版]]></title>
    <url>%2Fpost%2F3c234171.html</url>
    <content type="text"><![CDATA[震惊了李彦宏 , 看哭了马化腾 基于Scrapy、Redis、elasticsearch和django打造一个完整的搜索引擎网站 本教程一共八章：从零开始，直到搭建一个搜索引擎。 项目github代码地址：https://github.com/mtianyan/ArticleSpider 未来是什么时代？是数据时代！数据分析服务、互联网金融，数据建模、自然语言处理、医疗病例分析……越来越多的工作会基于数据来做，而爬虫正是快速获取数据最重要的方式，相比其它语言，Python爬虫更简单、高效 一、基础知识学习:1. 爬取策略的深度优先和广度优先目录： 网站的树结构 深度优先算法和实现 广度优先算法和实现 网站url树结构分层设计: bogbole.com blog.bogbole.com python.bogbole.com python.bogbole.com/123 环路链接问题： 从首页到下面节点。但是下面的链接节点又会有链接指向首页 所以：我们需要对于链接进行去重 1. 深度优先2. 广度优先 跳过已爬取的链接对于二叉树的遍历问题 深度优先(递归实现)： 顺着一条路，走到最深处。然后回头 广度优先(队列实现): 分层遍历：遍历完儿子辈。然后遍历孙子辈 Python实现深度优先过程code：1234567def depth_tree(tree_node): if tree_node is not None: print (tree_node._data) if tree_node._left is not None: return depth_tree(tree_node.left) if tree_node._right is not None: return depth_tree(tree_node,_right) Python实现广度优先过程code：1234567891011121314def level_queue(root): #利用队列实现树的广度优先遍历 if root is None: return my_queue = [] node = root my_queue.append(node) while my_queue: node = my_queue.pop(0) print (node.elem) if node.lchild is not None: my_queue.append(node.lchild) if node.rchild is not None: my_queue.append(node.rchild) 2. 爬虫网址去重策略 将访问过的url保存到数据库中 将url保存到set中。只需要O(1)的代价就可以查询到url 100000000*2byte*50个字符/1024/1024/1024 = 9G url经过md5等方法哈希后保存到set中，将url压缩到固定长度而且不重复 用bitmap方法，将访问过的url通过hash函数映射到某一位 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 scrapy去重使用的是第三种方法：后面分布式scrapy-redis会讲解bloomfilter方法。 3. Python字符串编码问题解决： 计算机只能处理数字，文本转换为数字才能处理，计算机中8个bit作为一个字节，所以一个字节能表示的最大数字就是255 计算机是美国人发明的，所以一个字节就可以标识所有单个字符，所以ASCII(一个字节)编码就成为美国人的标准编码 但是ASCII处理中文明显不够，中文不止255个汉字，所以中国制定了GB2312编码，用两个字节表示一个汉字。GB2312将ASCII也包含进去了。同理，日文，韩文，越来越多的国家为了解决这个问题就都发展了一套编码，标准越来越多，如果出现多种语言混合显示就一定会出现乱码 于是unicode出现了，它将所有语言包含进去了。 看一下ASCII和unicode编码: 字母A用ASCII编码十进制是65，二进制 0100 0001 汉字”中” 已近超出ASCII编码的范围，用unicode编码是20013二进制是01001110 00101101 A用unicode编码只需要前面补0二进制是 00000000 0100 0001 乱码问题解决的，但是如果内容全是英文，unicode编码比ASCII编码需要多一倍的存储空间，传输也会变慢。 所以此时出现了可变长的编码”utf-8” ,把英文：1字节，汉字3字节，特别生僻的变成4-6字节，如果传输大量的英文，utf8作用就很明显。 读取文件，进行操作时转换为unicode编码进行处理保存文件时，转换为utf-8编码。以便于传输读文件的库会将转换为unicode python2 默认编码格式为ASCII，Python3 默认编码为 utf-81234#python3import syssys.getdefaultencoding()s.encoding('utf-8') 12345678#python2import syssys.getdefaultencoding()s = "我和你"su = u"我和你"~~s.encode("utf-8")#会报错~~s.decode("gb2312").encode("utf-8")su.encode("utf-8") 二、伯乐在线爬取所有文章1. 初始化文件目录基础环境 python 3.5.1 JetBrains PyCharm 2016.3.2 mysql+navicat 为了便于日后的部署：我们开发使用了虚拟环境。 1234567891011pip install virtualenvpip install virtualenvwrapper-win安装虚拟环境管理mkvirtualenv articlespider3创建虚拟环境workon articlespider3直接进入虚拟环境deactivate退出激活状态workon知道有哪些虚拟环境 scrapy项目初始化介绍 自行官网下载py35对应得whl文件进行pip离线安装Scrapy 1.3.3 命令行创建scrapy项目123cd desktopscrapy startproject ArticleSpider scrapy目录结构 scrapy借鉴了django的项目思想 scrapy.cfg：配置文件。 setings.py：设置 12SPIDER_MODULES = [&apos;ArticleSpider.spiders&apos;] #存放spider的路径NEWSPIDER_MODULE = &apos;ArticleSpider.spiders&apos; pipelines.py: 做跟数据存储相关的东西 middilewares.py: 自己定义的middlewares 定义方法，处理响应的IO操作 init.py: 项目的初始化文件。 items.py： 定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据 创建我们的spider12cd ArticleSpiderscrapy genspider jobbole blog.jobbole.com 可以看到直接为我们创建好的空项目里已经有了模板代码。如下： 123456789101112# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] # start_urls是一个带爬的列表， #spider会为我们把请求下载网页做到，直接到parse阶段 start_urls = ['http://blog.jobbole.com/'] def parse(self, response): pass scray在命令行启动某一个Spyder的命令:1scrapy crawl jobbole 在windows报出错误 ImportError: No module named &#39;win32api&#39; 1pip install pypiwin32#解决 创建我们的调试工具类* 在项目根目录里创建main.py作为调试工具文件1234567891011121314151617# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/3/28 12:06'from scrapy.cmdline import executeimport sysimport os#将系统当前目录设置为项目根目录#os.path.abspath(__file__)为当前文件所在绝对路径#os.path.dirname为文件所在目录#H:\CodePath\spider\ArticleSpider\main.py#H:\CodePath\spider\ArticleSpidersys.path.append(os.path.dirname(os.path.abspath(__file__)))#执行命令，相当于在控制台cmd输入改名了execute(["scrapy", "crawl" , "jobbole"]) settings.py的设置不遵守reboots协议 ROBOTSTXT_OBEY = False 在jobble.py打上断点:12def parse(self, response): pass 可以看到他返回的htmlresponse对象:对象内部： body:网页内容 _DEFAULT_ENCODING= ‘ascii’ encoding= ‘utf-8’ 可以看出scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换. 2. 提取伯乐在线内容xpath的使用xpath让你可以不懂前端html，不看html的详细结构，只需要会右键查看就能获取网页上任何内容。速度远超beautifulsoup。目录: 1. xpath简介 2. xpath术语与语法 3. xpath抓取误区：javasrcipt生成html与html源文件的区别 4. xpath抓取实例 为什么要使用xpath？ xpath使用路径表达式在xml和html中进行导航 xpath包含有一个标准函数库 xpath是一个w3c的标准 xpath速度要远远超beautifulsoup。 xpath节点关系 父节点*上一层节点* 子节点 兄弟节点*同胞节点* 先辈节点*父节点，爷爷节点* 后代节点*儿子，孙子*xpath语法: 表达式 说明 article 选取所有article元素的所有子节点 /article 选取根元素article article/a 选取所有属于article的子元素的a元素 //div 选取所有div元素（不管出现在文档里的任何地方） article//div 选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置 //@class 选取所有名为class的属性 xpath语法-谓语: 表达式 说明 /article/div[1 选取属于article子元素的第一个div元素 /article/div[last()] 选取属于article子元素的最后一个div元素 /article/div[last()-1] 选取属于article子元素的倒数第二个div元素 //div[@color] 选取所有拥有color属性的div元素 //div[@color=’red’] 选取所有color属性值为red的div元素 xpath语法: 表达式 说明 /div/* 选取属于div元素的所有子节点 //* 选取所有元素 //div[@*] 选取所有带属性的div 元素 //div/a 丨//div/p 选取所有div元素的a和p元素 //span丨//ul 选取文档中的span和ul元素 article/div/p丨//span 选取所有属于article元素的div元素的p元素以及文档中所有的 span元素 xpath抓取误区 firebugs插件 取某一个网页上元素的xpath地址 如:http://blog.jobbole.com/110287/ 在标题处右键使用firebugs查看元素。然后在&lt;h1&gt;2016 腾讯软件开发面试题（部分）&lt;/h1&gt;右键查看xpath 123456789101112# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): re_selector = response.xpath("/html/body/div[3]/div[3]/div[1]/div[1]/h1") # print(re_selector) pass 调试debug可以看到1re_selector =(selectorlist)[] 可以看到返回的是一个空列表，列表是为了如果我们当前的xpath路径下还有层级目录时可以进行选取空说明没取到值： 我们可以来chorme里观察一下 chorme取到的值//*[@id=&quot;post-110287&quot;]/div[1]/h1 chormexpath代码 12345678910111213# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): re_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1') # print(re_selector) pass 可以看出此时可以取到值 分析页面，可以发现页面内有一部html是通过JavaScript ajax交互来生成的，因此在f12检查元素时的页面结构里有，而xpath不对xpath是基于html源代码文件结构来找的 xpath可以有多种多样的写法： 123re_selector = response.xpath("/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()")re2_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1/text()')re3_selector = response.xpath('//div[@class="entry-header“]/h1/text()') 推荐使用id型。因为页面id唯一。 推荐使用class型，因为后期循环爬取可扩展通用性强。 通过了解了这些此时我们已经可以抓取到页面的标题，此时可以使用xpath利器照猫画虎抓取任何内容。只需要点击右键查看xpath。 开启控制台调试 scrapy shell http://blog.jobbole.com/110287/ 完整的xpath提取伯乐在线字段代码 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import scrapyimport reclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): #提取文章的具体字段 title = response.xpath('//div[@class="entry-header"]/h1/text()').extract_first("") create_date = response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace("·","").strip() praise_nums = response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0] fav_nums = response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = match_re.group(1) comment_nums = response.xpath("//a[@href='#article-comment']/span/text()").extract()[0] match_re = re.match(".*?(\d+).*", comment_nums) if match_re: comment_nums = match_re.group(1) content = response.xpath("//div[@class='entry']").extract()[0] tag_list = response.xpath("//p[@class='entry-meta-hide-on-mobile']/a/text()").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) pass css选择器的使用：12345678910111213141516171819202122232425# 通过css选择器提取字段 # front_image_url = response.meta.get("front_image_url", "") #文章封面图 title = response.css(".entry-header h1::text").extract_first() create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip() praise_nums = response.css(".vote-post-up h10::text").extract()[0] fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0] match_re = re.match(".*?(\d+).*", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css("div.entry").extract()[0] tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) pass 3. 爬取所有文章yield关键字 12#使用request下载详情页面，下载完成后回调方法parse_detail()提取文章内容中的字段yield Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) scrapy.http import Request下载网页 12from scrapy.http import RequestRequest(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) parse拼接网址应对herf内有可能网址不全 1234from urllib import parseurl=parse.urljoin(response.url,post_url)parse.urljoin("http://blog.jobbole.com/all-posts/","http://blog.jobbole.com/111535/")#结果为http://blog.jobbole.com/111535/ class层级关系 12next_url = response.css(".next.page-numbers::attr(href)").extract_first("")#如果.next .pagenumber 是指两个class为层级关系。而不加空格为同一个标签 twist异步机制 Scrapy使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。在任何情况下，都不要写阻塞的代码。阻塞的代码包括: 访问文件、数据库或者Web 产生新的进程并需要处理新进程的输出，如运行shell命令 执行系统层次操作的代码，如等待系统队列 实现全部文章字段下载的代码： 1234567891011121314151617181920def parse(self, response): """ 1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析 2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse """ # 解析列表页中的所有文章url并交给scrapy下载后并进行解析 post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() for post_url in post_urls: #request下载完成之后，回调parse_detail进行文章详情页的解析 # Request(url=post_url,callback=self.parse_detail) print(response.url) print(post_url) yield Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) #遇到href没有域名的解决方案 #response.url + post_url print(post_url) # 提取下一页并交给scrapy进行下载 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse) 全部文章的逻辑流程图 4. scrapy的items整合字段数据爬取的任务就是从非结构的数据中提取出结构性的数据。items 可以让我们自定义自己的字段（类似于字典，但比字典的功能更齐全） 在当前页，需要提取多个url 原始写法,extract之后则生成list列表，无法进行二次筛选： 1post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() 改进写法: 12345post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: #获取封面图的url image_url = post_node.css("img::attr(src)").extract_first("") post_url = post_node.css("::attr(href)").extract_first("") 在下载网页的时候把获取到的封面图的url传给parse_detail的response在下载网页时将这个封面url获取到，并通过meta将他发送出去。在callback的回调函数中接收该值 123yield Request(url=parse.urljoin(response.url,post_url),meta=&#123;"front_image_url":image_url&#125;,callback=self.parse_detail)front_image_url = response.meta.get("front_image_url", "") urljoin的好处如果你没有域名，我就从response里取出来，如果你有域名则我对你起不了作用了 编写我们自定义的item并在jobboled.py中填充。 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() content = scrapy.Field() tags = scrapy.Field() import之后实例化，实例化之后填充： 12345678910111. from ArticleSpider.items import JobBoleArticleItem2. article_item = JobBoleArticleItem()3. article_item["title"] = title article_item["url"] = response.url article_item["create_date"] = create_date article_item["front_image_url"] = [front_image_url] article_item["praise_nums"] = praise_nums article_item["comment_nums"] = comment_nums article_item["fav_nums"] = fav_nums article_item["tags"] = tags article_item["content"] = content yield article_item将这个item传送到pipelines中pipelines可以接收到传送过来的item将setting.py中的pipeline配置取消注释12345# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &apos;ArticleSpider.pipelines.ArticlespiderPipeline&apos;: 300,&#125; 当我们的item被传输到pipeline我们可以将其进行存储到数据库等工作 setting设置下载图片pipeline123ITEM_PIPELINES=&#123;&apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,&#125; H:\CodePath\pyEnvs\articlespider3\Lib\site-packages\scrapy\pipelines里面有三个scrapy默认提供的pipeline提供了文件，图片，媒体。 ITEM_PIPELINES是一个数据管道的登记表，每一项具体的数字代表它的优先级，数字越小，越早进入。 setting设置下载图片的地址12# IMAGES_MIN_HEIGHT = 100# IMAGES_MIN_WIDTH = 100 设置下载图片的最小高度，宽度。 新建文件夹images在 123IMAGES_URLS_FIELD = "front_image_url"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images') 安装PILpip install pillow 定制自己的pipeline使其下载图片后能保存下它的本地路径get_media_requests()接收一个迭代器对象下载图片item_completed获取到图片的下载地址 继承并重写item_completed() 12345678910from scrapy.pipelines.images import ImagesPipelineclass ArticleImagePipeline(ImagesPipeline): #重写该方法可从result中获取到图片的实际下载地址 def item_completed(self, results, item, info): for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item setting中设置使用我们自定义的pipeline，而不是系统自带的 12345ITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.ArticlespiderPipeline': 300, # 'scrapy.pipelines.images.ImagesPipeline': 1, 'ArticleSpider.pipelines.ArticleImagePipeline':1,&#125; 图片url的md5处理新建package utils 123456789import hashlibdef get_md5(url): m = hashlib.md5() m.update(url) return m.hexdigest()if __name__ == "__main__": print(get_md5("http://jobbole.com".encode("utf-8"))) 不确定用户传入的是不是: 1234567def get_md5(url): #str就是unicode了 if isinstance(url, str): url = url.encode("utf-8") m = hashlib.md5() m.update(url) return m.hexdigest() 在jobbole.py中将url的md5保存下来 12from ArticleSpider.utils.common import get_md5article_item["url_object_id"] = get_md5(response.url) 5. 数据保存到本地文件以及mysql中保存到本地json文件import codecs打开文件避免一些编码问题，自定义JsonWithEncodingPipeline实现json本地保存 123456789101112class JsonWithEncodingPipeline(object): #自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): #将item转换为dict，然后生成json对象，false避免中文出错 lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item #当spider关闭的时候 def spider_closed(self, spider): self.file.close() setting.py注册pipeline 12345ITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.JsonWithEncodingPipeline': 2, # 'scrapy.pipelines.images.ImagesPipeline': 1, 'ArticleSpider.pipelines.ArticleImagePipeline':1,&#125; scrapy exporters JsonItemExporter导出 scrapy自带的导出： - &apos;CsvItemExporter&apos;, - &apos;XmlItemExporter&apos;, - &apos;JsonItemExporter&apos; from scrapy.exporters import JsonItemExporter 1234567891011121314class JsonExporterPipleline(object): #调用scrapy提供的json export导出json文件 def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 设置setting.py注册该pipeline 1'ArticleSpider.pipelines.JsonExporterPipleline ': 2 保存到数据库(mysql)数据库设计数据表，表的内容字段是和item一致的。数据库与item的关系。类似于django中model与form的关系。日期的转换，将字符串转换为datetime 12345import datetime try: create_date = datetime.datetime.strptime(create_date, "%Y/%m/%d").date() except Exception as e: create_date = datetime.datetime.now().date() 数据库表设计 三个num字段均设置不能为空，然后默认0. content设置为longtext 主键设置为url_object_id 数据库驱动安装pip install mysqlclient Linux报错解决方案:ubuntu:sudo apt-get install libmysqlclient-devcentos:sudo yum install python-devel mysql-devel 保存到数据库pipeline(同步）编写 1234567891011121314import MySQLdbclass MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', 'mima', 'article_spider', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) self.conn.commit() 保存到数据库的(异步Twisted)编写因为我们的爬取速度可能大于数据库存储的速度。异步操作。设置可配置参数seeting.py设置1234MYSQL_HOST = &quot;127.0.0.1&quot;MYSQL_DBNAME = &quot;article_spider&quot;MYSQL_USER = &quot;root&quot;MYSQL_PASSWORD = &quot;123456&quot; 代码中获取到设置的可配置参数twisted异步： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import MySQLdb.cursorsfrom twisted.enterprise import adbapi#连接池ConnectionPool# def __init__(self, dbapiName, *connargs, **connkw):class MysqlTwistedPipline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USER"], passwd = settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) #**dbparms--&gt;("MySQLdb",host=settings['MYSQL_HOST'] dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor, item): #执行具体的插入 #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params)``` 可选django.itemshttps://github.com/scrapy-plugins/scrapy-djangoitem可以让我们保存的item直接变成django的models.#### scrapy的itemloader来维护提取代码itemloadr提供了一个容器，让我们配置某一个字段该使用哪种规则。add_css add_value add_xpath```pythonfrom scrapy.loader import ItemLoader# 通过item loader加载item front_image_url = response.meta.get("front_image_url", "") # 文章封面图 item_loader = ItemLoader(item=JobBoleArticleItem(), response=response) item_loader.add_css("title", ".entry-header h1::text") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("create_date", "p.entry-meta-hide-on-mobile::text") item_loader.add_value("front_image_url", [front_image_url]) item_loader.add_css("praise_nums", ".vote-post-up h10::text") item_loader.add_css("comment_nums", "a[href='#article-comment'] span::text") item_loader.add_css("fav_nums", ".bookmark-btn::text") item_loader.add_css("tags", "p.entry-meta-hide-on-mobile a::text") item_loader.add_css("content", "div.entry") #调用这个方法来对规则进行解析生成item对象 article_item = item_loader.load_item() 所有值变成了list 对于这些值做一些处理函数item.py中对于item process处理函数MapCompose可以传入函数对于该字段进行处理，而且可以传入多个 1234567from scrapy.loader.processors import MapComposedef add_mtianyan(value): return value+"-mtianyan" title = scrapy.Field( input_processor=MapCompose(lambda x:x+"mtianyan",add_mtianyan), ) 注意：此处的自定义方法一定要写在代码前面。 1234create_date = scrapy.Field( input_processor=MapCompose(date_convert), output_processor=TakeFirst()) 只取list中的第一个值。 自定义itemloader实现默认提取第一个 123class ArticleItemLoader(ItemLoader): #自定义itemloader实现默认提取第一个 default_output_processor = TakeFirst() list保存原值 123456def return_value(value): return valuefront_image_url = scrapy.Field( output_processor=MapCompose(return_value) ) 下载图片pipeline增加if增强通用性 123456789class ArticleImagePipeline(ImagesPipeline): #重写该方法可从result中获取到图片的实际下载地址 def item_completed(self, results, item, info): if "front_image_url" in item: for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item 自定义的item带处理函数的完整代码 1234567891011121314151617181920212223242526class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field( input_processor=MapCompose(date_convert), ) url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field( output_processor=MapCompose(return_value) ) front_image_path = scrapy.Field() praise_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) comment_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) fav_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) #因为tag本身是list，所以要重写 tags = scrapy.Field( input_processor=MapCompose(remove_comment_tags), output_processor=Join(",") ) content = scrapy.Field() 三、知乎网问题和答案爬取1. 基础知识session和cookie机制 cookie：浏览器支持的存储方式key-value http无状态请求，两次请求没有联系 session的工作原理 （1）当一个session第一次被启用时，一个唯一的标识被存储于本地的cookie中。 （2）首先使用session_start()函数，从session仓库中加载已经存储的session变量。 （3）通过使用session_register()函数注册session变量。 （4）脚本执行结束时，未被销毁的session变量会被自动保存在本地一定路径下的session库中. request模拟知乎的登录http状态码 获取crsftoken 12345678910def get_xsrf(): #获取xsrf code response = requests.get("https://www.zhihu.com",headers =header) # # print(response.text) # text ='&lt;input type="hidden" name="_xsrf" value="ca70366e5de5d133c3ae09fb16d9b0fa"/&gt;' match_obj = re.match('.*name="_xsrf" value="(.*?)"', response.text) if match_obj: return (match_obj.group(1)) else: return "" python模拟知乎登录代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# _*_ coding: utf-8 _*_import requeststry: import cookielibexcept: import http.cookiejar as cookielibimport re__author__ = 'mtianyan'__date__ = '2017/5/23 16:42'import requeststry: import cookielibexcept: import http.cookiejar as cookielibimport resession = requests.session()session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")try: session.cookies.load(ignore_discard=True)except: print ("cookie未能加载")agent = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36"header = &#123; "HOST":"www.zhihu.com", "Referer": "https://www.zhizhu.com", 'User-Agent': agent&#125;def is_login(): #通过个人中心页面返回状态码来判断是否为登录状态 inbox_url = "https://www.zhihu.com/question/56250357/answer/148534773" response = session.get(inbox_url, headers=header, allow_redirects=False) if response.status_code != 200: return False else: return Truedef get_xsrf(): #获取xsrf code response = session.get("https://www.zhihu.com", headers=header) response_text = response.text #reDOTAll 匹配全文 match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) return xsrfdef get_index(): response = session.get("https://www.zhihu.com", headers=header) with open("index_page.html", "wb") as f: f.write(response.text.encode("utf-8")) print ("ok")def get_captcha(): import time t = str(int(time.time()*1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) t = session.get(captcha_url, headers=header) with open("captcha.jpg","wb") as f: f.write(t.content) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") return captchadef zhihu_login(account, password): #知乎登录 if re.match("^1\d&#123;10&#125;",account): print ("手机号码登录") post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": get_xsrf(), "phone_num": account, "password": password, "captcha":get_captcha() &#125; else: if "@" in account: #判断用户名是否为邮箱 print("邮箱方式登录") post_url = "https://www.zhihu.com/login/email" post_data = &#123; "_xsrf": get_xsrf(), "email": account, "password": password &#125; response_text = session.post(post_url, data=post_data, headers=header) session.cookies.save()# get_index()# is_login()# get_captcha()zhihu_login("phone", "mima") 2. scrapy创建知乎爬虫登录1scrapy genspider zhihu www.zhihu.com 因为知乎我们需要先进行登录，所以我们重写它的start_requests 12def start_requests(self): return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)] 下载首页然后回调login函数。 login函数请求验证码并回调login_after_captcha函数.此处通过meta将post_data传送出去，后面的回调函数来用。 1234567891011121314151617181920212223def login(self, response): response_text = response.text #获取xsrf。 match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) if xsrf: post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": xsrf, "phone_num": "phone", "password": "mima", "captcha": "" &#125; import time t = str(int(time.time() * 1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) #请求验证码并回调login_after_captcha. yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;"post_data":post_data&#125;, callback=self.login_after_captcha) login_after_captcha函数将验证码图片保存到本地，然后使用PIL库打开图片，肉眼识别后在控制台输入验证码值然后接受步骤一的meta数据，一并提交至登录接口。回调check_login检查是否登录成功。 123456789101112131415161718192021222324def login_after_captcha(self, response): with open("captcha.jpg", "wb") as f: f.write(response.body) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") post_data = response.meta.get("post_data", &#123;&#125;) post_url = "https://www.zhihu.com/login/phone_num" post_data["captcha"] = captcha return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] check_login函数，验证服务器的返回数据判断是否成功scrapy会对request的URL去重(RFPDupeFilter)，加上dont_filter则告诉它这个URL不参与去重. 源码中的startrequest: 123def start_requests(self): for url in self.start_urls: yield self.make_requests_from_url(url) 我们将原本的start_request的代码放在了现在重写的，回调链最后的check_login 123456def check_login(self, response): #验证服务器的返回数据判断是否成功 text_json = json.loads(response.text) if "msg" in text_json and text_json["msg"] == "登录成功": for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) ###3. 知乎数据表设计 上图为知乎答案版本1 上图为知乎答案版本2 设置数据表字段 问题字段 回答字段 zhihu_id zhihu_id topics url url question_id title author_id content content answer_num parise_num comments_num comments_num watch_user_num create_time click_num update_time crawl_time crawl_time 知乎url分析 点具体问题下查看更多。可获得接口： https://www.zhihu.com/api/v4/questions/25914034/answers?include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=43&amp;sort_by=default 重点参数：offset=43isend = truenext href=”/question/25460323” 1all_urls = [parse.urljoin(response.url, url) for url in all_urls] 从首页获取所有a标签。如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数parse_question如果不是question页面则直接进一步跟踪。 123456789101112131415161718def parse(self, response): """ 提取出html页面中的所有url 并跟踪这些url进行一步爬取 如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数 """ all_urls = response.css("a::attr(href)").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] #使用lambda函数对于每一个url进行过滤，如果是true放回列表，返回false去除。 all_urls = filter(lambda x:True if x.startswith("https") else False, all_urls) for url in all_urls: match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", url) if match_obj: # 如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) else: # 如果不是question页面则直接进一步跟踪 yield scrapy.Request(url, headers=self.headers, callback=self.parse) 进入parse_question函数处理创建我们的item item要用到的方法ArticleSpider\utils\common.py： 123456789def extract_num(text): #从字符串中提取出数字 match_re = re.match(".*?(\d+).*", text) if match_re: nums = int(match_re.group(1)) else: nums = 0 return nums setting.py中设置SQL_DATETIME_FORMAT = &quot;%Y-%m-%d %H:%M:%S&quot; SQL_DATE_FORMAT = &quot;%Y-%m-%d&quot;使用： 1from ArticleSpider.settings import SQL_DATETIME_FORMAT 知乎的问题 item 1234567891011121314151617181920212223242526272829303132333435363738394041424344class ZhihuQuestionItem(scrapy.Item): #知乎的问题 item zhihu_id = scrapy.Field() topics = scrapy.Field() url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() answer_num = scrapy.Field() comments_num = scrapy.Field() watch_user_num = scrapy.Field() click_num = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) """ zhihu_id = self["zhihu_id"][0] topics = ",".join(self["topics"]) url = self["url"][0] title = "".join(self["title"]) content = "".join(self["content"]) answer_num = extract_num("".join(self["answer_num"])) comments_num = extract_num("".join(self["comments_num"])) if len(self["watch_user_num"]) == 2: watch_user_num = int(self["watch_user_num"][0]) click_num = int(self["watch_user_num"][1]) else: watch_user_num = int(self["watch_user_num"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, params 知乎问题回答item 123456789101112131415161718192021222324252627282930313233class ZhihuAnswerItem(scrapy.Item): #知乎的问题回答item zhihu_id = scrapy.Field() url = scrapy.Field() question_id = scrapy.Field() author_id = scrapy.Field() content = scrapy.Field() parise_num = scrapy.Field() comments_num = scrapy.Field() create_time = scrapy.Field() update_time = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num), update_time=VALUES(update_time) """ create_time = datetime.datetime.fromtimestamp(self["create_time"]).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self["update_time"]).strftime(SQL_DATETIME_FORMAT) params = ( self["zhihu_id"], self["url"], self["question_id"], self["author_id"], self["content"], self["parise_num"], self["comments_num"], create_time, update_time, self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params 有了两个item之后，我们继续完善我们的逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041def parse_question(self, response): #处理question页面， 从页面中提取出具体的question item if "QuestionHeader-title" in response.text: #处理新版本 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) item_loader.add_css("title", "h1.QuestionHeader-title::text") item_loader.add_css("content", ".QuestionHeader-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", ".List-headerText span::text") item_loader.add_css("comments_num", ".QuestionHeader-actions button::text") item_loader.add_css("watch_user_num", ".NumberBoard-value::text") item_loader.add_css("topics", ".QuestionHeader-topics .Popover div::text") question_item = item_loader.load_item() else: #处理老版本页面的item提取 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) # item_loader.add_css("title", ".zh-question-title h2 a::text") item_loader.add_xpath("title", "//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()") item_loader.add_css("content", "#zh-question-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", "#zh-question-answer-num::text") item_loader.add_css("comments_num", "#zh-question-meta-wrap a[name='addcomment']::text") # item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text") item_loader.add_xpath("watch_user_num", "//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()") item_loader.add_css("topics", ".zm-tag-editor-labels a::text") question_item = item_loader.load_item() yield scrapy.Request(self.start_answer_url.format(question_id, 20, 0), headers=self.headers, callback=self.parse_answer) yield question_item 处理问题回答提取出需要的字段 123456789101112131415161718192021222324def parse_answer(self, reponse): #处理question的answer ans_json = json.loads(reponse.text) is_end = ans_json["paging"]["is_end"] next_url = ans_json["paging"]["next"] #提取answer的具体字段 for answer in ans_json["data"]: answer_item = ZhihuAnswerItem() answer_item["zhihu_id"] = answer["id"] answer_item["url"] = answer["url"] answer_item["question_id"] = answer["question"]["id"] answer_item["author_id"] = answer["author"]["id"] if "id" in answer["author"] else None answer_item["content"] = answer["content"] if "content" in answer else None answer_item["parise_num"] = answer["voteup_count"] answer_item["comments_num"] = answer["comment_count"] answer_item["create_time"] = answer["created_time"] answer_item["update_time"] = answer["updated_time"] answer_item["crawl_time"] = datetime.datetime.now() yield answer_item if not is_end: yield scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer) 知乎提取字段流程图： 深度优先： 提取出页面所有的url，并过滤掉不需要的url 如果是questionurl就进入question的解析 把该问题的爬取完了然后就返回初始解析 将item写入数据库pipelines.py错误处理插入时错误可通过该方法监控123def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) 改造pipeline使其变得更通用原本具体硬编码的pipeline 1234567def do_insert(self, cursor, item): #执行具体的插入 insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) 改写后的： 1234 def do_insert(self, cursor, item):#根据不同的item 构建不同的sql语句并插入到mysql中insert_sql, params = item.get_insert_sql()cursor.execute(insert_sql, params) 可选方法一： 1234567if item.__class__.__name__ == "JobBoleArticleItem": #执行具体的插入 insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) 推荐方法：把sql语句等放到item里面：jobboleitem类内部方法 12345678def get_insert_sql(self): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(fav_nums) """ params = (self["title"], self["url"], self["create_date"], self["fav_nums"]) return insert_sql, params 知乎问题： 12345678910111213141516171819202122232425262728293031def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) """ zhihu_id = self["zhihu_id"][0] topics = ",".join(self["topics"]) url = self["url"][0] title = "".join(self["title"]) content = "".join(self["content"]) answer_num = extract_num("".join(self["answer_num"])) comments_num = extract_num("".join(self["comments_num"])) if len(self["watch_user_num"]) == 2: watch_user_num = int(self["watch_user_num"][0]) click_num = int(self["watch_user_num"][1]) else: watch_user_num = int(self["watch_user_num"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, params 知乎回答： 1234567891011121314151617181920def get_insert_sql(self): #插入知乎回答表的sql语句 insert_sql = """ insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num), update_time=VALUES(update_time) """ create_time = datetime.datetime.fromtimestamp(self["create_time"]).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self["update_time"]).strftime(SQL_DATETIME_FORMAT) params = ( self["zhihu_id"], self["url"], self["question_id"], self["author_id"], self["content"], self["parise_num"], self["comments_num"], create_time, update_time, self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params 第二次爬取到相同数据，更新数据 12ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) 调试技巧 123456789101112if match_obj: #如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) #方便调试 breakelse: #方便调试 pass #如果不是question页面则直接进一步跟踪 #方便调试 # yield scrapy.Request(url, headers=self.headers, callback=self.parse) 12#方便调试 # yield question_item 错误排查[key error] titlepipeline中debug定位到哪一个item的错误。 四、通过CrawlSpider对招聘网站拉钩网进行整站爬取推荐工具cmderhttp://cmder.net/下载full版本，使我们在windows环境下也可以使用linux部分命令。配置path环境变量 1. 设计拉勾网的数据表结构 2. 初始化拉钩网项目并解读crawl源码scrapy genspider --list查看可使用的初始化模板ailable templates: basic crawl csvfeed xmlfeed 1scrapy genspider -t crawl lagou www.lagou.com cmd与pycharm不同，mark rootsetting.py 设置目录crawl模板 123456789101112131415class LagouSpider(CrawlSpider): name = 'lagou' allowed_domains = ['www.lagou.com'] start_urls = ['http://www.lagou.com/'] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_item(self, response): i = &#123;&#125; #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract() #i['name'] = response.xpath('//div[@id="name"]').extract() #i['description'] = response.xpath('//div[@id="description"]').extract() return i 源码阅读剖析https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider 提供了一些可以让我们进行简单的follow的规则，link，迭代爬取 rules： 规则，crawel spider读取并执行 parse_start_url(response)： example： rules是一个可迭代对象，里面有Rule实例-&gt;LinkExtractor的分析allow=(&#39;category\.php&#39;, ), callback=&#39;parse_item&#39;,allow允许的url模式。callback，要回调的函数名。因为rules里面没有self，无法获取到方法。 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)') item['name'] = response.xpath('//td[@id="item_name"]/text()').extract() item['description'] = response.xpath('//td[@id="item_description"]/text()').extract() return item 分析拉勾网模板代码 将http加上s 重命名parse_item为我们自定义的parse_job 点击class LagouSpider(CrawlSpider):的CrawlSpider，进入crawl源码 class CrawlSpider(Spider):可以看出它继承于spider 入口：def start_requests(self): alt+左右方向键，不同代码跳转 5-&gt;之后默认parse CrawlSpider里面有parse函数。但是这次我们不能向以前一样覆盖 Crawl.py核心函数parse。 parse函数调用_parse_response 12def parse(self, response): return self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=True) _parse_response 判断是否有callback即有没有self.parse_start_url 我们可以重载parse_start_url加入自己的处理 把参数传递给函数，并调用process_results函数 _parse_response函数 12345678910def _parse_response(self, response, callback, cb_kwargs, follow=True): if callback: cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item if follow and self._follow_links: for request_or_item in self._requests_to_follow(response): yield request_or_item parse_start_url的return值将会被process_results方法接收处理如果不重写，因为返回为空，然后就相当于什么都没做 12def process_results(self, response, results): return results 点击followlink 123def set_crawler(self, crawler): super(CrawlSpider, self).set_crawler(crawler) self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True) 如果setting中有这个参数，则可以进一步执行到parse _requests_to_follow 判断传入的是不是response，如果不是直接returns 针对当前response设置一个空set，去重 把self的rules通过enumerate变成一个可迭代对象 跳转rules详情 拿到link通过link_extractor.extract_links抽取出具体的link 执行我们的process_links link制作完成发起Request,回调_response_downloaded函数 然后执行parse_respose 1234567891011121314def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) r = Request(url=link.url, callback=self._response_downloaded) r.meta.update(rule=n, link_text=link.text) yield rule.process_request(r) _compile_rules 在我们初始化时会调用_compile_rules copy.copy(r) for r in self.rules]将我们的rules进行一个copy 调用回调函数get_method。 调用rules里面我们定义的process_links 调用rules里面我们定义的process_request 123456789101112def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) self.process_links = process_links self.process_request = process_request 可以通过在rules里面传入我们自己的处理函数，实现对url的自定义。达到负载均衡，多地不同ip访问。 _response_downloaded通过rule取到具体的rule调用我们自己的回调函数 123def _response_downloaded(self, response): rule = self._rules[response.meta['rule']] return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow) allow ：符合这个url我就爬取 deny : 符合这个url规则我就放弃 allow_domin : 这个域名下的我才处理 allow_domin : 这个域名下的我不处理 restrict_xpaths：进一步限定xpath 123self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None, deny_extensions=None, restrict_css=() extract_links如果有restrict_xpaths，他会进行读取执行 12345678910111213def extract_links(self, response): base_url = get_base_url(response) if self.restrict_xpaths: docs = [subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)] else: docs = [response.selector] all_links = [] for doc in docs: links = self._extract_links(doc, response.url, response.encoding, base_url) all_links.extend(self._process_links(links)) return unique_list(all_links) get_base_url: urllib.parse.urljoin替我们拼接好url 1234567891011121314151617def get_base_url(text, baseurl='', encoding='utf-8'): """Return the base url if declared in the given HTML `text`, relative to the given base url. If no base url is found, the given `baseurl` is returned. """ text = to_unicode(text, encoding) m = _baseurl_re.search(text) if m: return moves.urllib.parse.urljoin( safe_url_string(baseurl), safe_url_string(m.group(1), encoding=encoding) ) else: return safe_url_string(baseurl) 编写rule规则12345rules = ( Rule(LinkExtractor(allow=("zhaopin/.*",)), follow=True), Rule(LinkExtractor(allow=("gongsi/j\d+.html",)), follow=True), Rule(LinkExtractor(allow=r'jobs/\d+.html'), callback='parse_job', follow=True),) 3. 设计lagou的items需要用到的方法 123456789from w3lib.html import remove_tagsdef remove_splash(value): #去掉工作城市的斜线 return value.replace("/","")def handle_jobaddr(value): addr_list = value.split("\n") addr_list = [item.strip() for item in addr_list if item.strip()!="查看地图"] return "".join(addr_list) 定义好的item12345678910111213141516171819202122232425262728class LagouJobItem(scrapy.Item): #拉勾网职位信息 title = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() salary = scrapy.Field() job_city = scrapy.Field( input_processor=MapCompose(remove_splash), ) work_years = scrapy.Field( input_processor = MapCompose(remove_splash), ) degree_need = scrapy.Field( input_processor = MapCompose(remove_splash), ) job_type = scrapy.Field() publish_time = scrapy.Field() job_advantage = scrapy.Field() job_desc = scrapy.Field() job_addr = scrapy.Field( input_processor=MapCompose(remove_tags, handle_jobaddr), ) company_name = scrapy.Field() company_url = scrapy.Field() tags = scrapy.Field( input_processor = Join(",") ) crawl_time = scrapy.Field() 重写的itemloader设置默认只提取第一个 123class LagouJobItemLoader(ItemLoader): #自定义itemloader default_output_processor = TakeFirst() 4. 提取字段值并存入数据库12345678910111213141516171819202122232425def parse_job(self, response): #解析拉勾网的职位 item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response) item_loader.add_css("title", ".job-name::attr(title)") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("salary", ".job_request .salary::text") item_loader.add_xpath("job_city", "//*[@class='job_request']/p/span[2]/text()") item_loader.add_xpath("work_years", "//*[@class='job_request']/p/span[3]/text()") item_loader.add_xpath("degree_need", "//*[@class='job_request']/p/span[4]/text()") item_loader.add_xpath("job_type", "//*[@class='job_request']/p/span[5]/text()") item_loader.add_css("tags", '.position-label li::text') item_loader.add_css("publish_time", ".publish_time::text") item_loader.add_css("job_advantage", ".job-advantage p::text") item_loader.add_css("job_desc", ".job_bt div") item_loader.add_css("job_addr", ".work_addr") item_loader.add_css("company_name", "#job_company dt a img::attr(alt)") item_loader.add_css("company_url", "#job_company dt a::attr(href)") item_loader.add_value("crawl_time", datetime.now()) job_item = item_loader.load_item() return job_item 获得的拉勾网item数据 5. items中添加get_insert_sql实现存入数据库12345678910111213141516def get_insert_sql(self): insert_sql = """ insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need, job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url, tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc) """ params = ( self["title"], self["url"], self["url_object_id"], self["salary"], self["job_city"], self["work_years"], self["degree_need"], self["job_type"], self["publish_time"], self["job_advantage"], self["job_desc"], self["job_addr"], self["company_name"], self["company_url"], self["job_addr"], self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params 五、爬虫与反爬虫1. 基础知识如何使我们的爬虫不被禁止掉 爬虫： 自动获取数据的程序，关键是批量的获取 反爬虫： 使用技术手段防止爬虫程序的方法 误伤： 反爬虫技术将普通用户识别为爬虫，效果再好也不能用 学校，网吧，出口的公网ip只有一个，所以禁止ip不能用。 ip动态分配。a爬封b 成本： 反爬虫人力和机器成本 拦截： 拦截率越高，误伤率越高 反爬虫的目的： 爬虫与反爬虫的对抗过程： 使用检查可以查看到价格，而查看网页源代码无法查看到价格字段。scrapy下载到的网页时网页源代码。js（ajax）填充的动态数据无法通过网页获取到。 2. scrapy架构及源码介绍 我们编写的spider，然后yield一个request发送给engine engine拿到什么都不做然后给scheduler engine会生成一个request给engine engine拿到之后通过downloadermiddleware 给downloader downloader再发送response回来给engine。 engine拿到之后，response给spider。 spider进行处理，解析出item &amp; request， item-&gt;给itempipeline；如果是request，跳转步骤二 path：articlespider3\Lib\site-packages\scrapy\core engine.py： scheduler.py downloader item pipeline spider engine.py：重要函数schedule enqueue_request：把request放scheduler _next_request_from_scheduler:从调度器拿。 123456def schedule(self, request, spider): self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider) if not self.slot.scheduler.enqueue_request(request): self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider) articlespider3\Lib\site-packages\scrapy\core\downloader\handlers 支持文件，ftp，http下载(https). 后期定制middleware： spidermiddlewire downloadmiddlewire django和scrapy结构类似 3. scrapy的两个重要类：request和response类似于django httprequest 1yield Request(url=parse.urljoin(response.url, post_url)) request参数： 12345class Request(object_ref): def __init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None): cookies：Lib\site-packages\scrapy\downloadermiddlewares\cookies.py 1cookiejarkey = request.meta.get("cookiejar") priority: 优先级，影响调度顺序 dont_filter：我的同样的request不会被过滤 errback：错误时的回调函数 https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response errback example： 1234567891011121314151617181920212223242526272829303132333435363738394041class ErrbackSpider(scrapy.Spider): name = "errback_example" start_urls = [ "http://www.httpbin.org/", # HTTP 200 expected "http://www.httpbin.org/status/404", # Not found error "http://www.httpbin.org/status/500", # server issue "http://www.httpbin.org:12345/", # non-responding host, timeout expected "http://www.httphttpbinbin.org/", # DNS error expected ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info('Got successful response from &#123;&#125;'.format(response.url)) # do something useful here... def errback_httpbin(self, failure): # log all failures self.logger.error(repr(failure)) # in case you want to do something special for some errors, # you may need the failure's type: if failure.check(HttpError): # these exceptions come from HttpError spider middleware # you can get the non-200 response response = failure.value.response self.logger.error('HttpError on %s', response.url) elif failure.check(DNSLookupError): # this is the original request request = failure.request self.logger.error('DNSLookupError on %s', request.url) elif failure.check(TimeoutError, TCPTimedOutError): request = failure.request self.logger.error('TimeoutError on %s', request.url) response类 12def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None): self.headers = Headers(headers or &#123;&#125;) response的参数：request：yield出来的request，会放在response，让我们知道它是从哪里来的 4. 自行编写随机更换useagent setting中设置 1234user_agent_list = [ 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0', 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36',] 然后在代码中使用。 123456from settings import user_agent_list import random random_index =random.randint(0,len(user_agent_list)) random_agent = user_agent_list[random_index] 'User-Agent': random_agent 12345import randomrandom_index = random.randint(0, len(user_agent_list))random_agent = user_agent_list[random_index]self.headers["User-Agent"] = random_agentyield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) 但是问题：每个request之前都得这样做。 5. middlewire配置及编写fake UseAgent代理池取消DOWNLOADER_MIDDLEWARES的注释状态 123DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.MyCustomDownloaderMiddleware': 543,&#125; articlespider3\Lib\site-packages\scrapy\downloadermiddlewares\useragent.py 123456789101112131415161718class UserAgentMiddleware(object): """This middleware allows spiders to override the user_agent""" def __init__(self, user_agent='Scrapy'): self.user_agent = user_agent @classmethod def from_crawler(cls, crawler): o = cls(crawler.settings['USER_AGENT']) crawler.signals.connect(o.spider_opened, signal=signals.spider_opened) return o def spider_opened(self, spider): self.user_agent = getattr(spider, 'user_agent', self.user_agent) def process_request(self, request, spider): if self.user_agent: request.headers.setdefault(b'User-Agent', self.user_agent) 重要方法process_request 配置默认useagent为none 1234DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.MyCustomDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None&#125; 使用fakeuseragentpip install fake-useragent settinf.py设置随机模式RANDOM_UA_TYPE = &quot;random&quot; 123456789101112131415161718from fake_useragent import UserAgentclass RandomUserAgentMiddlware(object): #随机更换user-agent def __init__(self, crawler): super(RandomUserAgentMiddlware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 6. 使用西刺代理创建ip代理池保存到数据库*ip动态变化：重启路由器等 ip代理的原理： 不直接发送自己真实ip，而使用中间代理商（代理服务器），那么服务器不知道我们的ip也就不会把我们禁掉setting.py设置 1234class RandomProxyMiddleware(object): #动态设置ip代理 def process_request(self, request, spider): request.meta["proxy"] = "http://111.198.219.151:8118" 使用西刺代理创建代理池保存到数据库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/5/24 16:27'import requestsfrom scrapy.selector import Selectorimport MySQLdbconn = MySQLdb.connect(host="127.0.0.1", user="root", passwd="mima", db="article_spider", charset="utf8")cursor = conn.cursor()def crawl_ips(): #爬取西刺的免费ip代理 headers = &#123;"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0"&#125; for i in range(1568): re = requests.get("http://www.xicidaili.com/nn/&#123;0&#125;".format(i), headers=headers) selector = Selector(text=re.text) all_trs = selector.css("#ip_list tr") ip_list = [] for tr in all_trs[1:]: speed_str = tr.css(".bar::attr(title)").extract()[0] if speed_str: speed = float(speed_str.split("秒")[0]) all_texts = tr.css("td::text").extract() ip = all_texts[0] port = all_texts[1] proxy_type = all_texts[5] ip_list.append((ip, port, proxy_type, speed)) for ip_info in ip_list: cursor.execute( "insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, 'HTTP')".format( ip_info[0], ip_info[1], ip_info[3] ) ) conn.commit()class GetIP(object): def delete_ip(self, ip): #从数据库中删除无效的ip delete_sql = """ delete from proxy_ip where ip='&#123;0&#125;' """.format(ip) cursor.execute(delete_sql) conn.commit() return True def judge_ip(self, ip, port): #判断ip是否可用 http_url = "http://www.baidu.com" proxy_url = "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) try: proxy_dict = &#123; "http":proxy_url, &#125; response = requests.get(http_url, proxies=proxy_dict) except Exception as e: print ("invalid ip and port") self.delete_ip(ip) return False else: code = response.status_code if code &gt;= 200 and code &lt; 300: print ("effective ip") return True else: print ("invalid ip and port") self.delete_ip(ip) return False def get_random_ip(self): #从数据库中随机获取一个可用的ip random_sql = """ SELECT ip, port FROM proxy_ip ORDER BY RAND() LIMIT 1 """ result = cursor.execute(random_sql) for ip_info in cursor.fetchall(): ip = ip_info[0] port = ip_info[1] judge_re = self.judge_ip(ip, port) if judge_re: return "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) else: return self.get_random_ip()# print (crawl_ips())if __name__ == "__main__": get_ip = GetIP() get_ip.get_random_ip() 使用scrapy_proxies创建ip代理池 pip install scrapy_proxies 收费，但是简单https://github.com/scrapy-plugins/scrapy-crawlera tor隐藏。vpnhttp://www.theonionrouter.com/ 7. 通过云打码实现验证码的识别http://www.yundama.com/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/6/24 16:48'import jsonimport requestsclass YDMHttp(object): apiurl = 'http://api.yundama.com/api.php' username = '' password = '' appid = '' appkey = '' def __init__(self, username, password, appid, appkey): self.username = username self.password = password self.appid = str(appid) self.appkey = appkey def balance(self): data = &#123;'method': 'balance', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("获取剩余积分", ret_data["balance"]) return ret_data["balance"] else: return None def login(self): data = &#123;'method': 'login', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("登录成功", ret_data["uid"]) return ret_data["uid"] else: return None def decode(self, filename, codetype, timeout): data = &#123;'method': 'upload', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey, 'codetype': str(codetype), 'timeout': str(timeout)&#125; files = &#123;'file': open(filename, 'rb')&#125; response_data = requests.post(self.apiurl, files=files, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("识别成功", ret_data["text"]) return ret_data["text"] else: return Nonedef ydm(file_path): username = '' # 密码 password = '' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '' # 图片文件 filename = 'image/1.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 yundama = YDMHttp(username, password, appid, appkey) if (username == 'username'): print('请设置好相关参数再测试') else: # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 return yundama.decode(file_path, codetype, timeout);if __name__ == "__main__": # 用户名 username = '' # 密码 password = '' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 if (username == 'username'): print ('请设置好相关参数再测试') else: # 初始化 yundama = YDMHttp(username, password, appid, appkey) # 登陆云打码 uid = yundama.login(); print('uid: %s' % uid) # 登陆云打码 uid = yundama.login(); print ('uid: %s' % uid) # 查询余额 balance = yundama.balance(); print ('balance: %s' % balance) # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 text = yundama.decode(filename, codetype, timeout); 8. cookie的禁用。&amp; 设置下载速度http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html setting.py: 12# Disable cookies (enabled by default)COOKIES_ENABLED = False 设置下载速度： 12# The initial download delay#AUTOTHROTTLE_START_DELAY = 5 给不同的spider设置自己的setting值 123custom_settings = &#123; "COOKIES_ENABLED": True&#125; 六、scrapy进阶开发1. Selenium动态页面抓取Selenium （浏览器自动化测试框架)Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本 安装pip install selenium 文档地址：http://selenium-python.readthedocs.io/api.html安装webdriver.exe 天猫价格获取123456789101112from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#天猫价格获取browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025")t_selector = Selector(text=browser.page_source)print (t_selector.css(".tm-price::text").extract())# print (browser.page_source)browser.quit() 知乎模拟登录1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#知乎模拟登陆browser.get("https://www.zhihu.com/#signin")browser.find_element_by_css_selector(".view-signin input[name='account']").send_keys("phone")browser.find_element_by_css_selector(".view-signin input[name='password']").send_keys("mima")browser.find_element_by_css_selector(".view-signin button.sign-button").click() 微博模拟登录微博开放平台api 1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#selenium 完成微博模拟登录browser.get("http://weibo.com/")import timetime.sleep(5)browser.find_element_by_css_selector("#loginname").send_keys("1147727180@qq.com")browser.find_element_by_css_selector(".info_list.password input[node-type='password'] ").send_keys("mima")browser.find_element_by_xpath('//*[@id="pl_login_form"]/div/div[3]/div[6]/a').click() 模拟JavaScript鼠标下滑1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#开源中国博客browser.get("https://www.oschina.net/blog")import timetime.sleep(5)for i in range(3): browser.execute_script("window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;") time.sleep(3) 页面不加载图片12345678910from selenium import webdriverfrom scrapy.selector import Selector# 设置chromedriver不加载图片chrome_opt = webdriver.ChromeOptions()prefs = &#123;"profile.managed_default_content_settings.images":2&#125;chrome_opt.add_experimental_option("prefs", prefs)browser = webdriver.Chrome(executable_path="C:/chromedriver.exe",chrome_options=chrome_opt)browser.get("https://www.oschina.net/blog") phantomjs无界面的浏览器获取天猫价格12345678#phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重browser = webdriver.PhantomJS(executable_path="C:/phantomjs-2.1.1-windows/bin/phantomjs.exe")browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025")t_selector = Selector(text=browser.page_source)print (t_selector.css(".tm-price::text").extract())print (browser.page_source)# browser.quit() 2.selenium集成进scrapy如何集成 创建中间件。1234567891011121314from selenium import webdriverfrom scrapy.http import HtmlResponseclass JSPageMiddleware(object): #通过chrome请求动态网页 def process_request(self, request, spider): if spider.name == "jobbole": browser = webdriver.Chrome(executable_path="C:/chromedriver.exe") spider.browser.get(request.url) import time time.sleep(3) print ("访问:&#123;0&#125;".format(request.url)) return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding="utf-8", request=request) 使用selenium集成到具体spider中 信号量：dispatcher.connect 信号的映射，当spider结束该做什么 12345678910111213from scrapy.xlib.pydispatch import dispatcherfrom scrapy import signals #使用selenium def __init__(self): self.browser = webdriver.Chrome(executable_path="D:/Temp/chromedriver.exe") super(JobboleSpider, self).__init__() dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): #当爬虫退出的时候关闭chrome print ("spider closed") self.browser.quit() python下无界面浏览器pip install pyvirtualdisplay linux使用： 123456from pyvirtualdisplay import Displaydisplay = Display(visible=0, size=(800, 600))display.start()browser = webdriver.Chrome()browser.get() 错误：cmd=[‘xvfb’,’help’]os error sudo apt-get install xvfb pip install xvfbwrapper scrapy-splash:支持分布式，稳定性不如chorme https://github.com/scrapy-plugins/scrapy-splash selenium grid支持分布式 splinterhttps://github.com/cobrateam/splinter scrapy的暂停重启scrapy crawl lagou -s JOBDIR=job_info/001 pycharm进程直接杀死 kiil -9 一次 ctrl+c可接受信号 Lib\site-packages\scrapy\dupefilters.py 先hash将url变成定长的字符串然后使用集合set去重 telnet远程登录 telnet localhost 6023 连接当前spiderest()命令查看spider当前状态 spider.settings[&quot;COOKIES_ENABLED&quot;] Lib\site-packages\scrapy\extensions\telnet.py 数据收集 &amp; 状态收集Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，值大多是计数值。 该机制叫做数据收集器(Stats Collector)，可以通过 Crawler API 的属性 stats 来使用。在下面的章节 常见数据收集器使用方法 将给出例子来说明。 无论数据收集(stats collection)开启或者关闭，数据收集器永远都是可用的。 因此您可以import进自己的模块并使用其API(增加值或者设置新的状态键(stat keys))。 该做法是为了简化数据收集的方法: 您不应该使用超过一行代码来收集您的spider，Scrpay扩展或任何您使用数据收集器代码里头的状态。 http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/stats.html 状态收集，数据收集器 12# 收集伯乐在线所有404的url以及404页面数handle_httpstatus_list = [404] 七、scrapy-redis 分布式爬虫1. 分布式爬虫设计及redis介绍多个爬虫如何进行调度，一个集中的状态管理器 优点： 利用多机器带宽 利用多ip加速爬取速度 两个问题： request队列的集中管理 去重集中管理 分布式。 2. redis命令hexists course_dict mtianyanhexists course_dict mtianyan2 Redis HEXISTS命令被用来检查哈希字段是否存在。返回值回复整数，1或0。 1, 如果哈希包含字段。 0 如果哈希不包含字段，或key不存在。 hdel course_dict mtianyan Redis HDEL命令用于从存储在键散列删除指定的字段。如果没有这个哈希中存在指定的字段将被忽略。如果键不存在，它将被视为一个空的哈希与此命令将返回0。返回值回复整数，从散列中删除的字段的数量，不包括指定的但不是现有字段。 hgetall course_dict Redis Hgetall 命令用于返回哈希表中，所有的字段和值。在返回值里，紧跟每个字段名(field name)之后是字段的值(value)，所以返回值的长度是哈希表大小的两倍。 hset course_dict bobby “python scrapy” Redis Hset 命令用于为哈希表中的字段赋值 。如果哈希表不存在，一个新的哈希表被创建并进行 HSET 操作。如果字段已经存在于哈希表中，旧值将被覆盖。 hkey course_dict Redis Keys 命令用于查找所有符合给定模式 pattern 的 key 。。 hvals course_dict Redis Hvals 命令返回哈希表所有字段的值。 lpush mtianyan “scary”rpush mtianyan “scary” 存入key-value lrange mtianyan 0 10 取出mtianyan的0到10 命令 说明 lpop/rpop 左删除/右删除 llen mtianyan 长度 lindex mtianyan 3 第几个元素 sadd 集合做减法 siner 交集 spop 随机删除 srandmember 随机选择多个元素 smembers 获取set所有元素 srandmember 随机选择多个元素 zadd 每个数有分数 zcount key 0 100 0-100分数据量统计 3. scrapy-redis搭建分布式爬虫需要的环境： Python 2.7, 3.4 or 3.5Redis &gt;= 2.8Scrapy &gt;= 1.1redis-py &gt;= 2.10 pip install redis setting.py设置 12345SCHEDULER = "scrapy_redis.scheduler.Scheduler"DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125; 要继承redisspider 12345678from scrapy_redis.spiders import RedisSpiderclass MySpider(RedisSpider): name = 'myspider' def parse(self, response): # do stuff pass 启动spider scrapy runspider myspider.py push urls to redis:放置初始url进入队列redis-cli lpush myspider:start_urls http://google.com 搭建示例 创建新的scrapy项目 去github拷贝scrapy-redis源码 不同spider使用不同redis list将队列从内存放入redis中next_requests 所有的yield出去的request会被ScrapyRedisTest\scrapy_redis\scheduler.py的以及重写的enqueue_request接收 八、elasticsearch搭建搜索引擎 elasticsearch介绍：一个基于lucene的搜索服务器，分布式多用户的全文搜索引擎 java开发的 基于restful web接口自己搭建的网站或者程序，添加搜索功能比较困难所以我们希望搜索解决方案要高效零配置并且免费能够简单的通过json和http与搜索引擎交互希望搜索服务很稳定简单的将一台服务器扩展到多台服务器 内部功能：分词 搜索结果打分 解析搜索要求全文搜索引擎：solr sphinx很多大公司都用elasticsearch 戴尔 Facebook 微软等等 elasticsearch对Lucene进行了封装，既能存储数据，又能分析数据，适合与做搜索引擎关系数据搜索缺点： 无法对搜素结果进行打分排序 没有分布式，搜索麻烦，对程序员的要求比较高 无法解析搜索请求，对搜索的内容无法进行解析，如分词等 数据多了，效率低 需要分词，把关系，数据，重点分出来 nosql数据库： 文档数据库 json代码，在关系数据库中数据存储，需要存到多个表，内部有多对多等关系之类的，需要涉及到多个表才能将json里面的内容存下来，nosql直接将一个json的内容存起来，作为一个文档存档到数据库。mongodb： 1. elasticsearch安装与配置 java sdk安装 elasticsearch安装官网下载 不使用官网的版本，提供原始的插件不多 elasticsearc-rtf github搜索，中文发行版，已经安装了很多插件 https://github.com/medcl/elasticsearch-rtf 运行elasticsearch的方法，在bin文件目录下进入命令行，执行elasticsearch.bat5.配置文件：elasticsearch-rtf\elasticsearch-rtf-master\config\elasticsearch.yml 2. elasticsearch两个重要插件：head和kibana的安装head插件相当于Navicat，用于管理数据库，基于浏览器 https://github.com/mobz/elasticsearch-head 1234567Running with built in servergit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm installnpm run startopen http://localhost:9100/ 配置elasticsearch与heade互通 kibana.bat 2. elasticsearch基础概念 集群：一个或多个节点组织在一起 节点：一个集群中的一台服务器 分片：索引划分为多份的能力，允许水平分割，扩展容量，多个分片响应请求 副本：分片的一份或多分，一个节点失败，其他节点顶上 |index | 数据库||type | 表||document | 行||fields | 列| 集合搜索和保存：增加了五种方法：OPTIONS &amp; PUT &amp; DELETE &amp; TRACE &amp; CONNECT 3. 倒排索引： 倒排索引待解决的问题： 4. elasticsearch命令 PUT lagou/job/11为id PUT lagou/job/不指明id自动生成uuid。 修改部分字段POST lagou/job/1/_update DELETE lagou/job/1 elasticserach批量操作: 查询index为testdb下的job1表的id为1和job2表的id为2的数据123456789101112131415GET _mget&#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; index已经指定了，所有在doc中就不用指定了 123456789101112GET testdb/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; 连type都一样，只是id不一样 1234567891011GET testdb/job1/_megt&#123; &quot;docs&quot;:[ &#123; &quot;_id&quot;:1 &#125;, &#123; &quot;_id&quot;:2 &#125; ]&#125; 或者继续简写 1234GET testdb/job1/_megt&#123; &quot;ids&quot;:[1,2]&#125; elasticsearch的bulk批量操作：可以合并多个操作，比如index，delete，update，create等等，包括从一个索引到另一个索引： action_and_meta_data\n option_source\n action_and_meta_data\n option_source\n …. action_and_meta_data\n option_source\n 每个操作都是由两行构成，除了delete除外，由元信息行和数据行组成注意数据不能美化，即只能是两行的形式，而不能是经过解析的标准的json排列形式，否则会报错 123POST _bulk&#123;&quot;index&quot;:...&#125;&#123;&quot;field&quot;:...&#125; elasticserach的mapping映射 elasticserach的mapping映射：创建索引时，可以预先定义字段的类型以及相关属性，每个字段定义一种类型，属性比mysql里面丰富，前面没有传入，因为elasticsearch会根据json源数据来猜测是什么基础类型。M挨批评就是我们自己定义的字段的数据类型，同时告诉elasticsearch如何索引数据以及是否可以被搜索。作用：会让索引建立的更加细致和完善，对于大多数是不需要我们自己定义 相关属性的配置 String类型： 两种text keyword。text会对内部的内容进行分析，索引，进行倒排索引等，为设置为keyword则会当成字符串，不会被分析，只能完全匹配才能找到String。 在es5已经被废弃了 日期类型：date 以及datetime等 数据类型:integer long double等等 bool类型 binary类型 复杂类型：object nested geo类型：geo-point地理位置 专业类型：ip competition object ：json里面内置的还有下层{}的对象 nested：数组形式的数据 elasticserach查询：大概分为三类： 基本查询: 组合查询： 过滤：查询同时，通过filter条件在不影响打分的情况下筛选数据 match查询: 后面为关键词，关于python的都会提取出来，match查询会对内容进行分词，并且会自动对传入的关键词进行大小写转换，内置ik分词器会进行切分，如python网站，只要搜到存在的任何一部分，都会返回GET lagou/job/_search 1234567&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;python&quot; &#125; &#125;&#125; term查询 区别，对传入的值不会做任何处理，就像keyword，只能查包含整个传入的内容的，一部分也不行，只能完全匹配 terms查询 title里传入多个值，只要有一个匹配，就会返回结果 控制查询的返回数量 12345678910GET lagou/_serach&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;python&quot; &#125; &#125;, &quot;form&quot;:1, &quot;size&quot;:2&#125; 通过这里就可以完成分页处理洛，从第一条开始查询两条 match_all 返回所有 1234567891011121314151617181920GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125;**match_phrase查询 短语查询**GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;title&quot;:&#123; &quot;query&quot;:&quot;python系统&quot;, &quot;slop&quot;:6 &#125; &#125; &#125;&#125; python系统，将其分词，分为词条，满足词条里面的所有词才会返回结果，slop参数说明两个词条之间的最小距离 multi_match查询 比如可以指定多个字段，比如查询title和desc这两个字段包含python的关键词文档 123456789GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;:&#123; &quot;query&quot;:&quot;python&quot;, &quot;fileds&quot;:[&quot;title^3&quot;,&quot;desc&quot;] &#125; &#125;&#125; query为要查询的关键词 fileds在哪些字段里查询关键词，只要其中某个字段中出现了都返回^3的意思为设置权重，在title中找到的权值为在desc字段中找到的权值的三倍 指定返回字段 12345678GET lagou/_search&#123; &quot;stored_fields&quot;:[&quot;title&quot;,&quot;company_name&quot;], &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;pyhton&quot; &#125; &#125;&#125; 通过sort把结果排序 1234567891011GET lagou/_search&#123; &quot;query&quot;;&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;sort&quot;:[&#123; &quot;comments&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125;]&#125; sort是一个数组，里面是一个字典，key就是要sort的字段，asc desc是升序降序的意思 查询范围 range查询 123456789101112GET lagou/_search&#123; &quot;query&quot;;&#123; &quot;range&quot;:&#123; &quot;comments&quot;:&#123; &quot;gte&quot;:10, &quot;lte&quot;:20, &quot;boost&quot;:2.0 &#125; &#125; &#125;&#125; range是在query里面的，boost是权重，gte lte是大于等于 小于等于的意思对时间的范围查询，则是以字符串的形式传入 wildcard模糊查询，可以使用通配符* 组合查询：bool查询 bool查询包括了must should must_not filter来完成格式如下： 123456bool:&#123; &quot;filter&quot;:[], &quot;must&quot;:[], &quot;should&quot;:[], &quot;must_not&quot;:[],&#125; 5. 把爬取的数据保存至elasticsearch12345678class ElasticsearchPipeline(object): #将数据写入到es中 def process_item(self, item, spider): #将item转换为es的数据 item.save_to_es() return item elasticsearch-dsl-py High level Python client for Elasticsearch pip install elasticsearch-dsl items.py 中将数据保存至es12345678910111213141516171819202122def save_to_es(self): article = ArticleType() article.title = self['title'] article.create_date = self["create_date"] article.content = remove_tags(self["content"]) article.front_image_url = self["front_image_url"] if "front_image_path" in self: article.front_image_path = self["front_image_path"] article.praise_nums = self["praise_nums"] article.fav_nums = self["fav_nums"] article.comment_nums = self["comment_nums"] article.url = self["url"] article.tags = self["tags"] article.meta.id = self["url_object_id"] article.suggest = gen_suggests(ArticleType._doc_type.index, ((article.title,10),(article.tags, 7))) article.save() redis_cli.incr("jobbole_count") return 6. elasticsearch结合django搭建搜索引擎获取elasticsearch的查询接口 123456789101112131415161718body=&#123; "query":&#123; "multi_match":&#123; "query":key_words, "fields":["tags", "title", "content"] &#125; &#125;, "from":(page-1)*10, "size":10, "highlight": &#123; "pre_tags": ['&lt;span class="keyWord"&gt;'], "post_tags": ['&lt;/span&gt;'], "fields": &#123; "title": &#123;&#125;, "content": &#123;&#125;, &#125; &#125; &#125; 使django与其交互。]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (八)elasticsearch结合django搭建搜索引擎]]></title>
    <url>%2Fpost%2F723b0e71.html</url>
    <content type="text"><![CDATA[八、elasticsearch搭建搜索引擎 elasticsearch介绍：一个基于lucene的搜索服务器，分布式多用户的全文搜索引擎java开发的 基于restful web接口。自己搭建的网站或者程序，添加搜索功能比较困难。所以我们希望搜索解决方案要高效零配置并且免费。elasticsearch能够简单的通过json和http与搜索引擎交互，支持分布式，可将一台服务器扩展到多台服务器 内部功能：分词 搜索结果打分 解析搜索要求全文搜索引擎：solr sphinx很多大公司都用elasticsearch 戴尔 Facebook 微软等等 elasticsearch对Lucene进行了封装，既能存储数据，又能分析数据，适合与做搜索引擎关系数据搜索缺点： 无法对搜素结果进行打分排序 没有分布式，搜索麻烦，对程序员的要求比较高 无法解析搜索请求，对搜索的内容无法进行解析，如分词等 数据多了，效率低 需要分词，把关系，数据，重点分出来 nosql数据库： 文档数据库 json代码，在关系数据库中数据存储，需要存到多个表，内部有多对多等关系之类的，需要涉及到多个表才能将json里面的内容存下来，nosql直接将一个json的内容存起来，作为一个文档存档到数据库。mongodb： 1. elasticsearch安装与配置 java sdk安装 elasticsearch安装官网下载 不使用官网的版本，提供原始的插件不多 elasticsearc-rtf github搜索，中文发行版，已经安装了很多插件 https://github.com/medcl/elasticsearch-rtf 运行elasticsearch的方法，在bin文件目录下进入命令行，执行elasticsearch.bat5.配置文件：elasticsearch-rtf\elasticsearch-rtf-master\config\elasticsearch.yml 2. elasticsearch两个重要插件：head和kibana的安装head插件相当于Navicat，用于管理数据库，基于浏览器 https://github.com/mobz/elasticsearch-head 1234567Running with built in servergit clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm installnpm run startopen http://localhost:9100/ 配置elasticsearch与heade互通 kibana.bat 2. elasticsearch基础概念 集群：一个或多个节点组织在一起 节点：一个集群中的一台服务器 分片：索引划分为多份的能力，允许水平分割，扩展容量，多个分片响应请求 副本：分片的一份或多分，一个节点失败，其他节点顶上 |index | 数据库||type | 表||document | 行||fields | 列| 集合搜索和保存：增加了五种方法：OPTIONS &amp; PUT &amp; DELETE &amp; TRACE &amp; CONNECT 3. 倒排索引： 倒排索引待解决的问题： 4. elasticsearch命令 PUT lagou/job/11为id PUT lagou/job/不指明id自动生成uuid。 修改部分字段POST lagou/job/1/_update DELETE lagou/job/1 elasticserach批量操作: 查询index为testdb下的job1表的id为1和job2表的id为2的数据123456789101112131415GET _mget&#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_index&quot;:&quot;testdb&quot;, &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; index已经指定了，所有在doc中就不用指定了 123456789101112GET testdb/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_type&quot;:&quot;job1&quot;, &quot;_id&quot;:1 &#125;, &#123; &quot;_type&quot;:&quot;job2&quot;, &quot;_id&quot;:2 &#125; ]&#125; 连type都一样，只是id不一样 1234567891011GET testdb/job1/_megt&#123; &quot;docs&quot;:[ &#123; &quot;_id&quot;:1 &#125;, &#123; &quot;_id&quot;:2 &#125; ]&#125; 或者继续简写 1234GET testdb/job1/_megt&#123; &quot;ids&quot;:[1,2]&#125; elasticsearch的bulk批量操作：可以合并多个操作，比如index，delete，update，create等等，包括从一个索引到另一个索引： action_and_meta_data\n option_source\n action_and_meta_data\n option_source\n …. action_and_meta_data\n option_source\n 每个操作都是由两行构成，除了delete除外，由元信息行和数据行组成注意数据不能美化，即只能是两行的形式，而不能是经过解析的标准的json排列形式，否则会报错 123POST _bulk&#123;&quot;index&quot;:...&#125;&#123;&quot;field&quot;:...&#125; elasticserach的mapping映射 elasticserach的mapping映射：创建索引时，可以预先定义字段的类型以及相关属性，每个字段定义一种类型，属性比mysql里面丰富，前面没有传入，因为elasticsearch会根据json源数据来猜测是什么基础类型。M挨批评就是我们自己定义的字段的数据类型，同时告诉elasticsearch如何索引数据以及是否可以被搜索。作用：会让索引建立的更加细致和完善，对于大多数是不需要我们自己定义 相关属性的配置 String类型： 两种text keyword。text会对内部的内容进行分析，索引，进行倒排索引等，为设置为keyword则会当成字符串，不会被分析，只能完全匹配才能找到String。 在es5已经被废弃了 日期类型：date 以及datetime等 数据类型:integer long double等等 bool类型 binary类型 复杂类型：object nested geo类型：geo-point地理位置 专业类型：ip competition object ：json里面内置的还有下层{}的对象 nested：数组形式的数据 elasticserach查询：大概分为三类： 基本查询: 组合查询： 过滤：查询同时，通过filter条件在不影响打分的情况下筛选数据 match查询: 后面为关键词，关于python的都会提取出来，match查询会对内容进行分词，并且会自动对传入的关键词进行大小写转换，内置ik分词器会进行切分，如python网站，只要搜到存在的任何一部分，都会返回GET lagou/job/_search 1234567&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;python&quot; &#125; &#125;&#125; term查询 区别，对传入的值不会做任何处理，就像keyword，只能查包含整个传入的内容的，一部分也不行，只能完全匹配 terms查询 title里传入多个值，只要有一个匹配，就会返回结果 控制查询的返回数量 12345678910GET lagou/_serach&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;python&quot; &#125; &#125;, &quot;form&quot;:1, &quot;size&quot;:2&#125; 通过这里就可以完成分页处理洛，从第一条开始查询两条 match_all 返回所有 1234567891011121314151617181920GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125;**match_phrase查询 短语查询**GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;title&quot;:&#123; &quot;query&quot;:&quot;python系统&quot;, &quot;slop&quot;:6 &#125; &#125; &#125;&#125; python系统，将其分词，分为词条，满足词条里面的所有词才会返回结果，slop参数说明两个词条之间的最小距离 multi_match查询 比如可以指定多个字段，比如查询title和desc这两个字段包含python的关键词文档 123456789GET lagou/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;:&#123; &quot;query&quot;:&quot;python&quot;, &quot;fileds&quot;:[&quot;title^3&quot;,&quot;desc&quot;] &#125; &#125;&#125; query为要查询的关键词 fileds在哪些字段里查询关键词，只要其中某个字段中出现了都返回^3的意思为设置权重，在title中找到的权值为在desc字段中找到的权值的三倍 指定返回字段 12345678GET lagou/_search&#123; &quot;stored_fields&quot;:[&quot;title&quot;,&quot;company_name&quot;], &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;title&quot;:&quot;pyhton&quot; &#125; &#125;&#125; 通过sort把结果排序 1234567891011GET lagou/_search&#123; &quot;query&quot;;&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;sort&quot;:[&#123; &quot;comments&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125;]&#125; sort是一个数组，里面是一个字典，key就是要sort的字段，asc desc是升序降序的意思 查询范围 range查询 123456789101112GET lagou/_search&#123; &quot;query&quot;;&#123; &quot;range&quot;:&#123; &quot;comments&quot;:&#123; &quot;gte&quot;:10, &quot;lte&quot;:20, &quot;boost&quot;:2.0 &#125; &#125; &#125;&#125; range是在query里面的，boost是权重，gte lte是大于等于 小于等于的意思对时间的范围查询，则是以字符串的形式传入 wildcard模糊查询，可以使用通配符* 组合查询：bool查询 bool查询包括了must should must_not filter来完成格式如下： 123456bool:&#123; &quot;filter&quot;:[], &quot;must&quot;:[], &quot;should&quot;:[], &quot;must_not&quot;:[],&#125; 5. 把爬取的数据保存至elasticsearch12345678class ElasticsearchPipeline(object): #将数据写入到es中 def process_item(self, item, spider): #将item转换为es的数据 item.save_to_es() return item elasticsearch-dsl-py High level Python client for Elasticsearch pip install elasticsearch-dsl items.py 中将数据保存至es12345678910111213141516171819202122def save_to_es(self): article = ArticleType() article.title = self['title'] article.create_date = self["create_date"] article.content = remove_tags(self["content"]) article.front_image_url = self["front_image_url"] if "front_image_path" in self: article.front_image_path = self["front_image_path"] article.praise_nums = self["praise_nums"] article.fav_nums = self["fav_nums"] article.comment_nums = self["comment_nums"] article.url = self["url"] article.tags = self["tags"] article.meta.id = self["url_object_id"] article.suggest = gen_suggests(ArticleType._doc_type.index, ((article.title,10),(article.tags, 7))) article.save() redis_cli.incr("jobbole_count") return 6. elasticsearch结合django搭建搜索引擎获取elasticsearch的查询接口 123456789101112131415161718body=&#123; "query":&#123; "multi_match":&#123; "query":key_words, "fields":["tags", "title", "content"] &#125; &#125;, "from":(page-1)*10, "size":10, "highlight": &#123; "pre_tags": ['&lt;span class="keyWord"&gt;'], "post_tags": ['&lt;/span&gt;'], "fields": &#123; "title": &#123;&#125;, "content": &#123;&#125;, &#125; &#125; &#125; 使django与其交互。]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (七)scrapy-redis分布式爬虫]]></title>
    <url>%2Fpost%2F2b297ffe.html</url>
    <content type="text"><![CDATA[七、scrapy-redis 分布式爬虫 使用resis + scrapy-redis 创建分布式爬虫 1. 分布式爬虫设计及redis介绍多个爬虫如何进行调度，一个集中的状态管理器 优点： 利用多机器带宽 利用多ip加速爬取速度 两个问题： request队列的集中管理 去重集中管理 分布式。 2. redis命令hexists course_dict mtianyanhexists course_dict mtianyan2 Redis HEXISTS命令被用来检查哈希字段是否存在。返回值回复整数，1或0。 1, 如果哈希包含字段。 0 如果哈希不包含字段，或key不存在。 hdel course_dict mtianyan Redis HDEL命令用于从存储在键散列删除指定的字段。如果没有这个哈希中存在指定的字段将被忽略。如果键不存在，它将被视为一个空的哈希与此命令将返回0。返回值回复整数，从散列中删除的字段的数量，不包括指定的但不是现有字段。 hgetall course_dict Redis Hgetall 命令用于返回哈希表中，所有的字段和值。在返回值里，紧跟每个字段名(field name)之后是字段的值(value)，所以返回值的长度是哈希表大小的两倍。 hset course_dict bobby “python scrapy” Redis Hset 命令用于为哈希表中的字段赋值 。如果哈希表不存在，一个新的哈希表被创建并进行 HSET 操作。如果字段已经存在于哈希表中，旧值将被覆盖。 hkey course_dict Redis Keys 命令用于查找所有符合给定模式 pattern 的 key 。。 hvals course_dict Redis Hvals 命令返回哈希表所有字段的值。 lpush mtianyan “scary”rpush mtianyan “scary” 存入key-value lrange mtianyan 0 10 取出mtianyan的0到10 命令 说明 lpop/rpop 左删除/右删除 llen mtianyan 长度 lindex mtianyan 3 第几个元素 sadd 集合做减法 siner 交集 spop 随机删除 srandmember 随机选择多个元素 smembers 获取set所有元素 srandmember 随机选择多个元素 zadd 每个数有分数 zcount key 0 100 0-100分数据量统计 3. scrapy-redis搭建分布式爬虫需要的环境： Python 2.7, 3.4 or 3.5Redis &gt;= 2.8Scrapy &gt;= 1.1redis-py &gt;= 2.10 pip install redis setting.py设置 12345SCHEDULER = "scrapy_redis.scheduler.Scheduler"DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 300&#125; 要继承redisspider 12345678from scrapy_redis.spiders import RedisSpiderclass MySpider(RedisSpider): name = 'myspider' def parse(self, response): # do stuff pass 启动spider scrapy runspider myspider.py push urls to redis:放置初始url进入队列redis-cli lpush myspider:start_urls http://google.com 搭建示例 创建新的scrapy项目 去github拷贝scrapy-redis源码 不同spider使用不同redis list将队列从内存放入redis中next_requests 所有的yield出去的request会被ScrapyRedisTest\scrapy_redis\scheduler.py的以及重写的enqueue_request接收]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (六)scrapy进阶开发]]></title>
    <url>%2Fpost%2Ffb760a12.html</url>
    <content type="text"><![CDATA[六、scrapy进阶开发 Selenium 获取天猫价格，模拟知乎登录，微博模拟登录，模拟JavaScript鼠标下滑，页面不加载图片，phantomjs无界面的浏览器获取天猫价格，selenium集成进scrapy，scrapy的暂停重启。 1. Selenium动态页面抓取Selenium （浏览器自动化测试框架)Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本 安装pip install selenium 文档地址：http://selenium-python.readthedocs.io/api.html安装webdriver.exe 天猫价格获取123456789101112from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#天猫价格获取browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025")t_selector = Selector(text=browser.page_source)print (t_selector.css(".tm-price::text").extract())# print (browser.page_source)browser.quit() 知乎模拟登录1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#知乎模拟登陆browser.get("https://www.zhihu.com/#signin")browser.find_element_by_css_selector(".view-signin input[name='account']").send_keys("18487255487")browser.find_element_by_css_selector(".view-signin input[name='password']").send_keys("ty158917")browser.find_element_by_css_selector(".view-signin button.sign-button").click() 微博模拟登录微博开放平台api 1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#selenium 完成微博模拟登录browser.get("http://weibo.com/")import timetime.sleep(5)browser.find_element_by_css_selector("#loginname").send_keys("1147727180@qq.com")browser.find_element_by_css_selector(".info_list.password input[node-type='password'] ").send_keys("tudoudou5283")browser.find_element_by_xpath('//*[@id="pl_login_form"]/div/div[3]/div[6]/a').click() 模拟JavaScript鼠标下滑1234567891011from selenium import webdriverfrom scrapy.selector import Selectorbrowser = webdriver.Chrome(executable_path="C:/chromedriver.exe")#开源中国博客browser.get("https://www.oschina.net/blog")import timetime.sleep(5)for i in range(3): browser.execute_script("window.scrollTo(0, document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;") time.sleep(3) 页面不加载图片12345678910from selenium import webdriverfrom scrapy.selector import Selector# 设置chromedriver不加载图片chrome_opt = webdriver.ChromeOptions()prefs = &#123;"profile.managed_default_content_settings.images":2&#125;chrome_opt.add_experimental_option("prefs", prefs)browser = webdriver.Chrome(executable_path="C:/chromedriver.exe",chrome_options=chrome_opt)browser.get("https://www.oschina.net/blog") phantomjs无界面的浏览器获取天猫价格12345678#phantomjs, 无界面的浏览器， 多进程情况下phantomjs性能会下降很严重browser = webdriver.PhantomJS(executable_path="C:/phantomjs-2.1.1-windows/bin/phantomjs.exe")browser.get("https://detail.tmall.com/item.htm?spm=a230r.1.14.3.yYBVG6&amp;id=538286972599&amp;cm_id=140105335569ed55e27b&amp;abbucket=15&amp;sku_properties=10004:709990523;5919063:6536025")t_selector = Selector(text=browser.page_source)print (t_selector.css(".tm-price::text").extract())print (browser.page_source)# browser.quit() 2.selenium集成进scrapy如何集成 创建中间件。1234567891011121314from selenium import webdriverfrom scrapy.http import HtmlResponseclass JSPageMiddleware(object): #通过chrome请求动态网页 def process_request(self, request, spider): if spider.name == "jobbole": browser = webdriver.Chrome(executable_path="C:/chromedriver.exe") spider.browser.get(request.url) import time time.sleep(3) print ("访问:&#123;0&#125;".format(request.url)) return HtmlResponse(url=spider.browser.current_url, body=spider.browser.page_source, encoding="utf-8", request=request) 使用selenium集成到具体spider中 信号量：dispatcher.connect 信号的映射，当spider结束该做什么 12345678910111213from scrapy.xlib.pydispatch import dispatcherfrom scrapy import signals #使用selenium def __init__(self): self.browser = webdriver.Chrome(executable_path="D:/Temp/chromedriver.exe") super(JobboleSpider, self).__init__() dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): #当爬虫退出的时候关闭chrome print ("spider closed") self.browser.quit() python下无界面浏览器pip install pyvirtualdisplay linux使用： 123456from pyvirtualdisplay import Displaydisplay = Display(visible=0, size=(800, 600))display.start()browser = webdriver.Chrome()browser.get() 错误：cmd=[‘xvfb’,’help’]os error sudo apt-get install xvfb pip install xvfbwrapper scrapy-splash:支持分布式，稳定性不如chorme https://github.com/scrapy-plugins/scrapy-splash selenium grid支持分布式 splinterhttps://github.com/cobrateam/splinter scrapy的暂停重启scrapy crawl lagou -s JOBDIR=job_info/001 pycharm进程直接杀死 kiil -9 一次 ctrl+c可接受信号 Lib\site-packages\scrapy\dupefilters.py 先hash将url变成定长的字符串然后使用集合set去重 telnet远程登录 telnet localhost 6023 连接当前spiderest()命令查看spider当前状态 spider.settings[&quot;COOKIES_ENABLED&quot;] Lib\site-packages\scrapy\extensions\telnet.py 数据收集 &amp; 状态收集Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，值大多是计数值。 该机制叫做数据收集器(Stats Collector)，可以通过 Crawler API 的属性 stats 来使用。在下面的章节 常见数据收集器使用方法 将给出例子来说明。 无论数据收集(stats collection)开启或者关闭，数据收集器永远都是可用的。 因此您可以import进自己的模块并使用其API(增加值或者设置新的状态键(stat keys))。 该做法是为了简化数据收集的方法: 您不应该使用超过一行代码来收集您的spider，Scrpay扩展或任何您使用数据收集器代码里头的状态。 http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/stats.html 状态收集，数据收集器 12# 收集伯乐在线所有404的url以及404页面数handle_httpstatus_list = [404]]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (五)爬虫与反爬虫的战争]]></title>
    <url>%2Fpost%2F791a397f.html</url>
    <content type="text"><![CDATA[五、爬虫与反爬虫 介绍反爬虫的基本知识，随机更换useagent，fake UseAgent代理池，西刺代理创建ip代理池，云打码实现验证码的识别等。 1. 基础知识如何使我们的爬虫不被禁止掉 爬虫： 自动获取数据的程序，关键是批量的获取 反爬虫： 使用技术手段防止爬虫程序的方法 误伤： 反爬虫技术将普通用户识别为爬虫，效果再好也不能用 学校，网吧，出口的公网ip只有一个，所以禁止ip不能用。 ip动态分配。a爬封b 成本： 反爬虫人力和机器成本 拦截： 拦截率越高，误伤率越高 反爬虫的目的： 爬虫与反爬虫的对抗过程： 使用检查可以查看到价格，而查看网页源代码无法查看到价格字段。scrapy下载到的网页时网页源代码。js（ajax）填充的动态数据无法通过网页获取到。 2. scrapy架构及源码介绍 我们编写的spider，然后yield一个request发送给engine engine拿到什么都不做然后给scheduler engine会生成一个request给engine engine拿到之后通过downloadermiddleware 给downloader downloader再发送response回来给engine。 engine拿到之后，response给spider。 spider进行处理，解析出item &amp; request， item-&gt;给itempipeline；如果是request，跳转步骤二 path：articlespider3\Lib\site-packages\scrapy\core engine.py： scheduler.py downloader item pipeline spider engine.py：重要函数schedule enqueue_request：把request放scheduler _next_request_from_scheduler:从调度器拿。 123456def schedule(self, request, spider): self.signals.send_catch_log(signal=signals.request_scheduled, request=request, spider=spider) if not self.slot.scheduler.enqueue_request(request): self.signals.send_catch_log(signal=signals.request_dropped, request=request, spider=spider) articlespider3\Lib\site-packages\scrapy\core\downloader\handlers 支持文件，ftp，http下载(https). 后期定制middleware： spidermiddlewire downloadmiddlewire django和scrapy结构类似 3. scrapy的两个重要类：request和response类似于django httprequest 1yield Request(url=parse.urljoin(response.url, post_url)) request参数： 12345class Request(object_ref): def __init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None): cookies：Lib\site-packages\scrapy\downloadermiddlewares\cookies.py 1cookiejarkey = request.meta.get("cookiejar") priority: 优先级，影响调度顺序 dont_filter：我的同样的request不会被过滤 errback：错误时的回调函数 https://doc.scrapy.org/en/1.2/topics/request-response.html?highlight=response errback example： 1234567891011121314151617181920212223242526272829303132333435363738394041class ErrbackSpider(scrapy.Spider): name = "errback_example" start_urls = [ "http://www.httpbin.org/", # HTTP 200 expected "http://www.httpbin.org/status/404", # Not found error "http://www.httpbin.org/status/500", # server issue "http://www.httpbin.org:12345/", # non-responding host, timeout expected "http://www.httphttpbinbin.org/", # DNS error expected ] def start_requests(self): for u in self.start_urls: yield scrapy.Request(u, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True) def parse_httpbin(self, response): self.logger.info('Got successful response from &#123;&#125;'.format(response.url)) # do something useful here... def errback_httpbin(self, failure): # log all failures self.logger.error(repr(failure)) # in case you want to do something special for some errors, # you may need the failure's type: if failure.check(HttpError): # these exceptions come from HttpError spider middleware # you can get the non-200 response response = failure.value.response self.logger.error('HttpError on %s', response.url) elif failure.check(DNSLookupError): # this is the original request request = failure.request self.logger.error('DNSLookupError on %s', request.url) elif failure.check(TimeoutError, TCPTimedOutError): request = failure.request self.logger.error('TimeoutError on %s', request.url) response类 12def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None): self.headers = Headers(headers or &#123;&#125;) response的参数：request：yield出来的request，会放在response，让我们知道它是从哪里来的 4. 自行编写随机更换useagent setting中设置 1234user_agent_list = [ 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0', 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36',] 然后在代码中使用。 123456from settings import user_agent_list import random random_index =random.randint(0,len(user_agent_list)) random_agent = user_agent_list[random_index] 'User-Agent': random_agent 12345import randomrandom_index = random.randint(0, len(user_agent_list))random_agent = user_agent_list[random_index]self.headers["User-Agent"] = random_agentyield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) 但是问题：每个request之前都得这样做。 5. middlewire配置及编写fake UseAgent代理池取消DOWNLOADER_MIDDLEWARES的注释状态 123DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.MyCustomDownloaderMiddleware': 543,&#125; articlespider3\Lib\site-packages\scrapy\downloadermiddlewares\useragent.py 123456789101112131415161718class UserAgentMiddleware(object): """This middleware allows spiders to override the user_agent""" def __init__(self, user_agent='Scrapy'): self.user_agent = user_agent @classmethod def from_crawler(cls, crawler): o = cls(crawler.settings['USER_AGENT']) crawler.signals.connect(o.spider_opened, signal=signals.spider_opened) return o def spider_opened(self, spider): self.user_agent = getattr(spider, 'user_agent', self.user_agent) def process_request(self, request, spider): if self.user_agent: request.headers.setdefault(b'User-Agent', self.user_agent) 重要方法process_request 配置默认useagent为none 1234DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.MyCustomDownloaderMiddleware': 543, 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None&#125; 使用fakeuseragentpip install fake-useragent setting.py设置随机模式RANDOM_UA_TYPE = &quot;random&quot; 123456789101112131415161718from fake_useragent import UserAgentclass RandomUserAgentMiddlware(object): #随机更换user-agent def __init__(self, crawler): super(RandomUserAgentMiddlware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random") @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 6. 使用西刺代理创建ip代理池保存到数据库*ip动态变化：重启路由器等 ip代理的原理： 不直接发送自己真实ip，而使用中间代理商（代理服务器），那么服务器不知道我们的ip也就不会把我们禁掉setting.py设置 1234class RandomProxyMiddleware(object): #动态设置ip代理 def process_request(self, request, spider): request.meta["proxy"] = "http://111.198.219.151:8118" 使用西刺代理创建代理池保存到数据库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/5/24 16:27'import requestsfrom scrapy.selector import Selectorimport MySQLdbconn = MySQLdb.connect(host="127.0.0.1", user="root", passwd="ty158917", db="article_spider", charset="utf8")cursor = conn.cursor()def crawl_ips(): #爬取西刺的免费ip代理 headers = &#123;"User-Agent":"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0"&#125; for i in range(1568): re = requests.get("http://www.xicidaili.com/nn/&#123;0&#125;".format(i), headers=headers) selector = Selector(text=re.text) all_trs = selector.css("#ip_list tr") ip_list = [] for tr in all_trs[1:]: speed_str = tr.css(".bar::attr(title)").extract()[0] if speed_str: speed = float(speed_str.split("秒")[0]) all_texts = tr.css("td::text").extract() ip = all_texts[0] port = all_texts[1] proxy_type = all_texts[5] ip_list.append((ip, port, proxy_type, speed)) for ip_info in ip_list: cursor.execute( "insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, 'HTTP')".format( ip_info[0], ip_info[1], ip_info[3] ) ) conn.commit()class GetIP(object): def delete_ip(self, ip): #从数据库中删除无效的ip delete_sql = """ delete from proxy_ip where ip='&#123;0&#125;' """.format(ip) cursor.execute(delete_sql) conn.commit() return True def judge_ip(self, ip, port): #判断ip是否可用 http_url = "http://www.baidu.com" proxy_url = "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) try: proxy_dict = &#123; "http":proxy_url, &#125; response = requests.get(http_url, proxies=proxy_dict) except Exception as e: print ("invalid ip and port") self.delete_ip(ip) return False else: code = response.status_code if code &gt;= 200 and code &lt; 300: print ("effective ip") return True else: print ("invalid ip and port") self.delete_ip(ip) return False def get_random_ip(self): #从数据库中随机获取一个可用的ip random_sql = """ SELECT ip, port FROM proxy_ip ORDER BY RAND() LIMIT 1 """ result = cursor.execute(random_sql) for ip_info in cursor.fetchall(): ip = ip_info[0] port = ip_info[1] judge_re = self.judge_ip(ip, port) if judge_re: return "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) else: return self.get_random_ip()# print (crawl_ips())if __name__ == "__main__": get_ip = GetIP() get_ip.get_random_ip() 使用scrapy_proxies创建ip代理池 pip install scrapy_proxies 收费，但是简单https://github.com/scrapy-plugins/scrapy-crawlera tor隐藏。vpnhttp://www.theonionrouter.com/ 7. 通过云打码实现验证码的识别http://www.yundama.com/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/6/24 16:48'import jsonimport requestsclass YDMHttp(object): apiurl = 'http://api.yundama.com/api.php' username = '' password = '' appid = '' appkey = '' def __init__(self, username, password, appid, appkey): self.username = username self.password = password self.appid = str(appid) self.appkey = appkey def balance(self): data = &#123;'method': 'balance', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("获取剩余积分", ret_data["balance"]) return ret_data["balance"] else: return None def login(self): data = &#123;'method': 'login', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey&#125; response_data = requests.post(self.apiurl, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("登录成功", ret_data["uid"]) return ret_data["uid"] else: return None def decode(self, filename, codetype, timeout): data = &#123;'method': 'upload', 'username': self.username, 'password': self.password, 'appid': self.appid, 'appkey': self.appkey, 'codetype': str(codetype), 'timeout': str(timeout)&#125; files = &#123;'file': open(filename, 'rb')&#125; response_data = requests.post(self.apiurl, files=files, data=data) ret_data = json.loads(response_data.text) if ret_data["ret"] == 0: print ("识别成功", ret_data["text"]) return ret_data["text"] else: return Nonedef ydm(file_path): username = '' # 密码 password = '' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '' # 图片文件 filename = 'image/1.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 yundama = YDMHttp(username, password, appid, appkey) if (username == 'username'): print('请设置好相关参数再测试') else: # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 return yundama.decode(file_path, codetype, timeout);if __name__ == "__main__": # 用户名 username = '' # 密码 password = '' # 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！ appid = # 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！ appkey = '' # 图片文件 filename = 'image/captcha.jpg' # 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 5000 # 超时时间，秒 timeout = 60 # 检查 if (username == 'username'): print ('请设置好相关参数再测试') else: # 初始化 yundama = YDMHttp(username, password, appid, appkey) # 登陆云打码 uid = yundama.login(); print('uid: %s' % uid) # 登陆云打码 uid = yundama.login(); print ('uid: %s' % uid) # 查询余额 balance = yundama.balance(); print ('balance: %s' % balance) # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果 text = yundama.decode(filename, codetype, timeout); 8. cookie的禁用。&amp; 设置下载速度http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/autothrottle.html setting.py: 12# Disable cookies (enabled by default)COOKIES_ENABLED = False 设置下载速度： 12# The initial download delay#AUTOTHROTTLE_START_DELAY = 5 给不同的spider设置自己的setting值 123custom_settings = &#123; "COOKIES_ENABLED": True&#125;]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (四)通过CrawlSpider对拉勾网进行整站爬取]]></title>
    <url>%2Fpost%2Fd083b798.html</url>
    <content type="text"><![CDATA[四、通过CrawlSpider对招聘网站拉钩网进行整站爬取 使用CrawlSpider对于拉勾网进行整站爬取。附带源码解读，数据库建表与爬取后将数据存入数据库等内容。 推荐工具cmderhttp://cmder.net/下载full版本，使我们在windows环境下也可以使用linux部分命令。配置path环境变量 1. 设计拉勾网的数据表结构 2. 初始化拉钩网项目并解读crawl源码scrapy genspider --list查看可使用的初始化模板ailable templates: basic crawl csvfeed xmlfeed 1scrapy genspider -t crawl lagou www.lagou.com cmd与pycharm不同，mark rootsetting.py 设置目录crawl模板 123456789101112131415class LagouSpider(CrawlSpider): name = 'lagou' allowed_domains = ['www.lagou.com'] start_urls = ['http://www.lagou.com/'] rules = ( Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), ) def parse_item(self, response): i = &#123;&#125; #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract() #i['name'] = response.xpath('//div[@id="name"]').extract() #i['description'] = response.xpath('//div[@id="description"]').extract() return i 源码阅读剖析https://doc.scrapy.org/en/1.3/topics/spiders.html#crawlspider 提供了一些可以让我们进行简单的follow的规则，link，迭代爬取 rules： 规则，crawel spider读取并执行 parse_start_url(response)： example： rules是一个可迭代对象，里面有Rule实例-&gt;LinkExtractor的分析allow=(&#39;category\.php&#39;, ), callback=&#39;parse_item&#39;,allow允许的url模式。callback，要回调的函数名。因为rules里面没有self，无法获取到方法。 12345678910111213141516171819202122232425import scrapyfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass MySpider(CrawlSpider): name = 'example.com' allowed_domains = ['example.com'] start_urls = ['http://www.example.com'] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'), ) def parse_item(self, response): self.logger.info('Hi, this is an item page! %s', response.url) item = scrapy.Item() item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)') item['name'] = response.xpath('//td[@id="item_name"]/text()').extract() item['description'] = response.xpath('//td[@id="item_description"]/text()').extract() return item 分析拉勾网模板代码 将http加上s 重命名parse_item为我们自定义的parse_job 点击class LagouSpider(CrawlSpider):的CrawlSpider，进入crawl源码 class CrawlSpider(Spider):可以看出它继承于spider 入口：def start_requests(self): alt+左右方向键，不同代码跳转 5-&gt;之后默认parse CrawlSpider里面有parse函数。但是这次我们不能向以前一样覆盖 Crawl.py核心函数parse。 parse函数调用_parse_response 12def parse(self, response): return self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=True) _parse_response 判断是否有callback即有没有self.parse_start_url 我们可以重载parse_start_url加入自己的处理 把参数传递给函数，并调用process_results函数 _parse_response函数 12345678910def _parse_response(self, response, callback, cb_kwargs, follow=True): if callback: cb_res = callback(response, **cb_kwargs) or () cb_res = self.process_results(response, cb_res) for requests_or_item in iterate_spider_output(cb_res): yield requests_or_item if follow and self._follow_links: for request_or_item in self._requests_to_follow(response): yield request_or_item parse_start_url的return值将会被process_results方法接收处理如果不重写，因为返回为空，然后就相当于什么都没做 12def process_results(self, response, results): return results 点击followlink 123def set_crawler(self, crawler): super(CrawlSpider, self).set_crawler(crawler) self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True) 如果setting中有这个参数，则可以进一步执行到parse _requests_to_follow 判断传入的是不是response，如果不是直接returns 针对当前response设置一个空set，去重 把self的rules通过enumerate变成一个可迭代对象 跳转rules详情 拿到link通过link_extractor.extract_links抽取出具体的link 执行我们的process_links link制作完成发起Request,回调_response_downloaded函数 然后执行parse_respose 1234567891011121314def _requests_to_follow(self, response): if not isinstance(response, HtmlResponse): return seen = set() for n, rule in enumerate(self._rules): links = [lnk for lnk in rule.link_extractor.extract_links(response) if lnk not in seen] if links and rule.process_links: links = rule.process_links(links) for link in links: seen.add(link) r = Request(url=link.url, callback=self._response_downloaded) r.meta.update(rule=n, link_text=link.text) yield rule.process_request(r) _compile_rules 在我们初始化时会调用_compile_rules copy.copy(r) for r in self.rules]将我们的rules进行一个copy 调用回调函数get_method。 调用rules里面我们定义的process_links 调用rules里面我们定义的process_request 123456789101112def _compile_rules(self): def get_method(method): if callable(method): return method elif isinstance(method, six.string_types): return getattr(self, method, None) self._rules = [copy.copy(r) for r in self.rules] for rule in self._rules: rule.callback = get_method(rule.callback) rule.process_links = get_method(rule.process_links) rule.process_request = get_method(rule.process_request) self.process_links = process_links self.process_request = process_request 可以通过在rules里面传入我们自己的处理函数，实现对url的自定义。达到负载均衡，多地不同ip访问。 _response_downloaded通过rule取到具体的rule调用我们自己的回调函数 123def _response_downloaded(self, response): rule = self._rules[response.meta['rule']] return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow) allow ：符合这个url我就爬取 deny : 符合这个url规则我就放弃 allow_domin : 这个域名下的我才处理 allow_domin : 这个域名下的我不处理 restrict_xpaths：进一步限定xpath 123self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None, deny_extensions=None, restrict_css=() extract_links如果有restrict_xpaths，他会进行读取执行 12345678910111213def extract_links(self, response): base_url = get_base_url(response) if self.restrict_xpaths: docs = [subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)] else: docs = [response.selector] all_links = [] for doc in docs: links = self._extract_links(doc, response.url, response.encoding, base_url) all_links.extend(self._process_links(links)) return unique_list(all_links) get_base_url: urllib.parse.urljoin替我们拼接好url 1234567891011121314151617def get_base_url(text, baseurl='', encoding='utf-8'): """Return the base url if declared in the given HTML `text`, relative to the given base url. If no base url is found, the given `baseurl` is returned. """ text = to_unicode(text, encoding) m = _baseurl_re.search(text) if m: return moves.urllib.parse.urljoin( safe_url_string(baseurl), safe_url_string(m.group(1), encoding=encoding) ) else: return safe_url_string(baseurl) 编写rule规则12345rules = ( Rule(LinkExtractor(allow=("zhaopin/.*",)), follow=True), Rule(LinkExtractor(allow=("gongsi/j\d+.html",)), follow=True), Rule(LinkExtractor(allow=r'jobs/\d+.html'), callback='parse_job', follow=True),) 3. 设计lagou的items需要用到的方法 123456789from w3lib.html import remove_tagsdef remove_splash(value): #去掉工作城市的斜线 return value.replace("/","")def handle_jobaddr(value): addr_list = value.split("\n") addr_list = [item.strip() for item in addr_list if item.strip()!="查看地图"] return "".join(addr_list) 定义好的item12345678910111213141516171819202122232425262728class LagouJobItem(scrapy.Item): #拉勾网职位信息 title = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() salary = scrapy.Field() job_city = scrapy.Field( input_processor=MapCompose(remove_splash), ) work_years = scrapy.Field( input_processor = MapCompose(remove_splash), ) degree_need = scrapy.Field( input_processor = MapCompose(remove_splash), ) job_type = scrapy.Field() publish_time = scrapy.Field() job_advantage = scrapy.Field() job_desc = scrapy.Field() job_addr = scrapy.Field( input_processor=MapCompose(remove_tags, handle_jobaddr), ) company_name = scrapy.Field() company_url = scrapy.Field() tags = scrapy.Field( input_processor = Join(",") ) crawl_time = scrapy.Field() 重写的itemloader设置默认只提取第一个 123class LagouJobItemLoader(ItemLoader): #自定义itemloader default_output_processor = TakeFirst() 4. 提取字段值并存入数据库12345678910111213141516171819202122232425def parse_job(self, response): #解析拉勾网的职位 item_loader = LagouJobItemLoader(item=LagouJobItem(), response=response) item_loader.add_css("title", ".job-name::attr(title)") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("salary", ".job_request .salary::text") item_loader.add_xpath("job_city", "//*[@class='job_request']/p/span[2]/text()") item_loader.add_xpath("work_years", "//*[@class='job_request']/p/span[3]/text()") item_loader.add_xpath("degree_need", "//*[@class='job_request']/p/span[4]/text()") item_loader.add_xpath("job_type", "//*[@class='job_request']/p/span[5]/text()") item_loader.add_css("tags", '.position-label li::text') item_loader.add_css("publish_time", ".publish_time::text") item_loader.add_css("job_advantage", ".job-advantage p::text") item_loader.add_css("job_desc", ".job_bt div") item_loader.add_css("job_addr", ".work_addr") item_loader.add_css("company_name", "#job_company dt a img::attr(alt)") item_loader.add_css("company_url", "#job_company dt a::attr(href)") item_loader.add_value("crawl_time", datetime.now()) job_item = item_loader.load_item() return job_item 获得的拉勾网item数据 5. items中添加get_insert_sql实现存入数据库12345678910111213141516def get_insert_sql(self): insert_sql = """ insert into lagou_job(title, url, url_object_id, salary, job_city, work_years, degree_need, job_type, publish_time, job_advantage, job_desc, job_addr, company_name, company_url, tags, crawl_time) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE salary=VALUES(salary), job_desc=VALUES(job_desc) """ params = ( self["title"], self["url"], self["url_object_id"], self["salary"], self["job_city"], self["work_years"], self["degree_need"], self["job_type"], self["publish_time"], self["job_advantage"], self["job_desc"], self["job_addr"], self["company_name"], self["company_url"], self["job_addr"], self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (三)知乎网问题和答案爬取]]></title>
    <url>%2Fpost%2Fb9bf70b2.html</url>
    <content type="text"><![CDATA[知乎网问题和答案爬取 对于知乎进行模拟登录以及验证码的处理,对于两种不同新旧样式进行区分。数据库建表将爬取的问题与答案存入数据库 1. 基础知识session和cookie机制 cookie：浏览器支持的存储方式key-value http无状态请求，两次请求没有联系 session的工作原理 （1）当一个session第一次被启用时，一个唯一的标识被存储于本地的cookie中。 （2）首先使用session_start()函数，从session仓库中加载已经存储的session变量。 （3）通过使用session_register()函数注册session变量。 （4）脚本执行结束时，未被销毁的session变量会被自动保存在本地一定路径下的session库中. request模拟知乎的登录http状态码 获取crsftoken 12345678910def get_xsrf(): #获取xsrf code response = requests.get("https://www.zhihu.com",headers =header) # # print(response.text) # text ='&lt;input type="hidden" name="_xsrf" value="ca70366e5de5d133c3ae09fb16d9b0fa"/&gt;' match_obj = re.match('.*name="_xsrf" value="(.*?)"', response.text) if match_obj: return (match_obj.group(1)) else: return "" python模拟知乎登录代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111# _*_ coding: utf-8 _*_import requeststry: import cookielibexcept: import http.cookiejar as cookielibimport re__author__ = 'mtianyan'__date__ = '2017/5/23 16:42'import requeststry: import cookielibexcept: import http.cookiejar as cookielibimport resession = requests.session()session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")try: session.cookies.load(ignore_discard=True)except: print ("cookie未能加载")agent = "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36"header = &#123; "HOST":"www.zhihu.com", "Referer": "https://www.zhizhu.com", 'User-Agent': agent&#125;def is_login(): #通过个人中心页面返回状态码来判断是否为登录状态 inbox_url = "https://www.zhihu.com/question/56250357/answer/148534773" response = session.get(inbox_url, headers=header, allow_redirects=False) if response.status_code != 200: return False else: return Truedef get_xsrf(): #获取xsrf code response = session.get("https://www.zhihu.com", headers=header) response_text = response.text #reDOTAll 匹配全文 match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) return xsrfdef get_index(): response = session.get("https://www.zhihu.com", headers=header) with open("index_page.html", "wb") as f: f.write(response.text.encode("utf-8")) print ("ok")def get_captcha(): import time t = str(int(time.time()*1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) t = session.get(captcha_url, headers=header) with open("captcha.jpg","wb") as f: f.write(t.content) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") return captchadef zhihu_login(account, password): #知乎登录 if re.match("^1\d&#123;10&#125;",account): print ("手机号码登录") post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": get_xsrf(), "phone_num": account, "password": password, "captcha":get_captcha() &#125; else: if "@" in account: #判断用户名是否为邮箱 print("邮箱方式登录") post_url = "https://www.zhihu.com/login/email" post_data = &#123; "_xsrf": get_xsrf(), "email": account, "password": password &#125; response_text = session.post(post_url, data=post_data, headers=header) session.cookies.save()# get_index()# is_login()# get_captcha()zhihu_login("phone", "mima") 2. scrapy创建知乎爬虫登录1scrapy genspider zhihu www.zhihu.com 因为知乎我们需要先进行登录，所以我们重写它的start_requests 12def start_requests(self): return [scrapy.Request('https://www.zhihu.com/#signin', headers=self.headers, callback=self.login)] 下载首页然后回调login函数。 login函数请求验证码并回调login_after_captcha函数.此处通过meta将post_data传送出去，后面的回调函数来用。 1234567891011121314151617181920212223def login(self, response): response_text = response.text #获取xsrf。 match_obj = re.match('.*name="_xsrf" value="(.*?)"', response_text, re.DOTALL) xsrf = '' if match_obj: xsrf = (match_obj.group(1)) if xsrf: post_url = "https://www.zhihu.com/login/phone_num" post_data = &#123; "_xsrf": xsrf, "phone_num": "phone", "password": "mima", "captcha": "" &#125; import time t = str(int(time.time() * 1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) #请求验证码并回调login_after_captcha. yield scrapy.Request(captcha_url, headers=self.headers, meta=&#123;"post_data":post_data&#125;, callback=self.login_after_captcha) login_after_captcha函数将验证码图片保存到本地，然后使用PIL库打开图片，肉眼识别后在控制台输入验证码值然后接受步骤一的meta数据，一并提交至登录接口。回调check_login检查是否登录成功。 123456789101112131415161718192021222324def login_after_captcha(self, response): with open("captcha.jpg", "wb") as f: f.write(response.body) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") post_data = response.meta.get("post_data", &#123;&#125;) post_url = "https://www.zhihu.com/login/phone_num" post_data["captcha"] = captcha return [scrapy.FormRequest( url=post_url, formdata=post_data, headers=self.headers, callback=self.check_login )] check_login函数，验证服务器的返回数据判断是否成功scrapy会对request的URL去重(RFPDupeFilter)，加上dont_filter则告诉它这个URL不参与去重. 源码中的startrequest: 123def start_requests(self): for url in self.start_urls: yield self.make_requests_from_url(url) 我们将原本的start_request的代码放在了现在重写的，回调链最后的check_login 123456def check_login(self, response): #验证服务器的返回数据判断是否成功 text_json = json.loads(response.text) if "msg" in text_json and text_json["msg"] == "登录成功": for url in self.start_urls: yield scrapy.Request(url, dont_filter=True, headers=self.headers) ###3. 知乎数据表设计 上图为知乎答案版本1 上图为知乎答案版本2 设置数据表字段 问题字段 回答字段 zhihu_id zhihu_id topics url url question_id title author_id content content answer_num parise_num comments_num comments_num watch_user_num create_time click_num update_time crawl_time crawl_time 知乎url分析 点具体问题下查看更多。可获得接口： https://www.zhihu.com/api/v4/questions/25914034/answers?include=data%5B%2A%5D.is_normal%2Cis_collapsed%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Cmark_infos%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cupvoted_followees%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%3F%28type%3Dbest_answerer%29%5D.topics&amp;limit=20&amp;offset=43&amp;sort_by=default 重点参数：offset=43isend = truenext href=”/question/25460323” 1all_urls = [parse.urljoin(response.url, url) for url in all_urls] 从首页获取所有a标签。如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数parse_question如果不是question页面则直接进一步跟踪。 123456789101112131415161718def parse(self, response): """ 提取出html页面中的所有url 并跟踪这些url进行一步爬取 如果提取的url中格式为 /question/xxx 就下载之后直接进入解析函数 """ all_urls = response.css("a::attr(href)").extract() all_urls = [parse.urljoin(response.url, url) for url in all_urls] #使用lambda函数对于每一个url进行过滤，如果是true放回列表，返回false去除。 all_urls = filter(lambda x:True if x.startswith("https") else False, all_urls) for url in all_urls: match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", url) if match_obj: # 如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) else: # 如果不是question页面则直接进一步跟踪 yield scrapy.Request(url, headers=self.headers, callback=self.parse) 进入parse_question函数处理创建我们的item item要用到的方法ArticleSpider\utils\common.py： 123456789def extract_num(text): #从字符串中提取出数字 match_re = re.match(".*?(\d+).*", text) if match_re: nums = int(match_re.group(1)) else: nums = 0 return nums setting.py中设置SQL_DATETIME_FORMAT = &quot;%Y-%m-%d %H:%M:%S&quot; SQL_DATE_FORMAT = &quot;%Y-%m-%d&quot;使用： 1from ArticleSpider.settings import SQL_DATETIME_FORMAT 知乎的问题 item 1234567891011121314151617181920212223242526272829303132333435363738394041424344class ZhihuQuestionItem(scrapy.Item): #知乎的问题 item zhihu_id = scrapy.Field() topics = scrapy.Field() url = scrapy.Field() title = scrapy.Field() content = scrapy.Field() answer_num = scrapy.Field() comments_num = scrapy.Field() watch_user_num = scrapy.Field() click_num = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) """ zhihu_id = self["zhihu_id"][0] topics = ",".join(self["topics"]) url = self["url"][0] title = "".join(self["title"]) content = "".join(self["content"]) answer_num = extract_num("".join(self["answer_num"])) comments_num = extract_num("".join(self["comments_num"])) if len(self["watch_user_num"]) == 2: watch_user_num = int(self["watch_user_num"][0]) click_num = int(self["watch_user_num"][1]) else: watch_user_num = int(self["watch_user_num"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, params 知乎问题回答item 123456789101112131415161718192021222324252627282930313233class ZhihuAnswerItem(scrapy.Item): #知乎的问题回答item zhihu_id = scrapy.Field() url = scrapy.Field() question_id = scrapy.Field() author_id = scrapy.Field() content = scrapy.Field() parise_num = scrapy.Field() comments_num = scrapy.Field() create_time = scrapy.Field() update_time = scrapy.Field() crawl_time = scrapy.Field() def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num), update_time=VALUES(update_time) """ create_time = datetime.datetime.fromtimestamp(self["create_time"]).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self["update_time"]).strftime(SQL_DATETIME_FORMAT) params = ( self["zhihu_id"], self["url"], self["question_id"], self["author_id"], self["content"], self["parise_num"], self["comments_num"], create_time, update_time, self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params 有了两个item之后，我们继续完善我们的逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041def parse_question(self, response): #处理question页面， 从页面中提取出具体的question item if "QuestionHeader-title" in response.text: #处理新版本 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) item_loader.add_css("title", "h1.QuestionHeader-title::text") item_loader.add_css("content", ".QuestionHeader-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", ".List-headerText span::text") item_loader.add_css("comments_num", ".QuestionHeader-actions button::text") item_loader.add_css("watch_user_num", ".NumberBoard-value::text") item_loader.add_css("topics", ".QuestionHeader-topics .Popover div::text") question_item = item_loader.load_item() else: #处理老版本页面的item提取 match_obj = re.match("(.*zhihu.com/question/(\d+))(/|$).*", response.url) if match_obj: question_id = int(match_obj.group(2)) item_loader = ItemLoader(item=ZhihuQuestionItem(), response=response) # item_loader.add_css("title", ".zh-question-title h2 a::text") item_loader.add_xpath("title", "//*[@id='zh-question-title']/h2/a/text()|//*[@id='zh-question-title']/h2/span/text()") item_loader.add_css("content", "#zh-question-detail") item_loader.add_value("url", response.url) item_loader.add_value("zhihu_id", question_id) item_loader.add_css("answer_num", "#zh-question-answer-num::text") item_loader.add_css("comments_num", "#zh-question-meta-wrap a[name='addcomment']::text") # item_loader.add_css("watch_user_num", "#zh-question-side-header-wrap::text") item_loader.add_xpath("watch_user_num", "//*[@id='zh-question-side-header-wrap']/text()|//*[@class='zh-question-followers-sidebar']/div/a/strong/text()") item_loader.add_css("topics", ".zm-tag-editor-labels a::text") question_item = item_loader.load_item() yield scrapy.Request(self.start_answer_url.format(question_id, 20, 0), headers=self.headers, callback=self.parse_answer) yield question_item 处理问题回答提取出需要的字段 123456789101112131415161718192021222324def parse_answer(self, reponse): #处理question的answer ans_json = json.loads(reponse.text) is_end = ans_json["paging"]["is_end"] next_url = ans_json["paging"]["next"] #提取answer的具体字段 for answer in ans_json["data"]: answer_item = ZhihuAnswerItem() answer_item["zhihu_id"] = answer["id"] answer_item["url"] = answer["url"] answer_item["question_id"] = answer["question"]["id"] answer_item["author_id"] = answer["author"]["id"] if "id" in answer["author"] else None answer_item["content"] = answer["content"] if "content" in answer else None answer_item["parise_num"] = answer["voteup_count"] answer_item["comments_num"] = answer["comment_count"] answer_item["create_time"] = answer["created_time"] answer_item["update_time"] = answer["updated_time"] answer_item["crawl_time"] = datetime.datetime.now() yield answer_item if not is_end: yield scrapy.Request(next_url, headers=self.headers, callback=self.parse_answer) 知乎提取字段流程图： 深度优先： 提取出页面所有的url，并过滤掉不需要的url 如果是questionurl就进入question的解析 把该问题的爬取完了然后就返回初始解析 将item写入数据库pipelines.py错误处理插入时错误可通过该方法监控123def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) 改造pipeline使其变得更通用原本具体硬编码的pipeline 1234567def do_insert(self, cursor, item): #执行具体的插入 insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) 改写后的： 1234def do_insert(self, cursor, item): #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params) 可选方法一： 1234567if item.__class__.__name__ == "JobBoleArticleItem": #执行具体的插入 insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) 推荐方法：把sql语句等放到item里面：jobboleitem类内部方法 12345678def get_insert_sql(self): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(fav_nums) """ params = (self["title"], self["url"], self["create_date"], self["fav_nums"]) return insert_sql, params 知乎问题： 12345678910111213141516171819202122232425262728293031def get_insert_sql(self): #插入知乎question表的sql语句 insert_sql = """ insert into zhihu_question(zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) """ zhihu_id = self["zhihu_id"][0] topics = ",".join(self["topics"]) url = self["url"][0] title = "".join(self["title"]) content = "".join(self["content"]) answer_num = extract_num("".join(self["answer_num"])) comments_num = extract_num("".join(self["comments_num"])) if len(self["watch_user_num"]) == 2: watch_user_num = int(self["watch_user_num"][0]) click_num = int(self["watch_user_num"][1]) else: watch_user_num = int(self["watch_user_num"][0]) click_num = 0 crawl_time = datetime.datetime.now().strftime(SQL_DATETIME_FORMAT) params = (zhihu_id, topics, url, title, content, answer_num, comments_num, watch_user_num, click_num, crawl_time) return insert_sql, params 知乎回答： 1234567891011121314151617181920def get_insert_sql(self): #插入知乎回答表的sql语句 insert_sql = """ insert into zhihu_answer(zhihu_id, url, question_id, author_id, content, parise_num, comments_num, create_time, update_time, crawl_time ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE content=VALUES(content), comments_num=VALUES(comments_num), parise_num=VALUES(parise_num), update_time=VALUES(update_time) """ create_time = datetime.datetime.fromtimestamp(self["create_time"]).strftime(SQL_DATETIME_FORMAT) update_time = datetime.datetime.fromtimestamp(self["update_time"]).strftime(SQL_DATETIME_FORMAT) params = ( self["zhihu_id"], self["url"], self["question_id"], self["author_id"], self["content"], self["parise_num"], self["comments_num"], create_time, update_time, self["crawl_time"].strftime(SQL_DATETIME_FORMAT), ) return insert_sql, params 第二次爬取到相同数据，更新数据 12ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num), comments_num=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num) 调试技巧 123456789101112if match_obj: #如果提取到question相关的页面则下载后交由提取函数进行提取 request_url = match_obj.group(1) yield scrapy.Request(request_url, headers=self.headers, callback=self.parse_question) #方便调试 breakelse: #方便调试 pass #如果不是question页面则直接进一步跟踪 #方便调试 # yield scrapy.Request(url, headers=self.headers, callback=self.parse) 12#方便调试 # yield question_item 错误排查[key error] titlepipeline中debug定位到哪一个item的错误。]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎- (二)伯乐在线爬取所有文章]]></title>
    <url>%2Fpost%2F1cc4531e.html</url>
    <content type="text"><![CDATA[伯乐在线爬取所有文章 使用scrapy对于伯乐在线的文章内容评论数，收藏数等进行爬取。包含从环境配置，软件安装，项目初始化，xpath，图片下载，数据保存到本地文件以及mysql（同步异步）等一系列内容。 二、伯乐在线爬取所有文章1. 初始化文件目录基础环境 python 3.5.1 JetBrains PyCharm 2016.3.2 mysql+navicat 为了便于日后的部署：我们开发使用了虚拟环境。 1234567891011pip install virtualenvpip install virtualenvwrapper-win安装虚拟环境管理mkvirtualenv articlespider3创建虚拟环境workon articlespider3直接进入虚拟环境deactivate退出激活状态workon知道有哪些虚拟环境 scrapy项目初始化介绍 自行官网下载py35对应得whl文件进行pip离线安装Scrapy 1.3.3 命令行创建scrapy项目123cd desktopscrapy startproject ArticleSpider scrapy目录结构 scrapy借鉴了django的项目思想 scrapy.cfg：配置文件。 setings.py：设置 12SPIDER_MODULES = [&apos;ArticleSpider.spiders&apos;] #存放spider的路径NEWSPIDER_MODULE = &apos;ArticleSpider.spiders&apos; pipelines.py: 做跟数据存储相关的东西 middilewares.py: 自己定义的middlewares 定义方法，处理响应的IO操作 init.py: 项目的初始化文件。 items.py： 定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据 创建我们的spider12cd ArticleSpiderscrapy genspider jobbole blog.jobbole.com 可以看到直接为我们创建好的空项目里已经有了模板代码。如下： 123456789101112# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] # start_urls是一个带爬的列表， #spider会为我们把请求下载网页做到，直接到parse阶段 start_urls = ['http://blog.jobbole.com/'] def parse(self, response): pass scray在命令行启动某一个Spyder的命令:1scrapy crawl jobbole 在windows报出错误 ImportError: No module named &#39;win32api&#39; 1pip install pypiwin32#解决 创建我们的调试工具类* 在项目根目录里创建main.py作为调试工具文件1234567891011121314151617# _*_ coding: utf-8 _*___author__ = 'mtianyan'__date__ = '2017/3/28 12:06'from scrapy.cmdline import executeimport sysimport os#将系统当前目录设置为项目根目录#os.path.abspath(__file__)为当前文件所在绝对路径#os.path.dirname为文件所在目录#H:\CodePath\spider\ArticleSpider\main.py#H:\CodePath\spider\ArticleSpidersys.path.append(os.path.dirname(os.path.abspath(__file__)))#执行命令，相当于在控制台cmd输入改名了execute(["scrapy", "crawl" , "jobbole"]) settings.py的设置不遵守reboots协议 ROBOTSTXT_OBEY = False 在jobble.py打上断点:12def parse(self, response): pass 可以看到他返回的htmlresponse对象:对象内部： body:网页内容 _DEFAULT_ENCODING= ‘ascii’ encoding= ‘utf-8’ 可以看出scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换. 2. 提取伯乐在线内容xpath的使用xpath让你可以不懂前端html，不看html的详细结构，只需要会右键查看就能获取网页上任何内容。速度远超beautifulsoup。目录: 1. xpath简介 2. xpath术语与语法 3. xpath抓取误区：javasrcipt生成html与html源文件的区别 4. xpath抓取实例 为什么要使用xpath？ xpath使用路径表达式在xml和html中进行导航 xpath包含有一个标准函数库 xpath是一个w3c的标准 xpath速度要远远超beautifulsoup。 xpath节点关系 父节点*上一层节点* 子节点 兄弟节点*同胞节点* 先辈节点*父节点，爷爷节点* 后代节点*儿子，孙子*xpath语法: 表达式 说明 article 选取所有article元素的所有子节点 /article 选取根元素article article/a 选取所有属于article的子元素的a元素 //div 选取所有div元素（不管出现在文档里的任何地方） article//div 选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置 //@class 选取所有名为class的属性 xpath语法-谓语: 表达式 说明 /article/div[1 选取属于article子元素的第一个div元素 /article/div[last()] 选取属于article子元素的最后一个div元素 /article/div[last()-1] 选取属于article子元素的倒数第二个div元素 //div[@color] 选取所有拥有color属性的div元素 //div[@color=’red’] 选取所有color属性值为red的div元素 xpath语法: 表达式 说明 /div/* 选取属于div元素的所有子节点 //* 选取所有元素 //div[@*] 选取所有带属性的div 元素 //div/a 丨//div/p 选取所有div元素的a和p元素 //span丨//ul 选取文档中的span和ul元素 article/div/p丨//span 选取所有属于article元素的div元素的p元素以及文档中所有的 span元素 xpath抓取误区 firebugs插件 取某一个网页上元素的xpath地址 如:http://blog.jobbole.com/110287/ 在标题处右键使用firebugs查看元素。然后在&lt;h1&gt;2016 腾讯软件开发面试题（部分）&lt;/h1&gt;右键查看xpath 123456789101112# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): re_selector = response.xpath("/html/body/div[3]/div[3]/div[1]/div[1]/h1") # print(re_selector) pass 调试debug可以看到1re_selector =(selectorlist)[] 可以看到返回的是一个空列表，列表是为了如果我们当前的xpath路径下还有层级目录时可以进行选取空说明没取到值： 我们可以来chorme里观察一下 chorme取到的值//*[@id=&quot;post-110287&quot;]/div[1]/h1 chormexpath代码 12345678910111213# -*- coding: utf-8 -*-import scrapyclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): re_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1') # print(re_selector) pass 可以看出此时可以取到值 分析页面，可以发现页面内有一部html是通过JavaScript ajax交互来生成的，因此在f12检查元素时的页面结构里有，而xpath不对xpath是基于html源代码文件结构来找的 xpath可以有多种多样的写法： 123re_selector = response.xpath("/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()")re2_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1/text()')re3_selector = response.xpath('//div[@class="entry-header“]/h1/text()') 推荐使用id型。因为页面id唯一。 推荐使用class型，因为后期循环爬取可扩展通用性强。 通过了解了这些此时我们已经可以抓取到页面的标题，此时可以使用xpath利器照猫画虎抓取任何内容。只需要点击右键查看xpath。 开启控制台调试 scrapy shell http://blog.jobbole.com/110287/ 完整的xpath提取伯乐在线字段代码 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import scrapyimport reclass JobboleSpider(scrapy.Spider): name = "jobbole" allowed_domains = ["blog.jobbole.com"] start_urls = ['http://blog.jobbole.com/110287/'] def parse(self, response): #提取文章的具体字段 title = response.xpath('//div[@class="entry-header"]/h1/text()').extract_first("") create_date = response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace("·","").strip() praise_nums = response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0] fav_nums = response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = match_re.group(1) comment_nums = response.xpath("//a[@href='#article-comment']/span/text()").extract()[0] match_re = re.match(".*?(\d+).*", comment_nums) if match_re: comment_nums = match_re.group(1) content = response.xpath("//div[@class='entry']").extract()[0] tag_list = response.xpath("//p[@class='entry-meta-hide-on-mobile']/a/text()").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) pass css选择器的使用：12345678910111213141516171819202122232425# 通过css选择器提取字段 # front_image_url = response.meta.get("front_image_url", "") #文章封面图 title = response.css(".entry-header h1::text").extract_first() create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip() praise_nums = response.css(".vote-post-up h10::text").extract()[0] fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0] match_re = re.match(".*?(\d+).*", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css("div.entry").extract()[0] tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) pass 3. 爬取所有文章yield关键字 12#使用request下载详情页面，下载完成后回调方法parse_detail()提取文章内容中的字段yield Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) scrapy.http import Request下载网页 12from scrapy.http import RequestRequest(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) parse拼接网址应对herf内有可能网址不全 1234from urllib import parseurl=parse.urljoin(response.url,post_url)parse.urljoin("http://blog.jobbole.com/all-posts/","http://blog.jobbole.com/111535/")#结果为http://blog.jobbole.com/111535/ class层级关系 12next_url = response.css(".next.page-numbers::attr(href)").extract_first("")#如果.next .pagenumber 是指两个class为层级关系。而不加空格为同一个标签 twist异步机制 Scrapy使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。在任何情况下，都不要写阻塞的代码。阻塞的代码包括: 访问文件、数据库或者Web 产生新的进程并需要处理新进程的输出，如运行shell命令 执行系统层次操作的代码，如等待系统队列 实现全部文章字段下载的代码： 1234567891011121314151617181920def parse(self, response): """ 1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析 2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse """ # 解析列表页中的所有文章url并交给scrapy下载后并进行解析 post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() for post_url in post_urls: #request下载完成之后，回调parse_detail进行文章详情页的解析 # Request(url=post_url,callback=self.parse_detail) print(response.url) print(post_url) yield Request(url=parse.urljoin(response.url,post_url),callback=self.parse_detail) #遇到href没有域名的解决方案 #response.url + post_url print(post_url) # 提取下一页并交给scrapy进行下载 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse) 全部文章的逻辑流程图 4. scrapy的items整合字段数据爬取的任务就是从非结构的数据中提取出结构性的数据。items 可以让我们自定义自己的字段（类似于字典，但比字典的功能更齐全） 在当前页，需要提取多个url 原始写法,extract之后则生成list列表，无法进行二次筛选： 1post_urls = response.css("#archive .floated-thumb .post-thumb a::attr(href)").extract() 改进写法: 12345post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: #获取封面图的url image_url = post_node.css("img::attr(src)").extract_first("") post_url = post_node.css("::attr(href)").extract_first("") 在下载网页的时候把获取到的封面图的url传给parse_detail的response在下载网页时将这个封面url获取到，并通过meta将他发送出去。在callback的回调函数中接收该值 123yield Request(url=parse.urljoin(response.url,post_url),meta=&#123;"front_image_url":image_url&#125;,callback=self.parse_detail)front_image_url = response.meta.get("front_image_url", "") urljoin的好处如果你没有域名，我就从response里取出来，如果你有域名则我对你起不了作用了 编写我们自定义的item并在jobboled.py中填充。 123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() content = scrapy.Field() tags = scrapy.Field() import之后实例化，实例化之后填充： 12345678910111. from ArticleSpider.items import JobBoleArticleItem2. article_item = JobBoleArticleItem()3. article_item["title"] = title article_item["url"] = response.url article_item["create_date"] = create_date article_item["front_image_url"] = [front_image_url] article_item["praise_nums"] = praise_nums article_item["comment_nums"] = comment_nums article_item["fav_nums"] = fav_nums article_item["tags"] = tags article_item["content"] = content yield article_item将这个item传送到pipelines中pipelines可以接收到传送过来的item将setting.py中的pipeline配置取消注释12345# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; &apos;ArticleSpider.pipelines.ArticlespiderPipeline&apos;: 300,&#125; 当我们的item被传输到pipeline我们可以将其进行存储到数据库等工作 setting设置下载图片pipeline123ITEM_PIPELINES=&#123;&apos;scrapy.pipelines.images.ImagesPipeline&apos;: 1,&#125; H:\CodePath\pyEnvs\articlespider3\Lib\site-packages\scrapy\pipelines里面有三个scrapy默认提供的pipeline提供了文件，图片，媒体。 ITEM_PIPELINES是一个数据管道的登记表，每一项具体的数字代表它的优先级，数字越小，越早进入。 setting设置下载图片的地址12# IMAGES_MIN_HEIGHT = 100# IMAGES_MIN_WIDTH = 100 设置下载图片的最小高度，宽度。 新建文件夹images在 123IMAGES_URLS_FIELD = "front_image_url"project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images') 安装PILpip install pillow 定制自己的pipeline使其下载图片后能保存下它的本地路径get_media_requests()接收一个迭代器对象下载图片item_completed获取到图片的下载地址 继承并重写item_completed() 12345678910from scrapy.pipelines.images import ImagesPipelineclass ArticleImagePipeline(ImagesPipeline): #重写该方法可从result中获取到图片的实际下载地址 def item_completed(self, results, item, info): for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item setting中设置使用我们自定义的pipeline，而不是系统自带的 12345ITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.ArticlespiderPipeline': 300, # 'scrapy.pipelines.images.ImagesPipeline': 1, 'ArticleSpider.pipelines.ArticleImagePipeline':1,&#125; 图片url的md5处理新建package utils 123456789import hashlibdef get_md5(url): m = hashlib.md5() m.update(url) return m.hexdigest()if __name__ == "__main__": print(get_md5("http://jobbole.com".encode("utf-8"))) 不确定用户传入的是不是: 1234567def get_md5(url): #str就是unicode了 if isinstance(url, str): url = url.encode("utf-8") m = hashlib.md5() m.update(url) return m.hexdigest() 在jobbole.py中将url的md5保存下来 12from ArticleSpider.utils.common import get_md5article_item["url_object_id"] = get_md5(response.url) 5. 数据保存到本地文件以及mysql中保存到本地json文件import codecs打开文件避免一些编码问题，自定义JsonWithEncodingPipeline实现json本地保存 123456789101112class JsonWithEncodingPipeline(object): #自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): #将item转换为dict，然后生成json对象，false避免中文出错 lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item #当spider关闭的时候 def spider_closed(self, spider): self.file.close() setting.py注册pipeline 12345ITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.JsonWithEncodingPipeline': 2, # 'scrapy.pipelines.images.ImagesPipeline': 1, 'ArticleSpider.pipelines.ArticleImagePipeline':1,&#125; scrapy exporters JsonItemExporter导出 scrapy自带的导出： - &apos;CsvItemExporter&apos;, - &apos;XmlItemExporter&apos;, - &apos;JsonItemExporter&apos; from scrapy.exporters import JsonItemExporter 1234567891011121314class JsonExporterPipleline(object): #调用scrapy提供的json export导出json文件 def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 设置setting.py注册该pipeline 1'ArticleSpider.pipelines.JsonExporterPipleline ': 2 保存到数据库(mysql)数据库设计数据表，表的内容字段是和item一致的。数据库与item的关系。类似于django中model与form的关系。日期的转换，将字符串转换为datetime 12345import datetime try: create_date = datetime.datetime.strptime(create_date, "%Y/%m/%d").date() except Exception as e: create_date = datetime.datetime.now().date() 数据库表设计 三个num字段均设置不能为空，然后默认0. content设置为longtext 主键设置为url_object_id 数据库驱动安装pip install mysqlclient Linux报错解决方案:ubuntu:sudo apt-get install libmysqlclient-devcentos:sudo yum install python-devel mysql-devel 保存到数据库pipeline(同步）编写 1234567891011121314import MySQLdbclass MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', 'mima', 'article_spider', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) self.conn.commit() 保存到数据库的(异步Twisted)编写因为我们的爬取速度可能大于数据库存储的速度。异步操作。设置可配置参数seeting.py设置1234MYSQL_HOST = &quot;127.0.0.1&quot;MYSQL_DBNAME = &quot;article_spider&quot;MYSQL_USER = &quot;root&quot;MYSQL_PASSWORD = &quot;123456&quot; 代码中获取到设置的可配置参数twisted异步： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import MySQLdb.cursorsfrom twisted.enterprise import adbapi#连接池ConnectionPool# def __init__(self, dbapiName, *connargs, **connkw):class MysqlTwistedPipline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USER"], passwd = settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) #**dbparms--&gt;("MySQLdb",host=settings['MYSQL_HOST'] dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): #处理异步插入的异常 print (failure) def do_insert(self, cursor, item): #执行具体的插入 #根据不同的item 构建不同的sql语句并插入到mysql中 insert_sql, params = item.get_insert_sql() cursor.execute(insert_sql, params)``` 可选django.itemshttps://github.com/scrapy-plugins/scrapy-djangoitem可以让我们保存的item直接变成django的models.#### scrapy的itemloader来维护提取代码itemloadr提供了一个容器，让我们配置某一个字段该使用哪种规则。add_css add_value add_xpath```pythonfrom scrapy.loader import ItemLoader# 通过item loader加载item front_image_url = response.meta.get("front_image_url", "") # 文章封面图 item_loader = ItemLoader(item=JobBoleArticleItem(), response=response) item_loader.add_css("title", ".entry-header h1::text") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("create_date", "p.entry-meta-hide-on-mobile::text") item_loader.add_value("front_image_url", [front_image_url]) item_loader.add_css("praise_nums", ".vote-post-up h10::text") item_loader.add_css("comment_nums", "a[href='#article-comment'] span::text") item_loader.add_css("fav_nums", ".bookmark-btn::text") item_loader.add_css("tags", "p.entry-meta-hide-on-mobile a::text") item_loader.add_css("content", "div.entry") #调用这个方法来对规则进行解析生成item对象 article_item = item_loader.load_item() 所有值变成了list 对于这些值做一些处理函数item.py中对于item process处理函数MapCompose可以传入函数对于该字段进行处理，而且可以传入多个 1234567from scrapy.loader.processors import MapComposedef add_mtianyan(value): return value+"-mtianyan" title = scrapy.Field( input_processor=MapCompose(lambda x:x+"mtianyan",add_mtianyan), ) 注意：此处的自定义方法一定要写在代码前面。 1234create_date = scrapy.Field( input_processor=MapCompose(date_convert), output_processor=TakeFirst()) 只取list中的第一个值。 自定义itemloader实现默认提取第一个 123class ArticleItemLoader(ItemLoader): #自定义itemloader实现默认提取第一个 default_output_processor = TakeFirst() list保存原值 123456def return_value(value): return valuefront_image_url = scrapy.Field( output_processor=MapCompose(return_value) ) 下载图片pipeline增加if增强通用性 123456789class ArticleImagePipeline(ImagesPipeline): #重写该方法可从result中获取到图片的实际下载地址 def item_completed(self, results, item, info): if "front_image_url" in item: for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item 自定义的item带处理函数的完整代码 1234567891011121314151617181920212223242526class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field( input_processor=MapCompose(date_convert), ) url = scrapy.Field() url_object_id = scrapy.Field() front_image_url = scrapy.Field( output_processor=MapCompose(return_value) ) front_image_path = scrapy.Field() praise_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) comment_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) fav_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) #因为tag本身是list，所以要重写 tags = scrapy.Field( input_processor=MapCompose(remove_comment_tags), output_processor=Join(",") ) content = scrapy.Field()]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy分布式爬虫打造搜索引擎 - (一)基础知识]]></title>
    <url>%2Fpost%2F9e947300.html</url>
    <content type="text"><![CDATA[一、基础知识学习 深度优先广度优先策略的选择。爬虫url去重策略，python字符编码问题。 一、基础知识学习:爬取策略的深度优先和广度优先目录： 网站的树结构 深度优先算法和实现 广度优先算法和实现 网站url树结构分层设计: bogbole.com blog.bogbole.com python.bogbole.com python.bogbole.com/123 环路链接问题： 从首页到下面节点。但是下面的链接节点又会有链接指向首页 所以：我们需要对于链接进行去重 1. 深度优先2. 广度优先 跳过已爬取的链接对于二叉树的遍历问题 深度优先(递归实现)： 顺着一条路，走到最深处。然后回头 广度优先(队列实现): 分层遍历：遍历完儿子辈。然后遍历孙子辈 Python实现深度优先过程code：1234567def depth_tree(tree_node): if tree_node is not None: print (tree_node._data) if tree_node._left is not None: return depth_tree(tree_node.left) if tree_node._right is not None: return depth_tree(tree_node,_right) Python实现广度优先过程code：1234567891011121314def level_queue(root): #利用队列实现树的广度优先遍历 if root is None: return my_queue = [] node = root my_queue.append(node) while my_queue: node = my_queue.pop(0) print (node.elem) if node.lchild is not None: my_queue.append(node.lchild) if node.rchild is not None: my_queue.append(node.rchild) 爬虫网址去重策略 将访问过的url保存到数据库中 将url保存到set中。只需要O(1)的代价就可以查询到url 100000000*2byte*50个字符/1024/1024/1024 = 9G url经过md5等方法哈希后保存到set中，将url压缩到固定长度而且不重复 用bitmap方法，将访问过的url通过hash函数映射到某一位 bloomfilter方法对bitmap进行改进，多重hash函数降低冲突 scrapy去重使用的是第三种方法：后面分布式scrapy-redis会讲解bloomfilter方法。 Python字符串编码问题解决： 计算机只能处理数字，文本转换为数字才能处理，计算机中8个bit作为一个字节，所以一个字节能表示的最大数字就是255 计算机是美国人发明的，所以一个字节就可以标识所有单个字符，所以ASCII(一个字节)编码就成为美国人的标准编码 但是ASCII处理中文明显不够，中文不止255个汉字，所以中国制定了GB2312编码，用两个字节表示一个汉字。GB2312将ASCII也包含进去了。同理，日文，韩文，越来越多的国家为了解决这个问题就都发展了一套编码，标准越来越多，如果出现多种语言混合显示就一定会出现乱码 于是unicode出现了，它将所有语言包含进去了。 看一下ASCII和unicode编码: 字母A用ASCII编码十进制是65，二进制 0100 0001 汉字”中” 已近超出ASCII编码的范围，用unicode编码是20013二进制是01001110 00101101 A用unicode编码只需要前面补0二进制是 00000000 0100 0001 乱码问题解决的，但是如果内容全是英文，unicode编码比ASCII编码需要多一倍的存储空间，传输也会变慢。 所以此时出现了可变长的编码”utf-8” ,把英文：1字节，汉字3字节，特别生僻的变成4-6字节，如果传输大量的英文，utf8作用就很明显。 读取文件，进行操作时转换为unicode编码进行处理保存文件时，转换为utf-8编码。以便于传输读文件的库会将转换为unicode python2 默认编码格式为ASCII，Python3 默认编码为 utf-81234#python3import syssys.getdefaultencoding()s.encoding('utf-8') 12345678#python2import syssys.getdefaultencoding()s = "我和你"su = u"我和你"~~s.encode("utf-8")#会报错~~s.decode("gb2312").encode("utf-8")su.encode("utf-8")]]></content>
      <categories>
        <category>Scrapy分布式爬虫打造搜索引擎</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>搜索引擎</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordpress常见问题]]></title>
    <url>%2Fpost%2Ff300d993.html</url>
    <content type="text"><![CDATA[在使用wordpress过程中遇到的一些问题及解决方案。 包括ftp传输服务器错误，Warning: scandir() has been disabled for security reasons in，后台ftp麻烦的手续关闭。 后台ftp传输连接服务器失败。 解决方案：按照lnmp教程中安装pureftp设置好自己的ftp用户名 Warning: scandir() has been disabled for security reasons in该方案同样可以解决wordpress后台一直重复提示更新翻译 解决方案：编辑网站目录/usr/local/php/etc/php.ini目录下php.inidisable_functions =scandir,passthru,exec,system,chroot,chgrp,chown,shell_exec,proc_open,proc_get_status,ini_alter,ini_alter,ini_restore,dl,pfsockopen,openlog,syslog,readlink,symlink,popepassthru,stream_socket_server,fsocket,fsockopen去掉其中scandir保存后通过putty下执行/etc/init.d/php-fpm restart重启服务,解决。 后台ftp麻烦的手续关闭。 解决方案：putty下执行chown -R www /home/wwwroot/www.mtianyan.cn(后面地址视个人wordpress安装目录而定)]]></content>
      <categories>
        <category>运维环境配置</category>
      </categories>
      <tags>
        <tag>网站搭建</tag>
        <tag>WordPress</tag>
        <tag>报错处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速搭建网站教程]]></title>
    <url>%2Fpost%2F3c03daae.html</url>
    <content type="text"><![CDATA[从零开始快速搭建自己的附带服务器网站的尝试 包含域名的购买，服务器的选择，WordPress的安装配置等。 准备工作： 先要拥有一个自己喜欢的域名。申请域名中国的有万网，国外就是狗爹（godaddy）（可以支付宝）。（www.godaddy.com） 目前经过亲测推荐万网，更方便快捷.采用外国服务器加万网域名仍然可以避免繁杂的备案. 租服务器，vps，虚拟主机。价格依次降低。（国外的不需要备案，国内的需要大概20天的备案） 仍旧推荐国外的搬瓦工vps（https://bandwagonhost.com），可以自行百度搬瓦工9.9刀一年。初玩者这个是很划算的。次选项：阿里云9.9一月的学生计划。2.腾讯1元一月的学生计划。 解析商（https://www.dnspod.cn）注册账号即提供免费服务 万网买的域名推荐使用自带的解析服务。 开始搭建：（以下为centos环境教程，推荐）（以笔者自己域名www.mtianyan.cn为例，请自行替换名字） 使用ssh类软件如putty连接服务器。按照教程安装lnmp一键安装包环境（http://lnmp.org/）。 按照lnmp里教程添加虚拟主机：将域名与服务器挂钩。 将你要建站的wordpress等安装包变成如www.mtianyan.cn.tar.gz里面打开应为www.mtianyan.cn名字的文件夹，文件夹内应直接包含upload，wordpress等文件夹下的多个文件和文件夹.然后用ftp传输软件传输至网站根目录/home/wwwroot/www.mtianyan.cn目录中。 putty下执行命令cd /home/wwwroot进入网站根目录，执行命令tar -xzvf/home/wwwroot/www.mtianyan.cn/www.mtianyan.cn.tar.gz（将www.mtianyan.cn替换为自己的网址） 数据库设置，根据自己前面设置的mysql密码进入phpmyadmin后台新建数据库。（参考链接：http://www.wordpress.la/codex-WordPress%E5%AE%89%E8%A3%85%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.html） 通过域名进入wordpress等安装界面，根据自己前面对数据库的名字设置,用户名,密码设置来修改这三项。其他空着不用修改。 大功告成！建好了自己的基于wordpress的网站。亲测，phpwind，discuz，ecshop，都与wordpress安装同理。]]></content>
      <categories>
        <category>运维环境配置</category>
      </categories>
      <tags>
        <tag>网站搭建</tag>
        <tag>WordPress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mtianyan编程之旅再起航]]></title>
    <url>%2Fpost%2Faaa786b0.html</url>
    <content type="text"><![CDATA[总结过去，开往未来 爱生活爱编程：雄关漫道真如铁，而今迈步从头越。 看着即将要过期的去年9.9刀初尝鲜的搬瓦工的服务器，和要过期的iuuwe的域名，不禁感叹时光荏苒如白驹过隙般匆匆流逝。 逝者如斯夫，不舍昼夜。唯有充实自己，重拾学习，才能更接近梦想的高度。 过去的一年发生了太多太多或开心或悲伤的记忆。 学习，编程总是断断续续，并没有什么收获。 什么都看了点，学了点，学java，学Android，学渗透，学逆向，学破解wifi，做黑苹果。 都是一阵一阵如龙卷风般的浅尝辄止。 静静思过，不是我太过滥情，兴趣太多，是没有一个固定的目标。 雄关漫道真如铁，而今迈步从头越。 就此开启学习之路，编程之旅。加油！为自己为每一个想改变的人！!]]></content>
      <categories>
        <category>生活杂谈</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
